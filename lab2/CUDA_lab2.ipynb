{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"19ueBr8jlSiMM5x3s-2hLq0mWch6FBP8P","timestamp":1710398692037},{"file_id":"1DrZtO7D_myOhD__SuhSbvQRg0G_THtu_","timestamp":1677229545976}],"collapsed_sections":["F9PmBZql0ow4","5gUDpbz5TZml","v1aLRuwryuL-","neXhTBlkk7Oz","VvOoerr-DqRy","rlA0oQTE3SqN","8oEZ69DPMEPK"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["---\n","# **LAB 2 - Modello di programmazione CUDA**\n","---"],"metadata":{"id":"ivxGuz4MVBBQ"}},{"cell_type":"markdown","metadata":{"id":"F9PmBZql0ow4"},"source":["# ▶️ CUDA setup"]},{"cell_type":"code","metadata":{"id":"p9RIwaPbVQHV"},"source":["!nvcc --version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n5YlC1IOTlNb"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GPU computing notebooks download (from github)"],"metadata":{"id":"gcg1GyK5srek"}},{"cell_type":"code","source":["!git clone https://github.com/giulianogrossi/GPUcomputing.git"],"metadata":{"id":"tyHOxci3s3H8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"227eLdP5csN1"},"source":["NVCC Plugin for Jupyter notebook"]},{"cell_type":"code","source":["%cd GPUcomputing/utils/nvcc4jupyter-master/\n","!!python3 -m build\n","%load_ext nvcc4jupyter\n","%cd /content/"],"metadata":{"id":"4TzxMBFds8aT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To use also a plugin for cpp sintax highlighting..."],"metadata":{"id":"r9QDvdB8xAuX"}},{"cell_type":"code","source":["!wget -O cpp_plugin.py https://gist.github.com/akshaykhadse/7acc91dd41f52944c6150754e5530c4b/raw/cpp_plugin.py\n","%load_ext cpp_plugin"],"metadata":{"id":"FvkjBRIk070c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5gUDpbz5TZml"},"source":["# ✅ Blocks and grids"]},{"cell_type":"markdown","metadata":{"id":"Kv8Bm5Oobs8g"},"source":["**Grid 1D**: prints DIMs and IDs of grid, block and thread\n"]},{"cell_type":"code","metadata":{"id":"RlbVvBaXCHBs"},"source":["%%cuda_group_save --name \"checkIndex.cu\" --group \"lez2\"\n","\n","#include <stdio.h>\n","\n","__global__ void checkIndex(void) {\n","\tprintf(\"threadIdx:(%d, %d, %d) blockIdx:(%d, %d, %d) \"\n","\t\t\t\t\t\"blockDim:(%d, %d, %d) gridDim:(%d, %d, %d)\\n\",\n","\t\t\t\t\tthreadIdx.x, threadIdx.y, threadIdx.z,\n","\t\t\t\t\tblockIdx.x, blockIdx.y, blockIdx.z,\n","\t\t\t\t\tblockDim.x, blockDim.y, blockDim.z,\n","\t\t\t\t\tgridDim.x,gridDim.y,gridDim.z);\n","}\n","\n","/*\n","* MAIN\n","*/\n","int main(int argc, char **argv) {\n","\n","\t// grid and block definition\n","\tdim3 block(4);\n","\tdim3 grid(3);\n","\n","\t// Print from host\n","\tprintf(\"Print from host:\\n\");\n","\tprintf(\"grid.x = %d\\t grid.y = %d\\t grid.z = %d\\n\", grid.x, grid.y, grid.z);\n","\tprintf(\"block.x = %d\\t block.y = %d\\t block.z %d\\n\\n\", block.x, block.y, block.z);\n","\n","\t// Print from device\n","\tprintf(\"Print from device:\\n\");\n","\tcheckIndex<<<grid, block>>>();\n","\n","\t// reset device\n","\tcudaDeviceReset();\n","\treturn(0);\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ Run..."],"metadata":{"id":"JYBScKiE2ySK"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez2/checkIndex.cu -o checkIndex\n","!./checkIndex\n"],"metadata":{"id":"ngBc-5i6rDmR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Hvii50CGGfP"},"source":["Definire un kernel con block 2D e grid 2D che:\n","\n","1. usando ID di thread e block calcola la seguente espressione `s = threadIdx.x * blockDim.x + threadIdx.y * blockDim.y + blockIdx.x + blockIdx.y`\n","2. stampa `sum =  2  <--> threadIdx:(*,*), blockIdx:(*, *), blockDim:(*, *)` se `s` è un numero della sequenza di Fibonacci\n","\n","\n","NB: Sequenza di Fibonacci ([Fibonacci-wikipedia](https://it.wikipedia.org/wiki/Successione_di_Fibonacci))\n","$$\n","\\begin{align}\n","s_0 &= 0,\\\\\n","s_1 &= 1,\\\\\n","s_{n}&=s_{{n-1}}+s_{{n-2}},\\quad \\text{(per ogni $n>1$)}\n","\\end{align}\n","$$\n"]},{"cell_type":"markdown","source":["↘️ **SOL...**"],"metadata":{"id":"V-jbxio5Oy_Y"}},{"cell_type":"code","metadata":{"id":"4RNlS5U0adAk"},"source":["%%cuda_group_save --name \"checkIndex.cu\" --group \"lez2\"\n","#include <stdio.h>\n","\n","/*\n"," * Show DIMs & IDs for grid, block and thread\n"," */\n","__global__ void checkIndex(void) {\n","  uint tx = threadIdx.x;\n","  uint ty = threadIdx.y;\n","  uint bx = blockIdx.x;\n","  uint by = blockIdx.y;\n","  uint bxd = blockDim.x;\n","  uint byd = blockDim.y;\n","  uint sum = tx * bxd + ty * byd + bx + by;\n","\n","  // iterative def of fibonacci\n","  int fn2 = 0;\n","  int fn1 = 1;\n","  int fn = fn2 + fn1;\n","  while (fn < sum) {\n","    fn2 = fn1;\n","    fn1 = fn;\n","    fn = fn1 + fn2;\n","  }\n","  if (sum == fn || sum == 0 )\n","    printf(\"sum = %2d  <--> threadIdx:(%d, %d), blockIdx:(%d, %d), blockDim:(%d, %d)\\n\", sum, tx, ty, bx, by, bxd, byd);\n","}\n","\n","/*\n","* MAIN\n","*/\n","int main(int argc, char **argv) {\n","\n","\t// grid and block structure\n","\tdim3 block(5,5);\n","\tdim3 grid(3,3);\n","\n","\t// Print from host\n","\tprintf(\"Print from host:\\n\");\n","\tprintf(\"grid.x = %d\\t grid.y = %d\\t grid.z = %d\\n\", grid.x, grid.y, grid.z);\n","\tprintf(\"block.x = %d\\t block.y = %d\\t block.z %d\\n\\n\", block.x, block.y, block.z);\n","\n","\t// Print from device\n","\tprintf(\"Print from device:\\n\");\n","\tcheckIndex<<<grid, block>>>();\n","\n","\t// reset device\n","\tcudaDeviceReset();\n","\treturn (0);\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez2/checkIndex.cu -o checkIndex\n","!./checkIndex\n"],"metadata":{"id":"ezBsIEMfrSOP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↘️ **TODO...**"],"metadata":{"id":"jFSQv93R4WEQ"}},{"cell_type":"code","source":["%%cuda\n","#include <stdio.h>\n","\n","/*\n"," * Show DIMs & IDs for grid, block and thread\n"," */\n","__global__ void checkIndex(void) {\n","\n","  // TODO\n","\n","}\n","\n","int main(int argc, char **argv) {\n","\n","\t// grid and block structure\n","\n","\n","\t// Print from host\n","\tprintf(\"Print from host:\\n\");\n","\tprintf(\"grid.x = %d\\t grid.y = %d\\t grid.z = %d\\n\", grid.x, grid.y, grid.z);\n","\tprintf(\"block.x = %d\\t block.y = %d\\t block.z %d\\n\\n\", block.x, block.y, block.z);\n","\n","\t// Print from device\n","\n","\t// reset device\n","\tcudaDeviceReset();\n","\treturn (0);\n","}"],"metadata":{"id":"ogkWtHdH4TNN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez2/checkIndex.cu -o checkIndex\n","!./checkIndex\n"],"metadata":{"id":"CNcT6_p1C7Tl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ✅ Sum of vectors"],"metadata":{"id":"v1aLRuwryuL-"}},{"cell_type":"code","source":["%%cuda_group_save --name \"vector_sum.cu\" --group \"lez2\"\n","#include <stdio.h>\n","\n","#define CHECK(call)                                                            \\\n","{                                                                              \\\n","    const cudaError_t error = call;                                            \\\n","    if (error != cudaSuccess) {                                                \\\n","        fprintf(stderr, \"Error: %s:%d, \", __FILE__, __LINE__);                 \\\n","        fprintf(stderr, \"code: %d, reason: %s\\n\", error,                       \\\n","                cudaGetErrorString(error));                                    \\\n","    }                                                                          \\\n","}\n","\n","/**\n"," * CUDA Kernel: vector addition\n"," */\n","__global__ void vector_sum(const float *A, const float *B, float *C, int numElements) {\n","\tint i = blockDim.x * blockIdx.x + threadIdx.x;\n","\n","\tif (i < numElements)\n","\t\tC[i] = A[i] + B[i];\n","}\n","\n","/**\n"," * MAIN\n"," */\n","int main(void) {\n","\t// Error code to check return values for CUDA calls\n","\tcudaError_t err = cudaSuccess;\n","\n","\t// Print the vector length to be used, and compute its size\n","\tint N = 10000000;\n","\tsize_t size = N * sizeof(float);\n","\tprintf(\"Vector addition of %d elements (%f MB)\\n\", N, size/1E6);\n","\n","\t// Allocate the host input vector A,B,C\n","\tfloat *h_A = (float *) malloc(size);\n","\tfloat *h_B = (float *) malloc(size);\n","\tfloat *h_C = (float *) malloc(size);\n","\n","\t// Initialize the host input vectors\n","\tfor (int i = 0; i < N; ++i) {\n","\t\th_A[i] = rand() % 10;\n","\t\th_B[i] = rand() % 10;\n","\t}\n","\n","\t// Allocate the device input vector A,B,C\n","\tfloat *d_A = NULL;\n","\tCHECK(cudaMalloc((void **) &d_A, size));\n","\tfloat *d_B = NULL;\n","\tCHECK(cudaMalloc((void **) &d_B, size));\n","\tfloat *d_C = NULL;\n","\tCHECK(cudaMalloc((void **) &d_C, size));\n","\n","\t// Copy the host input vectors A and B in device memory\n","\tprintf(\"Copy input data from the host memory to the CUDA device\\n\");\n","\tCHECK(cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice));\n","\n","\t// Launch the Vector Add CUDA Kernel\n","\tint threadsPerBlock = 1024;\n","\tint blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n","\tprintf(\"CUDA kernel launch with %d blocks of %d threads\\n\", blocksPerGrid, threadsPerBlock);\n","\tvector_sum<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n","\tCHECK(cudaGetLastError());\n","\tif (err != cudaSuccess) {\n","\t\tfprintf(stderr, \"Failed to launch vectorAdd kernel (error code %s)!\\n\",\n","\t\t\t\tcudaGetErrorString(err));\n","\t\texit (EXIT_FAILURE);\n","\t}\n","\n","\t// Copy the device result vector in host memory\n","\tprintf(\"Copy output data from the CUDA device to the host memory\\n\");\n","\tCHECK(cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost));\n","\n","\t// Verify that the result vector is correct\n","\tfor (int i = 0; i < N; ++i) {\n","\t\tif (fabs(h_A[i] + h_B[i] - h_C[i]) > 1e-5) {\n","\t\t\tfprintf(stderr, \"Result verification failed at element %d!\\n\", i);\n","\t\t\texit (EXIT_FAILURE);\n","\t\t}\n","\t}\n","\n","\tprintf(\"Test PASSED\\n\");\n","\n","\t// Free device global memory\n","\tCHECK(cudaFree(d_A));\n","\tCHECK(cudaFree(d_B));\n","\tCHECK(cudaFree(d_C));\n","\n","\t// Free host memory\n","\tfree(h_A);\n","\tfree(h_B);\n","\tfree(h_C);\n","\n","\tprintf(\"Done\\n\");\n","\treturn 0;\n","}\n","\n"],"metadata":{"id":"6_XWLW9vyxAk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ Run..."],"metadata":{"id":"wGyhKRzF21oV"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez2/vector_sum.cu -o vector_sum\n","!./vector_sum\n"],"metadata":{"id":"NF5ItbArmKTZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"neXhTBlkk7Oz"},"source":["# ✅ Image flip - CPU"]},{"cell_type":"markdown","metadata":{"id":"AZZTbmBBGDf9"},"source":["**Visualizza immagine in python**: Librerie python per lettura/scrittura file di immagini e loro display: [openCV](https://docs.opencv.org/master/index.html) e [matplotlib](https://matplotlib.org/)"]},{"cell_type":"code","metadata":{"id":"JoDMZXCBxVDn"},"source":["import cv2 as cv\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# reads as a NumPy array: row (height) x column (width) x color (3)\n","dog = cv.imread('/content/GPUcomputing/images/dog.ppm')\n","print('Image size: ', dog.shape)\n","# BGR is converted to RGB\n","dog = cv.cvtColor(dog, cv.COLOR_BGR2RGB)\n","plt.imshow(dog)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%cpp -n flip_PPM.cpp -s xcode\n","\n","#include \"ppm.h\"\n","\n","int main(void) {\n","    char path[] = \"GPUcomputing/images/dog.ppm\";\n","    PPM *img = ppm_load(path);\n","    printf(\"PPM image size (w x h): %d x %d\\n\", img->width, img->height);\n","\n","    // flip horizontally\n","    PPM *img2 = ppm_copy(img);\n","    ppm_flipH(img2);\n","    ppm_write(img2, \"dogH.ppm\");\n","\n","    // flip vertically\n","    PPM *img3 = ppm_copy(img);\n","    ppm_flipV(img3);\n","    ppm_write(img3, \"dogV.ppm\");\n","\n","    return 0;\n","}"],"metadata":{"id":"9zvtCbVsrLeR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ Run..."],"metadata":{"id":"JsaUcyvG26H_"}},{"cell_type":"code","source":["!g++ -I GPUcomputing/utils/PPM GPUcomputing/utils/PPM/ppm.cpp flip_PPM.cpp -o flip_PPM\n","!./flip_PPM"],"metadata":{"id":"LSVfbfbWuMzn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2 as cv\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# reads as a NumPy array: row (height) x column (width) x color (3)\n","dogV = cv.imread('dogV.ppm')\n","dogV = cv.cvtColor(dogV, cv.COLOR_BGR2RGB)\n","dogH = cv.imread('dogH.ppm')\n","dogH = cv.cvtColor(dogH, cv.COLOR_BGR2RGB)\n","plt.imshow(dogV)\n","plt.show()\n","plt.imshow(dogH)\n","plt.show()"],"metadata":{"id":"S-8ukY9Cv-u3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2 as cv\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# reads as a NumPy array: row (height) x column (width) x color (3)\n","julia_jet = cv.imread('images/julia_jet.bmp')\n","print('Image size: ', julia_jet.shape)\n","# BGR is converted to RGB\n","julia_jet = cv.cvtColor(julia_jet, cv.COLOR_BGR2RGB)\n","julia_jetV = cv.imread('julia_jetV.bmp')\n","julia_jetV = cv.cvtColor(julia_jetV, cv.COLOR_BGR2RGB)\n","julia_jetH = cv.imread('julia_jetH.bmp')\n","julia_jetH = cv.cvtColor(julia_jetH, cv.COLOR_BGR2RGB)\n","plt.imshow(julia_jet)\n","plt.show()\n","plt.imshow(julia_jetV)\n","plt.show()\n","plt.imshow(julia_jetH)\n","plt.show()"],"metadata":{"id":"Kxeiv3nXr965"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VvOoerr-DqRy"},"source":["# ✅ Image flip - GPU"]},{"cell_type":"markdown","source":["↘️ **SOL...**"],"metadata":{"id":"UbY3_82q9Qfl"}},{"cell_type":"code","source":["%%cuda_group_save --name \"ppm_flipH_GPU.cu\" --group \"lez2\"\n","\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include \"ppm.h\"\n","#include \"../../GPUcomputing/utils/common.h\"\n","\n","/*\n"," * Kernel 1D that flips inplace the PPM image horizontally:\n"," * each thread only flips a single pixel (R,G,B)\n"," */\n","__global__ void ppm_flipH_GPU(PPM ppm) {\n","\n","   // ** pixel granularity **\n","   uint tid = blockIdx.x * blockDim.x + threadIdx.x;\n","   uint WIDTH = ppm.width;\n","   uint HALF = WIDTH;\n","   uint y = tid / HALF;\t     // row y\n","   uint x1 = tid % HALF;       // col x1\n","   uint x2 = WIDTH - x1 - 1;    // col x2\n","\n","   // check if y1 is in the first half of the image\n","   if (x1 >= WIDTH/2)\n","      return;\n","\n","   //  ** byte granularity **\n","   pel tmp;\n","   uint pel_idx1 = 3 * (x1 + y * HALF);\n","   tmp.r = ppm.image[pel_idx1];\n","   tmp.g = ppm.image[pel_idx1 + 1];\n","   tmp.b = ppm.image[pel_idx1 + 2];\n","\n","   // copy pel[y1,x] in pel[y2,x]\n","   uint pel_idx2 = 3 * (x2 + y * WIDTH);\n","   ppm.image[pel_idx1] =     ppm.image[pel_idx2];\n","   ppm.image[pel_idx1 + 1] = ppm.image[pel_idx2 + 1];\n","   ppm.image[pel_idx1 + 2] = ppm.image[pel_idx2 + 2];\n","\n","   // copy pel[y2,x] from tmp\n","   ppm.image[pel_idx2]     = tmp.r;\n","   ppm.image[pel_idx2 + 1] = tmp.g;\n","   ppm.image[pel_idx2 + 2] = tmp.b;\n","}\n","\n","/*\n"," * MAIN\n"," */\n","int main(int argc, char **argv) {\n","\n","   // PPM images\n","   PPM *ppm, *ppm1, *ppm2;  // Where images are stored in CPU\n","   PPM ppm_d;\t             // Where images are stored in GPU\n","\n","   // load a PPM image from file\n","   char path[] = \"GPUcomputing/images/dog.ppm\";\n","   ppm = ppm_load(path);\n","   ppm1 = ppm_copy(ppm);\n","   uint WIDTH = ppm->width;\n","   uint HEIGHT = ppm->height;\n","   printf(\"PPM image size (w x h): %d x %d\\n\", WIDTH, HEIGHT);\n","\n","   // set main params\n","   size_t nBytes= WIDTH * HEIGHT * sizeof(pel);\n","   ppm_d.width = WIDTH;\n","   ppm_d.height = HEIGHT;\n","   ppm_d.maxval = ppm->maxval;\n","\n","   // Allocate GPU buffer for the input and output images\n","   CHECK(cudaMalloc(&ppm_d.image, nBytes));\n","\n","   // copy image from CPU to GPU\n","   CHECK(cudaMemcpy(ppm_d.image, ppm->image, nBytes, cudaMemcpyHostToDevice));\n","\n","   // invoke kernels (define grid and block sizes)\n","   uint dimBlock = 256;\n","   uint dimGrid = (WIDTH/2 * HEIGHT + dimBlock - 1) / dimBlock;\n","\n","   double start = seconds();\n","   ppm_flipH_GPU <<<dimGrid, dimBlock>>> (ppm_d);\n","   CHECK(cudaDeviceSynchronize());\n","   double stopGPU = seconds() - start;\n","\n","   // copy image from GPU to CPU\n","   CHECK(cudaMemcpy(ppm1->image, ppm_d.image, nBytes, cudaMemcpyDeviceToHost));\n","   ppm_write(ppm1, \"ppm_flippedH.ppm\");\n","\n","   // check results with CPU\n","   ppm2 = ppm_copy(ppm);\n","   start = seconds();\n","   ppm_flipH(ppm2);\n","   double stopCPU = seconds() - start;\n","   char res = ppm_equal(ppm1, ppm2) ? 'Y' : 'N';\n","   printf(\"Are equal? %c\\n\", res);\n","   ppm_write(ppm2, \"output_flippedV_CPU.ppm\");\n","\n","   // times & speedup\n","   printf(\"CPU elapsed time: %.4f (msec) \\n\", stopCPU*1000);\n","   printf(\"CPU elapsed time: %.4f (msec) - Speedup %.1f\\n\", stopGPU*1000, stopCPU/stopGPU);\n","\n","   return (EXIT_SUCCESS);\n","}\n"],"metadata":{"id":"MHnd8KnSzB2V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ Run..."],"metadata":{"id":"Hgi9T70-zeY6"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez2/ppm_flipH_GPU.cu -o flipH -I GPUcomputing/utils/PPM  GPUcomputing/utils/PPM/ppm.cpp\n","!./flipH"],"metadata":{"id":"QRJyzue6zdk5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↘️ **TODO...**"],"metadata":{"id":"mqvAdVX8Nkr8"}},{"cell_type":"code","source":["%%cuda_group_save --name \"ppm_flipH_GPU.cu\" --group \"lez2\"\n","\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include \"ppm.h\"\n","#include \"../../GPUcomputing/utils/common.h\"\n","\n","/*\n"," * Kernel 1D that flips inplace the PPM image horizontally:\n"," * each thread only flips a single pixel (R,G,B)\n"," */\n","__global__ void ppm_flipH_GPU() {\n","\n","   // TODO\n","\n","}\n","\n","/*\n"," * MAIN\n"," */\n","int main(int argc, char **argv) {\n","\n","   // PPM images\n","\n","   // load a PPM image from file\n","   char path[] = \"GPUcomputing/images/dog.ppm\";\n","   ppm = ppm_load(path);\n","   ppm1 = ppm_copy(ppm);\n","   uint WIDTH = ppm->width;\n","   uint HEIGHT = ppm->height;\n","   printf(\"PPM image size (w x h): %d x %d\\n\", WIDTH, HEIGHT);\n","\n","   // set main params\n","   size_t nBytes= WIDTH * HEIGHT * sizeof(pel);\n","   ppm_d.width = WIDTH;\n","   ppm_d.height = HEIGHT;\n","   ppm_d.maxval = ppm->maxval;\n","\n","   // Allocate GPU buffer for the input and output images\n","\n","   // copy image from CPU to GPU\n","\n","   // invoke kernels (define grid and block sizes)\n","\n","   // copy image from GPU to CPU\n","\n","   // check results with CPU\n","   ppm2 = ppm_copy(ppm);\n","   start = seconds();\n","   ppm_flipH(ppm2);\n","   double stopCPU = seconds() - start;\n","   char res = ppm_equal(ppm1, ppm2) ? 'Y' : 'N';\n","   printf(\"Are equal? %c\\n\", res);\n","   ppm_write(ppm2, \"output_flippedV_CPU.ppm\");\n","\n","   // times & speedup\n","   printf(\"CPU elapsed time: %.4f (msec) \\n\", stopCPU*1000);\n","   printf(\"CPU elapsed time: %.4f (msec) - Speedup %.1f\\n\", stopGPU*1000, stopCPU/stopGPU);\n","\n","   return (EXIT_SUCCESS);\n","}\n"],"metadata":{"id":"2gEC4R0MBJAM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ Run..."],"metadata":{"id":"FoVntpFmBJAN"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez2/ppm_flipH_GPU.cu -o flipH -I GPUcomputing/utils/PPM  GPUcomputing/utils/PPM/ppm.cpp\n","!./flipH"],"metadata":{"id":"bAA6qT_WBJAO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↘️ **SOL...**"],"metadata":{"id":"hMivIeBaBjiu"}},{"cell_type":"code","metadata":{"id":"ZlGAr3KVB6Dw"},"source":["%%cuda_group_save --name \"ppm_flipV_GPU.cu\" --group \"lez2\"\n","\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include \"ppm.h\"\n","#include \"../../GPUcomputing/utils/common.h\"\n","\n","/*\n"," * Kernel 1D that flips inplace the PPM image vertically:\n"," * each thread only flips a single pixel (R,G,B)\n"," */\n","__global__ void ppm_flipV_GPU(PPM ppm) {\n","\n","   // ** pixel granularity **\n","   uint tid = blockIdx.x * blockDim.x + threadIdx.x;\n","   uint WIDTH = ppm.width;\n","   uint HEIGHT = ppm.height;\n","   uint y1 = tid / WIDTH;\t     // row y1\n","   uint y2 = HEIGHT - y1 - 1;   // row y2\n","   uint x = tid % WIDTH;        // col x (unique)\n","\n","   // check if y1 is in the first half of the image\n","   if (y1 >= HEIGHT/2)\n","      return;\n","\n","   //  ** byte granularity **\n","   pel tmp;\n","   uint pel_idx1 = 3 * (x + y1 * WIDTH);\n","   tmp.r = ppm.image[pel_idx1];\n","   tmp.g = ppm.image[pel_idx1 + 1];\n","   tmp.b = ppm.image[pel_idx1 + 2];\n","\n","   // copy pel[y1,x] in pel[y2,x]\n","   uint pel_idx2 = 3 * (x + y2 * WIDTH);\n","   ppm.image[pel_idx1] =     ppm.image[pel_idx2];\n","   ppm.image[pel_idx1 + 1] = ppm.image[pel_idx2 + 1];\n","   ppm.image[pel_idx1 + 2] = ppm.image[pel_idx2 + 2];\n","\n","   // copy pel[y2,x] from tmp\n","   ppm.image[pel_idx2]     = tmp.r;\n","   ppm.image[pel_idx2 + 1] = tmp.g;\n","   ppm.image[pel_idx2 + 2] = tmp.b;\n","}\n","\n","/*\n"," * MAIN\n"," */\n","int main(int argc, char **argv) {\n","\n","   // PPM images\n","   PPM *ppm, *ppm1, *ppm2;  // Where images are stored in CPU\n","   PPM ppm_d;\t             // Where images are stored in GPU\n","\n","   // load a PPM image from file\n","   char path[] = \"GPUcomputing/images/dog.ppm\";\n","   ppm = ppm_load(path);\n","   ppm1 = ppm_copy(ppm);\n","   uint WIDTH = ppm->width;\n","   uint HEIGHT = ppm->height;\n","   printf(\"PPM image size (w x h): %d x %d\\n\", WIDTH, HEIGHT);\n","\n","   // set main params\n","   size_t nBytes= WIDTH * HEIGHT * sizeof(pel);\n","   ppm_d.width = WIDTH;\n","   ppm_d.height = HEIGHT;\n","   ppm_d.maxval = ppm->maxval;\n","\n","   // Allocate GPU buffer for the input and output images\n","   CHECK(cudaMalloc(&ppm_d.image, nBytes));\n","\n","   // copy image from CPU to GPU\n","   CHECK(cudaMemcpy(ppm_d.image, ppm->image, nBytes, cudaMemcpyHostToDevice));\n","\n","   // invoke kernels (define grid and block sizes)\n","   uint dimBlock = 256;\n","   uint dimGrid = (WIDTH * HEIGHT/2 + dimBlock - 1) / dimBlock;\n","\n","   double start = seconds();\n","   ppm_flipV_GPU <<<dimGrid, dimBlock>>> (ppm_d);\n","   CHECK(cudaDeviceSynchronize());\n","\t double stopGPU = seconds() - start;\n","\n","   // copy image from GPU to CPU\n","   CHECK(cudaMemcpy(ppm1->image, ppm_d.image, nBytes, cudaMemcpyDeviceToHost));\n","   ppm_write(ppm1, \"ppm_flippedV_GPU.ppm\");\n","\n","   // check results with CPU\n","   ppm2 = ppm_copy(ppm);\n","   start = seconds();\n","   ppm_flipV(ppm2);\n","   double stopCPU = seconds() - start;\n","   char res = ppm_equal(ppm1, ppm2) ? 'Y' : 'N';\n","   printf(\"Are equal? %c\\n\", res);\n","   ppm_write(ppm2, \"ppm_flippedV_CPU.ppm\");\n","\n","   // times & speedup\n","   printf(\"CPU elapsed time: %.4f (msec) \\n\", stopCPU*1000);\n","   printf(\"CPU elapsed time: %.4f (msec) - Speedup %.1f\\n\", stopGPU*1000, stopCPU/stopGPU);\n","\n","   return (EXIT_SUCCESS);\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ Run..."],"metadata":{"id":"z1oOK-LTsG12"}},{"cell_type":"code","metadata":{"id":"I7XGTIu9DonC"},"source":["!nvcc -arch=sm_75 src/lez2/ppm_flipV_GPU.cu -o flipV -I GPUcomputing/utils/PPM  GPUcomputing/utils/PPM/ppm.cpp\n","!./flipV"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2 as cv\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# reads as a NumPy array: row (height) x column (width) x color (3)\n","dogV = cv.imread('ppm_flippedV_CPU.ppm')\n","dogV = cv.cvtColor(dogV, cv.COLOR_BGR2RGB)\n","dogH = cv.imread('ppm_flippedV_GPU.ppm')\n","dogH = cv.cvtColor(dogH, cv.COLOR_BGR2RGB)\n","plt.imshow(dogV)\n","plt.show()\n","plt.imshow(dogH)\n","plt.show()"],"metadata":{"id":"Dg6Naov-c79q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↘️ **TODO...**"],"metadata":{"id":"ePkWn1Pb0PwY"}},{"cell_type":"code","metadata":{"id":"oJz6JMmSBqDm"},"source":["%%cuda_group_save --name \"ppm_flipV_GPU.cu\" --group \"lez2\"\n","\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include \"ppm.h\"\n","#include \"../../GPUcomputing/utils/common.h\"\n","\n","/*\n"," * Kernel 1D that flips inplace the PPM image vertically:\n"," * each thread only flips a single pixel (R,G,B)\n"," */\n","__global__ void ppm_flipV_GPU() {\n","\n","  // TODO\n","}\n","\n","/*\n"," * MAIN\n"," */\n","int main(int argc, char **argv) {\n","\n","   // PPM images\n","   PPM *ppm, *ppm1, *ppm2;  // Where images are stored in CPU\n","\n","   // load a PPM image from file\n","   char path[] = \"GPUcomputing/images/dog.ppm\";\n","   ppm = ppm_load(path);\n","   ppm1 = ppm_copy(ppm);\n","   uint WIDTH = ppm->width;\n","   uint HEIGHT = ppm->height;\n","   printf(\"PPM image size (w x h): %d x %d\\n\", WIDTH, HEIGHT);\n","\n","   // set main params\n","   size_t nBytes= WIDTH * HEIGHT * sizeof(pel);\n","   ppm_d.width = WIDTH;\n","   ppm_d.height = HEIGHT;\n","   ppm_d.maxval = ppm->maxval;\n","\n","   // Allocate GPU buffer for the input and output images\n","\n","   // copy image from CPU to GPU\n","\n","   // invoke kernels (define grid and block sizes)\n","\n","   // copy image from GPU to CPU\n","\n","   // check results with CPU\n","   ppm2 = ppm_copy(ppm);\n","   start = seconds();\n","   ppm_flipV(ppm2);\n","   double stopCPU = seconds() - start;\n","   char res = ppm_equal(ppm1, ppm2) ? 'Y' : 'N';\n","   printf(\"Are equal? %c\\n\", res);\n","   ppm_write(ppm2, \"output_flippedV_CPU.ppm\");\n","\n","   // times & speedup\n","   printf(\"CPU elapsed time: %.4f (msec) \\n\", stopCPU*1000);\n","   printf(\"CPU elapsed time: %.4f (msec) - Speedup %.1f\\n\", stopGPU*1000, stopCPU/stopGPU);\n","\n","   return (EXIT_SUCCESS);\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ Run..."],"metadata":{"id":"JyXZdh4QBqDn"}},{"cell_type":"code","metadata":{"id":"YHgcEAClBqDn"},"source":["!nvcc -arch=sm_75 src/lez2/ppm_flipV_GPU.cu -o flipV -I GPUcomputing/utils/PPM  GPUcomputing/utils/PPM/ppm.cpp\n","!./flipV"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ✅ Sum of matrices"],"metadata":{"id":"rlA0oQTE3SqN"}},{"cell_type":"code","source":["%%cuda_group_save --name \"matrix_sum.cu\" --group \"lez2\"\n","\n","#include <stdio.h>\n","#include <stdlib.h>\n","\n","#define CHECK(call)                                                            \\\n","{                                                                              \\\n","    const cudaError_t error = call;                                            \\\n","    if (error != cudaSuccess)                                                  \\\n","    {                                                                          \\\n","        fprintf(stderr, \"Error: %s:%d, \", __FILE__, __LINE__);                 \\\n","        fprintf(stderr, \"code: %d, reason: %s\\n\", error,                       \\\n","                cudaGetErrorString(error));                                    \\\n","    }                                                                          \\\n","}\n","\n","// Matrices are stored in row-major order:\n","// M(row, col) = *(M.elements + row * M.width + col)\n","typedef struct {\n","  int width;\n","  int height;\n","  float* elements;\n","} Matrix;\n","\n","// Thread block size\n","#define BLOCK_SIZE 16\n","\n","/* Matrix sum kernel: Each thread calculates an element of C\n","*  by summing the corresponding elements of A and B\n","*/\n","__global__ void matSumKernel(Matrix A, Matrix B, Matrix C) {\n","\n","  int y = blockIdx.y * blockDim.y + threadIdx.y;\n","  int x = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","  // Check if the thread is within the matrix bounds\n","  if ((y < A.height) && (x < A.width))\n","    C.elements[y * C.width + x] = A.elements[y * A.width + x] + B.elements[y * B.width + x];\n","}\n","\n","/*\n"," * MAIN\n"," */\n"," int main(int argc, char **argv) {\n","\n","  // Matrix size\n","  int nx = 1 << 5;\n","  int ny = 1 << 5;\n","  int size = nx * ny;\n","  int nBytes = nx * ny * sizeof(float);\n","  printf(\"Matrix size: %d x %d\\n\", nx, ny);\n","\n","  // Matrix on the host (CPU) and device (GPU)\n","  Matrix A, B, C, d_A, d_B, d_C;\n","  A.width = B.width = C.width = nx;\n","  d_A.width = d_B.width = d_C.width = nx;\n","  A.height = B.height = C.height = ny;\n","  d_A.height = d_B.height = d_C.height = ny;\n","  A.elements = (float*)malloc(nBytes);\n","  B.elements = (float*)malloc(nBytes);\n","  C.elements = (float*)malloc(nBytes);\n","\n","  // Initialize A and B with 1.0f and 2.0f\n","  for (int i = 0; i < size; i++) {\n","    A.elements[i] = 1.0f;\n","    B.elements[i] = 2.0f;\n","  }\n","\n","  // device mem allocation & copy of A and B\n","  CHECK(cudaMalloc(&d_A.elements, nBytes));\n","  CHECK(cudaMalloc(&d_B.elements, nBytes));\n","  CHECK(cudaMalloc(&d_C.elements, nBytes));\n","  CHECK(cudaMemcpy(d_A.elements, A.elements, nBytes, cudaMemcpyHostToDevice));\n","  CHECK(cudaMemcpy(d_B.elements, B.elements, nBytes, cudaMemcpyHostToDevice));\n","\n","  // Invoke kernel\n","  dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);\n","  dim3 dimGrid((A.width + dimBlock.x - 1) / dimBlock.x, (A.height + dimBlock.y - 1) / dimBlock.y);\n","  matSumKernel<<<dimGrid, dimBlock>>>(d_A, d_B, d_C);\n","\n","  // Read C from device memory\n","  CHECK(cudaMemcpy(C.elements, d_C.elements, nBytes, cudaMemcpyDeviceToHost));\n","\n","  // check resutls\n","  int i;\n","  for (i = 0; i < size; i++) {\n","    if (C.elements[i] != 3.0f) {\n","      printf(\"Error: C[%d] = %f\\n\", i, C.elements[i]);\n","      break;\n","    }\n","  }\n","  if (i == size)\n","    printf(\"Test PASSED\\n\");\n","  else\n","    printf(\"Test not PASSED\\n\");\n","\n","  // Free device memory\n","  cudaFree(d_A.elements);\n","  cudaFree(d_B.elements);\n","  cudaFree(d_C.elements);\n","}\n","\n","\n"],"metadata":{"id":"jdD-PUY_3WXf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ Run..."],"metadata":{"id":"IiFir1oY2VW4"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez2/matrix_sum.cu -o matrix_sum\n","!./matrix_sum"],"metadata":{"id":"SfKgZ96pqccJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ✅ Image blurring"],"metadata":{"id":"8oEZ69DPMEPK"}},{"cell_type":"markdown","source":["↘️ **SOL...**"],"metadata":{"id":"hTbW781d-d16"}},{"cell_type":"code","source":["%%cuda_group_save --name \"ppm_blurGPU.cu\" --group \"lez2\"\n","\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include \"ppm.h\"\n","#include \"../../GPUcomputing/utils/common.h\"\n","\n","/*\n"," * Set pel (pixel element) in ppm image.\n"," */\n"," __device__ void ppm_setGPU(PPM ppm, int x, int y, pel c) {\n","  int i = x + y*ppm.width;\n","  ppm.image[3*i] = c.r;\n","  ppm.image[3*i + 1] = c.g;\n","  ppm.image[3*i + 2] = c.b;\n","}\n","\n","/*\n","* Get pel (pixel element) from ppm image.\n","*/\n","__device__ pel ppm_getGPU(PPM ppm, int x, int y) {\n","  pel p;\n","  int i = x + y*ppm.width;\n","  p.r = ppm.image[3*i];\n","  p.g = ppm.image[3*i + 1];\n","  p.b = ppm.image[3*i + 2];\n","  return p;\n","}\n","\n","/*\n"," * Kernel 2D for PPM image blurring\n"," */\n","__global__ void ppm_blurGPU(PPM ppm, PPM ppm1, int MASK_SIZE) {\n","\n","  int x = blockIdx.x * blockDim.x + threadIdx.x;\n","  int y = blockIdx.y * blockDim.y + threadIdx.y;\n","\n","  if(x < ppm.width && y < ppm.height) {\n","    float R=0, G=0, B=0;\n","    int numPixels = 0;\n","    int RADIUS = MASK_SIZE/2;\n","    for(int r = -RADIUS; r < RADIUS; ++r) {\n","        for(int c = -RADIUS; c < RADIUS; ++c) {\n","            int row = y + r;\n","            int col = x + c;\n","            if(row > -1 && row < ppm.height && col > -1 && col < ppm.width) {\n","                int i = col + row*ppm.width;\n","                R += ppm.image[3*i];\n","                G += ppm.image[3*i + 1];\n","                B += ppm.image[3*i + 2];\n","                numPixels++;\n","            }\n","        }\n","    }\n","    int i = x + y*ppm.width;\n","    ppm1.image[3*i]     = (color)(R/numPixels);\n","    ppm1.image[3*i + 1] = (color)(G/numPixels);\n","    ppm1.image[3*i + 2] = (color)(B/numPixels);\n","  }\n","}\n","\n","/*\n"," * MAIN\n"," */\n","int main(int argc, char **argv) {\n","\n","  // PPM images\n","  PPM *ppm, *ppm1, *ppm2;  // Where images are stored in CPU\n","  PPM ppm_d, ppm1_d;\t     // Where images are stored in GPU\n","\n","  // load a PPM image from file\n","  char path[] = \"GPUcomputing/images/dog.ppm\";\n","  ppm = ppm_load(path);\n","  ppm1 = ppm_copy(ppm);\n","  uint WIDTH = ppm->width;\n","  uint HEIGHT = ppm->height;\n","  printf(\"PPM image size (w x h): %d x %d\\n\", WIDTH, HEIGHT);\n","\n","  // set main params\n","  size_t nBytes= WIDTH * HEIGHT * sizeof(pel);\n","  ppm_d.width = WIDTH;\n","  ppm_d.height = HEIGHT;\n","  ppm_d.maxval = ppm->maxval;\n","  int MASK_SIZE = 21;\n","\n","  // Allocate GPU buffer for the input and output images\n","  CHECK(cudaMalloc(&ppm_d.image, nBytes));\n","  CHECK(cudaMalloc(&ppm1_d.image, nBytes));\n","\n","  // copy image from CPU to GPU\n","  CHECK(cudaMemcpy(ppm_d.image, ppm->image, nBytes, cudaMemcpyHostToDevice));\n","\n","  // invoke kernels (define grid and block sizes)\n","  uint dimBlock = 16;\n","  dim3 block(dimBlock, dimBlock);\n","  dim3 grid((WIDTH + block.x - 1) / block.x, (HEIGHT + block.y - 1) / block.y);\n","\n","  double start = seconds();\n","  ppm_blurGPU <<<grid, block>>> (ppm_d, ppm1_d, MASK_SIZE);\n","  CHECK(cudaDeviceSynchronize());\n","  double stopGPU = seconds() - start;\n","\n","  // copy image from GPU to CPU\n","  CHECK(cudaMemcpy(ppm1->image, ppm1_d.image, nBytes, cudaMemcpyDeviceToHost));\n","  ppm_write(ppm1, \"ppm_blurredGPU.ppm\");\n","\n","  // check results with CPU\n","  ppm2 = ppm_make(ppm->width, ppm->height, (pel){0,0,0});\n","  start = seconds();\n","  ppm_blur(ppm, ppm2, MASK_SIZE);\n","  double stopCPU = seconds() - start;\n","  ppm_write(ppm2, \"ppm_blurredCPU.ppm\");\n","  printf(\"PPM images are %s\\n\", ppm_equal(ppm1, ppm2) ? \"equal\" : \"not equal\");\n","\n","  // free device memory\n","  CHECK(cudaFree(ppm_d.image));\n","  CHECK(cudaFree(ppm1_d.image));\n","\n","  // times & speedup\n","  printf(\"CPU elapsed time: %.4f (msec) \\n\", stopCPU*1000);\n","  printf(\"GPU elapsed time: %.4f (msec) - Speedup %.1f\\n\", stopGPU*1000, stopCPU/stopGPU);\n","\n","  return (EXIT_SUCCESS);\n","}\n","\n"],"metadata":{"id":"f0t3lyKPMHfQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"MtPvhPzA3nwl"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez2/ppm_blurGPU.cu -o blur -I GPUcomputing/utils/PPM  GPUcomputing/utils/PPM/ppm.cpp\n","!./blur"],"metadata":{"id":"EWrIuCdsn6C-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↘️ **TODO...**"],"metadata":{"id":"DyzjzGiv8F-W"}},{"cell_type":"code","source":["%%cuda_group_save --name \"ppm_blurGPU.cu\" --group \"lez2\"\n","\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include \"ppm.h\"\n","#include \"../../GPUcomputing/utils/common.h\"\n","\n","/*\n"," * Set pel (pixel element) in ppm image.\n"," */\n"," __device__ void ppm_setGPU(PPM ppm, int x, int y, pel c) {\n","  int i = x + y*ppm.width;\n","  ppm.image[3*i] = c.r;\n","  ppm.image[3*i + 1] = c.g;\n","  ppm.image[3*i + 2] = c.b;\n","}\n","\n","/*\n","* Get pel (pixel element) from ppm image.\n","*/\n","__device__ pel ppm_getGPU(PPM ppm, int x, int y) {\n","  pel p;\n","  int i = x + y*ppm.width;\n","  p.r = ppm.image[3*i];\n","  p.g = ppm.image[3*i + 1];\n","  p.b = ppm.image[3*i + 2];\n","  return p;\n","}\n","\n","/*\n"," * Kernel 1D that flips inplace the PPM image vertically:\n"," * each thread only flips a single pixel (R,G,B)\n"," */\n","__global__ void ppm_blurGPU() {\n","\n","  // TODO\n","}\n","\n","/*\n"," * MAIN\n"," */\n","int main(int argc, char **argv) {\n","\n","  // PPM images\n","\n","  // load a PPM image from file\n","\n","  // set main params\n","\n","  // Allocate GPU buffer for the input and output images\n","\n","  // copy image from CPU to GPU\n","\n","  // invoke kernels (define grid and block sizes)\n","\n","  // copy image from GPU to CPU\n","\n","  // check results with CPU\n","  start = seconds();\n","  ppm_blur(ppm, ppm2, KERNEL_SIZE);\n","  double stopCPU = seconds() - start;\n","  ppm_write(ppm2, \"ppm_blurredCPU.ppm\");\n","  printf(\"PPM images are %s\\n\", ppm_equal(ppm1, ppm2) ? \"equal\" : \"not equal\");\n","\n","  // free device memory\n","\n","  // times & speedup\n","  printf(\"CPU elapsed time: %.4f (msec) \\n\", stopCPU*1000);\n","  printf(\"CPU elapsed time: %.4f (msec) - Speedup %.1f\\n\", stopGPU*1000, stopCPU/stopGPU);\n","\n","  return (EXIT_SUCCESS);\n","}\n","\n"],"metadata":{"id":"kYlibDxw8MR6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"g6HGH6tR8T-f"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez2/ppm_blurGPU.cu -o blur -I GPUcomputing/utils/PPM  GPUcomputing/utils/PPM/ppm.cpp\n","!./blur"],"metadata":{"id":"XxOmCOPY8Q7L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"ZXrrxFhbVD1B"}},{"cell_type":"code","source":["#include \"../utils/common.h\"\n","#include \"ppm.h\"\n","\n","#define BLOCK_SIZE   32\n","#define MASK_SIZE    21\n","#define TILE_SIZE    (BLOCK_SIZE + MASK_SIZE - 1)\n","\n","typedef struct {\n","   int width;\n","   int height;\n","   float* elements;\n"," } Matrix;\n","\n"," /*\n","  * 2D convolution using shared memory\n","  *   A: input matrix\n","  *   B: output matrix\n","  *   M: convolution mask matrix\n"," */\n","__global__ void conv2D(Matrix A, Matrix B, Matrix M) {\n","\n","   int x = blockIdx.x * blockDim.x + threadIdx.x; // Column index of matrix A\n","   int y = blockIdx.y * blockDim.y + threadIdx.y; // Row index of matrix A\n","\n","   int tile_size = BLOCK_SIZE + MASK_SIZE - 1;\n","   int radius = MASK_SIZE / 2;\n","\n","   // Allocate shared memory\n","   __shared__ float smem[TILE_SIZE][TILE_SIZE];\n","\n","   // Load data into shared memory\n","   for (int row = 0; row <= tile_size/blockDim.y; row++) {\n","      for (int col = 0; col <= tile_size/blockDim.x; col++) {\n","         int row_data = y + blockDim.y * row - radius;   // input data index row\n","         int col_data = x + blockDim.x * col - radius;   // input data index column\n","         int row_smem = threadIdx.y + blockDim.y * row;  // mask index row\n","         int col_smem = threadIdx.x + blockDim.x * col;  // mask index column\n","\n","         // Check valid range for smem and data\n","         if (row_smem < tile_size && col_smem < tile_size) {\n","            if (row_data >= 0 && row_data < A.height && col_data >= 0 && col_data < A.width) {\n","               smem[row_smem][col_smem] = A.elements[row_data * A.width + col_data];\n","            } else {\n","               smem[row_smem][col_smem] = 0.0f;\n","            }\n","         }\n","      }\n","   }\n","\n","   // Synchronize threads\n","   __syncthreads();\n","\n","   // Apply convolution\n","   float sum = 0.0f;\n","   for (int i = 0; i < MASK_SIZE; i++) {\n","      for (int j = 0; j < MASK_SIZE; j++) {\n","         int r = threadIdx.y + i;\n","         int c = threadIdx.x + j;\n","         if (r >= 0 && r < tile_size && c >= 0 && c < tile_size) {\n","            sum += smem[r][c] * M.elements[i * MASK_SIZE + j];\n","         }\n","      }\n","   }\n","\n","   // Write output\n","   if (y < A.height && x < A.width) {\n","      B.elements[y * B.width + x] = sum;\n","   }\n","}\n","\n","/*\n"," * Main function\n"," */\n","int main(void) {\n","   // Load image\n","   char path[] = \"../images/dog.ppm\";\n","   PPM *img = ppm_load(path);\n","   int WIDTH = img->width;\n","   int HEIGHT = img->height;\n","   printf(\"PPM image size (w x h): %d x %d\\n\", WIDTH, HEIGHT);\n","\n","   // extract channels and set matrices\n","   Matrix R, G, B;\n","   R.width = WIDTH; R.height = HEIGHT;\n","   G.width = WIDTH; G.height = HEIGHT;\n","   B.width = WIDTH; B.height = HEIGHT;\n","   R.elements = (float *) malloc(WIDTH * HEIGHT * sizeof(float));\n","   G.elements = (float *) malloc(WIDTH * HEIGHT * sizeof(float));\n","   B.elements = (float *) malloc(WIDTH * HEIGHT * sizeof(float));\n","   color *r = ppm_extract_channel(img, 0); // get red channel\n","   color *g = ppm_extract_channel(img, 1); // get green channel\n","   color *b = ppm_extract_channel(img, 2); // get blue channel\n","   for (int i = 0; i < WIDTH * HEIGHT; i++) {\n","      R.elements[i] = (float) r[i];\n","      G.elements[i] = (float) g[i];\n","      B.elements[i] = (float) b[i];\n","   }\n","\n","   // get gaussian filter mask\n","   float SIGMA = 10.0;\n","   Matrix M;\n","   M.width = WIDTH; M.height = HEIGHT;\n","   M.elements = gaussMask(MASK_SIZE, SIGMA);\n","\n","   // Allocate device memory\n","   Matrix d_R, d_G, d_B, d_M;\n","   d_R.width = R.width; d_R.height = R.height;\n","   d_G.width = G.width; d_G.height = G.height;\n","   d_B.width = B.width; d_B.height = B.height;\n","   d_M.width = M.width; d_M.height = M.height;\n","   CHECK(cudaMalloc(&d_R.elements, R.width * R.height * sizeof(float)));\n","   CHECK(cudaMalloc(&d_G.elements, G.width * G.height * sizeof(float)));\n","   CHECK(cudaMalloc(&d_B.elements, B.width * B.height * sizeof(float)));\n","   CHECK(cudaMalloc(&d_M.elements, M.width * M.height * sizeof(float)));\n","\n","   // Copy data to device\n","   CHECK(cudaMemcpy(d_R.elements, R.elements, R.width * R.height * sizeof(float), cudaMemcpyHostToDevice));\n","   CHECK(cudaMemcpy(d_G.elements, G.elements, G.width * G.height * sizeof(float), cudaMemcpyHostToDevice));\n","   CHECK(cudaMemcpy(d_B.elements, B.elements, B.width * B.height * sizeof(float), cudaMemcpyHostToDevice));\n","   CHECK(cudaMemcpy(d_M.elements, M.elements, M.width * M.height * sizeof(float), cudaMemcpyHostToDevice));\n","\n","   /***********************************************************/\n","\t/*                    conv2D on host                       */\n","\t/***********************************************************/\n","   printf(\"\\nCPU procedure...\\n\");\n","\tdouble start = seconds();\n","   PPM *img_filtered = ppm_make(WIDTH, HEIGHT, (pel) {0,0,0}); // create a new image\n","   ppm_gaussFilter(img, img_filtered, MASK_SIZE, SIGMA);\n","   ppm_write(img_filtered, \"output_gaussian.ppm\");\n","\tdouble stopCPU = seconds() - start;\n","   printf(\"   Host elapsed time: %f\\n\", stopCPU);\n","\n","   /***********************************************************/\n","\t/*                  GPU conv2D wih smem                    */\n","\t/***********************************************************/\n","   printf(\"\\nGPU conv2D with smem...\\n\");\n","   dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);\n","   dim3 dimGrid((WIDTH + BLOCK_SIZE - 1) / BLOCK_SIZE, (HEIGHT + BLOCK_SIZE - 1) / BLOCK_SIZE);\n","   start = seconds();\n","   conv2D<<<dimGrid, dimBlock>>>(d_R, d_R, d_M);\n","   conv2D<<<dimGrid, dimBlock>>>(d_G, d_G, d_M);\n","   conv2D<<<dimGrid, dimBlock>>>(d_B, d_B, d_M);\n","   CHECK(cudaDeviceSynchronize());\n","   double stopGPU = seconds() - start;\n","   printf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU, stopCPU / stopGPU);\n","\n","   // Copy data back to host\n","   Matrix R1, G1, B1;\n","   R1.width = WIDTH; R1.height = HEIGHT;\n","   G1.width = WIDTH; G1.height = HEIGHT;\n","   B1.width = WIDTH; B1.height = HEIGHT;\n","   R1.elements = (float *) malloc(WIDTH * HEIGHT * sizeof(float));\n","   G1.elements = (float *) malloc(WIDTH * HEIGHT * sizeof(float));\n","   B1.elements = (float *) malloc(WIDTH * HEIGHT * sizeof(float));\n","   CHECK(cudaMemcpy(R1.elements, d_R.elements, WIDTH * HEIGHT * sizeof(float), cudaMemcpyDeviceToHost));\n","   CHECK(cudaMemcpy(G1.elements, d_G.elements, WIDTH * HEIGHT * sizeof(float), cudaMemcpyDeviceToHost));\n","   CHECK(cudaMemcpy(B1.elements, d_B.elements, WIDTH * HEIGHT * sizeof(float), cudaMemcpyDeviceToHost));\n","   for (int i = 0; i < WIDTH * HEIGHT; i++) {\n","      r[i] = (color) R1.elements[i];\n","      g[i] = (color) G1.elements[i];\n","      b[i] = (color) B1.elements[i];\n","   }\n","\n","   // check results\n","   PPM *ppm_filtered = ppm_combine_channels(r, g, b, WIDTH, HEIGHT);\n","   ppm_write(img_filtered, \"output_gaussianGPU.ppm\");\n","\n","   return 0;\n","}"],"metadata":{"id":"IEU-15aAU-_5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↘️ **TODO...**"],"metadata":{"id":"DTn8lCaGVLeM"}}]}