{"cells":[{"cell_type":"markdown","id":"cd16b39b","metadata":{"id":"cd16b39b"},"source":["---\n","# **LAB 9 - CUDA in Python**\n","---"]},{"cell_type":"markdown","metadata":{"id":"-_i3qX0HhLrV"},"source":["# ▶️ CUDA setup"],"id":"-_i3qX0HhLrV"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JZS1v474hLrW","executionInfo":{"status":"ok","timestamp":1747255180599,"user_tz":-120,"elapsed":138,"user":{"displayName":"Giuliano Grossi","userId":"05046904419500019805"}},"outputId":"f9a56354-b3a8-4103-8437-26d7ee5510af"},"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2024 NVIDIA Corporation\n","Built on Thu_Jun__6_02:18:23_PDT_2024\n","Cuda compilation tools, release 12.5, V12.5.82\n","Build cuda_12.5.r12.5/compiler.34385749_0\n"]}],"source":["!nvcc --version"],"id":"JZS1v474hLrW"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TWZ7N-4OhLrW","executionInfo":{"status":"ok","timestamp":1747255185528,"user_tz":-120,"elapsed":145,"user":{"displayName":"Giuliano Grossi","userId":"05046904419500019805"}},"outputId":"96533917-6901-4414-a17e-4c2116329bef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed May 14 20:39:45 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   49C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"],"id":"TWZ7N-4OhLrW"},{"cell_type":"code","source":["!pip install numba-cuda==0.4.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xSoLaRWchLrX","executionInfo":{"status":"ok","timestamp":1747255234561,"user_tz":-120,"elapsed":7569,"user":{"displayName":"Giuliano Grossi","userId":"05046904419500019805"}},"outputId":"ec709fc6-9842-4a6a-c676-0f82f2b763c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting numba-cuda==0.4.0\n","  Downloading numba_cuda-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: numba>=0.59.1 in /usr/local/lib/python3.11/dist-packages (from numba-cuda==0.4.0) (0.60.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.59.1->numba-cuda==0.4.0) (0.43.0)\n","Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.11/dist-packages (from numba>=0.59.1->numba-cuda==0.4.0) (2.0.2)\n","Downloading numba_cuda-0.4.0-py3-none-any.whl (453 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/453.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.5/453.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.8/453.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numba-cuda\n","  Attempting uninstall: numba-cuda\n","    Found existing installation: numba-cuda 0.2.0\n","    Uninstalling numba-cuda-0.2.0:\n","      Successfully uninstalled numba-cuda-0.2.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 25.2.1 requires numba-cuda<0.3.0a0,>=0.2.0, but you have numba-cuda 0.4.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numba-cuda-0.4.0\n"]}],"id":"xSoLaRWchLrX"},{"cell_type":"code","source":["from numba import config\n","config.CUDA_ENABLE_PYNVJITLINK = 1"],"metadata":{"id":"iTq5M1SfhLrX"},"execution_count":null,"outputs":[],"id":"iTq5M1SfhLrX"},{"cell_type":"markdown","id":"32aab92c","metadata":{"id":"32aab92c"},"source":["# 🐍 Numba for CPU"]},{"cell_type":"markdown","id":"ec33a76a","metadata":{"id":"ec33a76a"},"source":["Monte Carlo Method to determine Pi.\n","\n","- Confirm the compiled version is behaving the same as the uncompiled version.\n","- Benchmark the uncompiled version.\n","- Benchmark the compiled version.\n","\n","Note: Numba saves the original Python implementation of the function in the **.py_func** attribute, so we can call the original Python code to make sure we get the same answer"]},{"cell_type":"code","execution_count":null,"id":"043af3e4","metadata":{"id":"043af3e4"},"outputs":[],"source":["from numba import jit\n","from numpy import testing\n","import random\n","\n","# Use the Numba compiler to compile this function\n","@jit(nopython=True)\n","def monte_carlo_pi(nsamples):\n","    acc = 0\n","    for i in range(nsamples):\n","        x = random.random()\n","        y = random.random()\n","        if (x**2 + y**2) < 1.0:\n","            acc += 1\n","    return 4.0 * acc / nsamples\n","\n","# Run the function\n","nsamples = 1000000\n","\n","# We will use numpy's `testing` library to confirm compiled and uncompiled versions run the same\n","testing.assert_almost_equal(monte_carlo_pi(nsamples), monte_carlo_pi.py_func(nsamples), decimal=2)"]},{"cell_type":"code","execution_count":null,"id":"da1e673b","metadata":{"id":"da1e673b"},"outputs":[],"source":["%timeit monte_carlo_pi(nsamples)"]},{"cell_type":"code","execution_count":null,"id":"7e2b5802","metadata":{"id":"7e2b5802"},"outputs":[],"source":["%timeit monte_carlo_pi.py_func(nsamples)"]},{"cell_type":"code","execution_count":null,"id":"1de59c01","metadata":{"id":"1de59c01"},"outputs":[],"source":["import numpy as np\n","\n","def my_sum(x, y):\n","    return x + y\n","\n","np_my_sum = np.frompyfunc(my_sum, 2, 1) # create ufunc\n","print(type(my_sum), type(np_my_sum))    # check types\n","print(my_sum(1, 1), np_my_sum(1, 1))    # check if the same"]},{"cell_type":"markdown","id":"7fde79c5","metadata":{"id":"7fde79c5"},"source":["### Define a ufunc using numba's vectorize..."]},{"cell_type":"code","execution_count":null,"id":"85d37cbc","metadata":{"id":"85d37cbc"},"outputs":[],"source":["from numba import vectorize\n","import numpy as np\n","\n","# Define a ufunc using numba's vectorize\n","@vectorize(['float64(float64, float64)'], target='cpu')\n","def multiply(x, y):\n","\treturn x * y\n","\n","# Test the ufunc\n","a = np.array([1.0, 2.0, 3.0, 4.0])\n","b = np.array([10.0, 20.0, 30.0, 40.0])\n","\n","result = multiply(a, b)\n","print(result)"]},{"cell_type":"code","execution_count":null,"id":"a3fedab5","metadata":{"id":"a3fedab5"},"outputs":[],"source":["import numpy as np\n","from numba import vectorize, int64, float32, float64\n","\n","# create default ufunc with datatypes conversion\n","@vectorize([int64(int64,int64), float32(float32,float32), float64(float64,float64)])\n","def numba_dtype_sum(x, y):\n","    return x + y\n","\n","print(type(numba_dtype_sum))  # check type\n","print(numba_dtype_sum(1, 1))  # check on scalars\n","print(numba_dtype_sum(np.ones(4), np.ones(4))) # check int arrays\n","print(numba_dtype_sum(np.random.rand(4), np.random.rand(4))) # check float arrays"]},{"cell_type":"markdown","id":"07e50f9d","metadata":{"id":"07e50f9d"},"source":["### Numba can parallelize loops using **parallel=True**..."]},{"cell_type":"code","execution_count":null,"id":"5707b316","metadata":{"id":"5707b316"},"outputs":[],"source":["from numba import njit, prange\n","import numpy as np\n","\n","@njit(parallel=True)\n","def parallel_sum(arr):\n","    total = 0\n","    for i in prange(len(arr)):  # Parallel execution\n","        total += arr[i]\n","    return total\n","\n","arr = np.random.rand(1000000)\n","print(parallel_sum(arr))"]},{"cell_type":"code","execution_count":null,"id":"4c806e08","metadata":{"id":"4c806e08"},"outputs":[],"source":["%timeit parallel_sum(arr)"]},{"cell_type":"code","execution_count":null,"id":"e8a510da","metadata":{"id":"e8a510da"},"outputs":[],"source":["%timeit arr.sum()"]},{"cell_type":"markdown","id":"4ac7368d","metadata":{"id":"4ac7368d"},"source":["### Type Specialization: Numba automatically specializes functions based on input types..."]},{"cell_type":"code","execution_count":null,"id":"27469f21","metadata":{"id":"27469f21"},"outputs":[],"source":["from numba import jit\n","\n","@jit(nopython=True)\n","def multiply(x, y):\n","   return x * y\n","\n","print(multiply(3, 4))      # Optimized for integers\n","print(multiply(3.5, 4.2))  # Optimized for floats\n"]},{"cell_type":"markdown","id":"c1807bca","metadata":{"id":"c1807bca"},"source":["### Execution time: compilation + execution..."]},{"cell_type":"code","execution_count":null,"id":"fd478be6","metadata":{"id":"fd478be6"},"outputs":[],"source":["from numba import jit\n","import numpy as np\n","import time\n","\n","\n","@jit(nopython=True)\n","def go_fast(a): # Function is compiled and runs in machine code\n","\ttrace = 0.0\n","\tfor i in range(a.shape[0]):\n","\t\ttrace += np.tanh(a[i])\n","\treturn trace\n","\n","x = np.arange(100000)\n","\n","# DO NOT REPORT THIS... COMPILATION TIME IS INCLUDED IN THE EXECUTION TIME!\n","start = time.time()\n","go_fast(x)\n","end = time.time()\n","print(\"Elapsed (with compilation) = %s\" % (end - start))\n","\n","# NOW THE FUNCTION IS COMPILED, RE-TIME IT EXECUTING FROM CACHE\n","start = time.time()\n","go_fast(x)\n","end = time.time()\n","print(\"Elapsed (after compilation) = %s\" % (end - start))\n"]},{"cell_type":"markdown","id":"654daecb","metadata":{"id":"654daecb"},"source":["# 🐍 Numba for GPU"]},{"cell_type":"markdown","id":"cb659914","metadata":{"id":"cb659914"},"source":["### Numba check and device detect..."]},{"cell_type":"code","execution_count":null,"id":"208066f9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"208066f9","executionInfo":{"status":"ok","timestamp":1747255238771,"user_tz":-120,"elapsed":13,"user":{"displayName":"Giuliano Grossi","userId":"05046904419500019805"}},"outputId":"e3ff248b-c2f1-446b-bcac-bcd0dee8c7bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["NumPy version: 2.0.2\n","Numba version: 0.60.0\n","CUDA driver version: (12, 4)\n","CUDA runtime version: (12, 5)\n","Found 1 CUDA devices\n","id 0             b'Tesla T4'                              [SUPPORTED]\n","                      Compute Capability: 7.5\n","                           PCI Device ID: 4\n","                              PCI Bus ID: 0\n","                                    UUID: GPU-51a7c953-ef53-32a7-00cd-14e39ff4f2e0\n","                                Watchdog: Disabled\n","             FP32/FP64 Performance Ratio: 32\n","Summary:\n","\t1/1 devices are supported\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}],"source":["import numba\n","from numba import cuda\n","import numpy as np\n","\n","print(f'NumPy version: {np.__version__}')\n","print(f'Numba version: {numba.__version__}')\n","print(f'CUDA driver version: {cuda.driver.get_version()}')\n","print(f'CUDA runtime version: {cuda.runtime.get_version()}')\n","\n","# device detect\n","cuda.detect()"]},{"cell_type":"markdown","id":"655752a3","metadata":{"id":"655752a3"},"source":["### Kernel configuration and definition..."]},{"cell_type":"code","execution_count":null,"id":"1dfc80c0","metadata":{"id":"1dfc80c0"},"outputs":[],"source":["import numpy as np\n","\n","# Define the kernel function\n","@cuda.jit\n","def increment_by_one(an_array):\n","\t# Thread id in a 1D block\n","\ttx = cuda.threadIdx.x\n","\t# Block id in a 1D grid\n","\tbx = cuda.blockIdx.x\n","\t# Block width, i.e. number of threads per block\n","\tbw = cuda.blockDim.x\n","\t# Compute flattened index inside the array\n","\tpos = tx + bx * bw\n","\tif pos < an_array.size:  # Check array boundaries\n","\t\tan_array[pos] += 1"]},{"cell_type":"code","execution_count":null,"id":"3e463d8b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3e463d8b","executionInfo":{"status":"ok","timestamp":1747255258201,"user_tz":-120,"elapsed":180,"user":{"displayName":"Giuliano Grossi","userId":"05046904419500019805"}},"outputId":"429d23aa-27bb-42a0-856c-9a68eeeb29c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["[1.7758104 1.4733319 1.2949641 1.6942185 1.5639066 1.6515328 1.7749755\n"," 1.5830696 1.9870557 1.0208046]\n"]}],"source":["# Create a random array\n","an_array = np.random.rand(1000000).astype(np.float32)\n","# Allocate device memory\n","d_an_array = cuda.to_device(an_array)\n","# Define the number of threads per block\n","threads_per_block = 256\n","# Define the number of blocks in the grid\n","blocks_per_grid = (an_array.size + (threads_per_block - 1)) // threads_per_block\n","# Launch the kernel\n","increment_by_one[blocks_per_grid, threads_per_block](d_an_array)\n","# Copy the result back to host\n","an_array = d_an_array.copy_to_host()\n","# Check the result\n","print(an_array[:10])  # Print first 10 elements to verify\n","\n"]},{"cell_type":"markdown","id":"a82ccc30","metadata":{"id":"a82ccc30"},"source":["# 🐍 Mat multiplication"]},{"cell_type":"markdown","id":"3aaee970","metadata":{"id":"3aaee970"},"source":["Multiplication..."]},{"cell_type":"code","execution_count":null,"id":"7e0ca18a","metadata":{"id":"7e0ca18a"},"outputs":[],"source":["from numba.types import float32\n","\n","BLOCK_SIZE = 32\n","\n","# kernel matrix multiplication\n","@cuda.jit\n","def matmul_gpu(A, B, C):\n","\t\"\"\"Perform square matrix multiplication of C = A * B.\"\"\"\n","\ti, j = cuda.grid(2)\n","\tif i < C.shape[0] and j < C.shape[1]:\n","\t\ttmp = 0.\n","\t\tfor k in range(A.shape[1]):\n","\t\t\ttmp += A[i, k] * B[k, j]\n","\t\tC[i, j] = tmp\n","\n","# kernel matrix multiplication with shared memory\n","@cuda.jit\n","def fast_matmul(A, B, C):\n","\t\"\"\"Perform square matrix multiplication of C = A * B using shared memory.\"\"\"\n","\t# Define an array in the shared memory\n","\tsA = cuda.shared.array(shape=(BLOCK_SIZE, BLOCK_SIZE), dtype=float32)\n","\tsB = cuda.shared.array(shape=(BLOCK_SIZE, BLOCK_SIZE), dtype=float32)\n","\n","\t# Calculate thread indices\n","\tx, y = cuda.grid(2)\n","\ttx = cuda.threadIdx.x\n","\tty = cuda.threadIdx.y\n","\tbpg = cuda.gridDim.x\n","\ttmp = float32(0.)\n","\n","\t# Each thread computes one element in the result matrix.\n","\tfor i in range(bpg):\n","\t\tsA[ty, tx] = 0\n","\t\tsB[ty, tx] = 0\n","\t\tif y < A.shape[0] and (tx + i * BLOCK_SIZE) < A.shape[1]:\n","\t\t\tsA[ty, tx] = A[y, tx + i * BLOCK_SIZE]\n","\t\tif x < B.shape[1] and (ty + i * BLOCK_SIZE) < B.shape[0]:\n","\t\t\tsB[ty, tx] = B[ty + i * BLOCK_SIZE, x]\n","\n","\t\t# Synchronize threads to ensure all data is loaded into shared memory\n","\t\tcuda.syncthreads()\n","\n","\t\t# Each thread computes one element in the result matrix.\n","\t\tfor j in range(BLOCK_SIZE):\n","\t\t\ttmp += sA[ty, j] * sB[j, tx]\n","\n","\t\t# Wait until all threads finish computing\n","\t\tcuda.syncthreads()\n","\n","\t# Write the result to global memory\n","\tif y < C.shape[0] and x < C.shape[1]:\n","\t\tC[y, x] = tmp"]},{"cell_type":"code","execution_count":null,"id":"cf3d1e4f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cf3d1e4f","executionInfo":{"status":"ok","timestamp":1747255452973,"user_tz":-120,"elapsed":677,"user":{"displayName":"Giuliano Grossi","userId":"05046904419500019805"}},"outputId":"975baeba-8a7c-46df-c51b-522fad6de2ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Matrix size: 8192x8192\n","Grid size: 256x256\n","Block size: 32x32\n"]}],"source":["import time\n","import numpy as np\n","\n","# generate random vals\n","np.random.seed(42)\n","SIZE = 1024*8\n","A = np.ones((SIZE,SIZE)).astype('float32')  # mat 1\n","B = np.ones((SIZE,SIZE)).astype('float32')  # mat 2\n","#C = np.zeros((SIZE,SIZE)).astype('float32')                       # mat where we store answer\n","\n","# transfer data to device\n","d_A = cuda.to_device(A) # Copy of A on the device\n","d_B = cuda.to_device(B) # Copy of B on the device\n","d_C = cuda.device_array_like(A) # malloc on the device\n","\n","# Define the number of threads in each block\n","block = (32, 32)  # each block will contain 32x32 threads, typically 128 - 512 threads/block\n","grid_x = int(np.ceil(A.shape[0] / block[0]))\n","grid_y = int(np.ceil(A.shape[1] / block[1]))\n","grid = (grid_x, grid_y)  # we calculate the gridsize (number of blocks) from array\n","print(f\"Matrix size: {SIZE}x{SIZE}\")\n","print(f\"Grid size: {grid_x}x{grid_y}\")\n","print(f\"Block size: {block[0]}x{block[1]}\")"]},{"cell_type":"code","execution_count":null,"id":"5330e195","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5330e195","executionInfo":{"status":"ok","timestamp":1747255307357,"user_tz":-120,"elapsed":21160,"user":{"displayName":"Giuliano Grossi","userId":"05046904419500019805"}},"outputId":"e5e0e8f8-2741-486f-9bec-0c46941fe91b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Elapsed (sec) = 21.116698265075684\n"]}],"source":["# execution of the kernel matmul_gpu\n","start = time.time()\n","matmul_gpu[grid, block](d_A, d_B, d_C)\n","# host and device sync\n","cuda.synchronize()\n","end = time.time()\n","print(\"Elapsed (sec) = %s\" % (end - start))"]},{"cell_type":"code","execution_count":null,"id":"03e2768d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"03e2768d","executionInfo":{"status":"ok","timestamp":1747255467408,"user_tz":-120,"elapsed":1786,"user":{"displayName":"Giuliano Grossi","userId":"05046904419500019805"}},"outputId":"20866644-229d-41f5-d2aa-ac3500f228c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Elapsed (sec) = 1.7897858619689941\n"]}],"source":["# execution of the kernel fast_matmul\n","start = time.time()\n","fast_matmul[grid, block](d_A, d_B, d_C)\n","# host and device sync\n","cuda.synchronize()\n","end = time.time()\n","print(\"Elapsed (sec) = %s\" % (end - start))"]},{"cell_type":"code","execution_count":null,"id":"d3a1d5f1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d3a1d5f1","executionInfo":{"status":"ok","timestamp":1747255479815,"user_tz":-120,"elapsed":9505,"user":{"displayName":"Giuliano Grossi","userId":"05046904419500019805"}},"outputId":"fcc56ada-de11-4ff5-f29b-9fbfd40e4017"},"outputs":[{"output_type":"stream","name":"stdout","text":["Elapsed (sec) = 9.402935266494751\n"]}],"source":["# using numpy function\n","start = time.time()\n","C = np.matmul(A, B)\n","end = time.time()\n","print(\"Elapsed (sec) = %s\" % (end - start))"]},{"cell_type":"code","execution_count":null,"id":"025f0275","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"025f0275","executionInfo":{"status":"ok","timestamp":1747255489064,"user_tz":-120,"elapsed":4347,"user":{"displayName":"Giuliano Grossi","userId":"05046904419500019805"}},"outputId":"6cdd063f-636c-4960-e6a2-3e2cad84e904"},"outputs":[{"output_type":"stream","name":"stdout","text":["Elapsed (sec) = 0.43741440773010254\n"]}],"source":["import torch\n","\n","# the first multiplication here is significantly slower\n","m1 = torch.from_numpy(A).cuda()\n","m2 = torch.from_numpy(B).cuda()\n","c = torch.zeros((m1.shape[0], m2.shape[1]), dtype=torch.float32).cuda()\n","\n","start = time.time()\n","torch.matmul(m1, m2, out=c)\n","torch.cuda.synchronize()\n","end = time.time()\n","print(\"Elapsed (sec) = %s\" % (end - start))"]},{"cell_type":"markdown","id":"67a5faae","metadata":{"id":"67a5faae"},"source":["# 🐍 Convolution 2D"]},{"cell_type":"markdown","id":"0f3a5ab3","metadata":{"id":"0f3a5ab3"},"source":["Parallel reduce..."]},{"cell_type":"code","execution_count":null,"id":"71c61c3b","metadata":{"id":"71c61c3b"},"outputs":[],"source":["import numpy as np\n","from numba import cuda\n","from numba.types import int32\n","\n","SHARED_DIM = 1024\n","\n","@cuda.jit\n","def reduce(data):\n","\ttid = cuda.threadIdx.x\n","\tsize = len(data)\n","\tif tid < size:\n","\t\ti = cuda.grid(1)\n","\n","\t\t# Declare an array in shared memory\n","\t\tshr = cuda.shared.array(SHARED_DIM, int32)\n","\t\tshr[tid] = data[i]\n","\n","\t\t# Ensure writes to shared memory are visible\n","\t\t# to all threads before reducing\n","\t\tcuda.syncthreads()\n","\n","\t\ts = 1\n","\t\twhile s < cuda.blockDim.x:\n","\t\t\tif tid % (2 * s) == 0:\n","\t\t\t\t\t# Stride by `s` and add\n","\t\t\t\t\tshr[tid] += shr[tid + s]\n","\t\t\ts *= 2\n","\t\t\tcuda.syncthreads()\n","\n","\t\t# After the loop, the zeroth  element contains the sum\n","\t\tif tid == 0:\n","\t\t\tdata[tid] = shr[tid]"]},{"cell_type":"code","execution_count":null,"id":"12869042","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"12869042","executionInfo":{"status":"ok","timestamp":1747255696458,"user_tz":-120,"elapsed":52,"user":{"displayName":"Giuliano Grossi","userId":"05046904419500019805"}},"outputId":"ab81e6b0-b0dd-4f94-892c-226892999355"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of elements:  1024\n","523776\n","523776\n"]}],"source":["# generate data\n","nelem = SHARED_DIM\n","a = cuda.to_device(np.arange(nelem, dtype=np.int32))\n","print(\"Number of elements: \", len(a))\n","\n","# kernel\n","reduce[1, nelem](a)\n","\n","# copy to host\n","b = a.copy_to_host()\n","\n","print(b[0])  # 523776\n","print(sum(np.arange(nelem)))  # 523776"]},{"cell_type":"markdown","id":"bd53530d","metadata":{"id":"bd53530d"},"source":["↩ SOLUTION: Convolution 2D..."]},{"cell_type":"code","execution_count":null,"id":"b0442866","metadata":{"id":"b0442866"},"outputs":[],"source":["import numpy as np\n","from numba import cuda\n","\n","# Define kernel size\n","KERNEL_SIZE = 3\n","BLOCK_SIZE = 16\n","TILE = (BLOCK_SIZE + KERNEL_SIZE - 1, BLOCK_SIZE + KERNEL_SIZE - 1)\n","\n","@cuda.jit\n","def conv2d_kernel(image, kernel, output):\n","\t# SMEM\n","\tshared_image = cuda.shared.array(shape=TILE, dtype=numba.float32)\n","\n","\tx, y = cuda.grid(2)\n","\ttx, ty = cuda.threadIdx.x, cuda.threadIdx.y\n","\tbx, by = cuda.blockIdx.x * BLOCK_SIZE, cuda.blockIdx.y * BLOCK_SIZE\n","\n","\t# Load into shared memory\n","\tfor i in range(KERNEL_SIZE):\n","\t\tfor j in range(KERNEL_SIZE):\n","\t\t\tx_offset = bx + tx - KERNEL_SIZE // 2 + i\n","\t\t\ty_offset = by + ty - KERNEL_SIZE // 2 + j\n","\n","\t\t\tif 0 <= x_offset < image.shape[0] and 0 <= y_offset < image.shape[1]:\n","\t\t\t\tshared_image[tx + i, ty + j] = image[x_offset, y_offset]\n","\t\t\telse:\n","\t\t\t\tshared_image[tx + i, ty + j] = 0  # Padding\n","\n","\tcuda.syncthreads()\n","\n","\t# Compute convolution\n","\tif x < output.shape[0] and y < output.shape[1]:\n","\t\tsum_val = 0.0\n","\t\tfor i in range(KERNEL_SIZE):\n","\t\t\tfor j in range(KERNEL_SIZE):\n","\t\t\t\tsum_val += shared_image[tx + i, ty + j] * kernel[i, j]\n","\n","\t\toutput[x, y] = sum_val"]},{"cell_type":"markdown","id":"682d5a0e","metadata":{"id":"682d5a0e"},"source":["↩ TODO: Convolution 2D..."]},{"cell_type":"code","execution_count":null,"id":"e20a19a5","metadata":{"id":"e20a19a5"},"outputs":[],"source":["import numpy as np\n","from numba import cuda\n","\n","# Define kernel size\n","KERNEL_SIZE = 3\n","BLOCK_SIZE = 16\n","TILE = (BLOCK_SIZE + KERNEL_SIZE - 1, BLOCK_SIZE + KERNEL_SIZE - 1)\n","\n","@cuda.jit\n","def conv2d_kernel(image, kernel, output):\n","\t# TODO: Implement the kernel function\n"]},{"cell_type":"code","execution_count":null,"id":"bec79011","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bec79011","executionInfo":{"status":"ok","timestamp":1747255744670,"user_tz":-120,"elapsed":1087,"user":{"displayName":"Giuliano Grossi","userId":"05046904419500019805"}},"outputId":"9d5cab79-5fd1-4c00-89a0-0dbdd91a0c90"},"outputs":[{"output_type":"stream","name":"stdout","text":["image.shape:  (8192, 8192)\n","kernel.shape: (3, 3)\n","Elapsed (sec) = 0.25177836418151855\n","output.shape: (8192, 8192)\n"]}],"source":["# Test the kernel\n","image = np.random.rand(8*1024, 8*1024).astype(np.float32)  # Example image\n","kernel = np.random.rand(KERNEL_SIZE, KERNEL_SIZE).astype(np.float32)  # Example Gaussian Blur kernel\n","print('image.shape: ',image.shape)\n","print('kernel.shape:', kernel.shape)\n","\n","# copy data H2D\n","d_image = cuda.to_device(image)\n","d_kernel = cuda.to_device(kernel)\n","d_output = cuda.device_array(image.shape, dtype=np.float32)\n","\n","# grdi and block dim\n","block_dim = (BLOCK_SIZE, BLOCK_SIZE)\n","grid_dim = (image.shape[0] // BLOCK_SIZE, image.shape[1] // BLOCK_SIZE)\n","\n","# kernel\n","start = time.time()\n","conv2d_kernel[grid_dim, block_dim](d_image, d_kernel, d_output)\n","cuda.synchronize()\n","end = time.time()\n","print(\"Elapsed (sec) = %s\" % (end - start))\n","output = d_output.copy_to_host()\n","print('output.shape:', output.shape)"]},{"cell_type":"code","execution_count":null,"id":"edf133f6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"edf133f6","executionInfo":{"status":"ok","timestamp":1747255756514,"user_tz":-120,"elapsed":2864,"user":{"displayName":"Giuliano Grossi","userId":"05046904419500019805"}},"outputId":"c56095af-b600-444e-a384-ea5228a13a54"},"outputs":[{"output_type":"stream","name":"stdout","text":["Elapsed (sec) = 2.2868008613586426\n"]}],"source":["from scipy import signal\n","\n","start = time.time()\n","grad = signal.convolve2d(image, kernel)\n","end = time.time()\n","print(\"Elapsed (sec) = %s\" % (end - start))"]},{"cell_type":"markdown","id":"acef8f2e","metadata":{"id":"acef8f2e"},"source":["# 🐍 Atomic functions"]},{"cell_type":"code","execution_count":null,"id":"4b7459ab","metadata":{"id":"4b7459ab"},"outputs":[],"source":["import numpy as np\n","from numba import cuda\n","\n","@cuda.jit\n","def find_max(result, values):\n","\t\"\"\"Find the maximum value in values and store in result[0]\"\"\"\n","\ttid = cuda.threadIdx.x\n","\tbid = cuda.blockIdx.x\n","\tbdim = cuda.blockDim.x\n","\ti = (bid * bdim) + tid\n","\tcuda.atomic.max(result, 0, values[i])\n","\n","#### RUNNING THE KERNEL ####\n","arr = np.random.rand(1024*1024).astype(np.float32)  # Example array\n","d_arr = cuda.to_device(arr)  # Copy to device\n","result = np.zeros(1, dtype=np.float64)\n","d_result = cuda.to_device(result)  # Copy to device\n","find_max[1024,1024](d_result, d_arr)\n","print(d_result[0]) # Found using cuda.atomic.max\n","print(max(arr))  # Print max(arr) for comparison (should be equal!)"]},{"cell_type":"markdown","id":"fab59d61","metadata":{"id":"fab59d61"},"source":["Write an Accelerated Histogramming Kernel:\n","\n","For this assessment, you will create an accelerated histogramming kernel. This will take an array of input data, a range, and a number of bins, and count how many of the input data elements land in each bin"]},{"cell_type":"code","execution_count":null,"id":"828f1baf","metadata":{"id":"828f1baf"},"outputs":[],"source":["def cpu_histogram(x, xmin, xmax, histogram_out):\n","\t'''Increment bin counts in histogram_out, given histogram range [xmin, xmax).'''\n","\t# Note that we don't have to pass in nbins explicitly, because the size of histogram_out determines it\n","\tnbins = histogram_out.shape[0]\n","\tbin_width = (xmax - xmin) / nbins\n","\n","\t# This is a very slow way to do this with NumPy, but looks similar to what you will do on the GPU\n","\tfor element in x:\n","\t\tbin_number = np.int32((element - xmin)/bin_width)\n","\t\tif bin_number >= 0 and bin_number < histogram_out.shape[0]:\n","\t\t\t# only increment if in range\n","\t\t\thistogram_out[bin_number] += 1"]},{"cell_type":"code","execution_count":null,"id":"13f75909","metadata":{"id":"13f75909"},"outputs":[],"source":["x = np.random.normal(size=10000, loc=0, scale=1).astype(np.float32)\n","xmin = np.float32(-4.0)\n","xmax = np.float32(4.0)\n","histogram_out = np.zeros(shape=10, dtype=np.int32)\n","\n","cpu_histogram(x, xmin, xmax, histogram_out)\n","\n","histogram_out"]},{"cell_type":"markdown","metadata":{"id":"np1ksJNcn-RK"},"source":["↩ SOL: Convolution 2D..."],"id":"np1ksJNcn-RK"},{"cell_type":"code","execution_count":null,"id":"15a3ba68","metadata":{"id":"15a3ba68"},"outputs":[],"source":["@cuda.jit\n","def cuda_histogram(x, xmin, xmax, histogram_out):\n","\t'''Increment bin counts in histogram_out, given histogram range [xmin, xmax).'''\n","\n","\tnbins = histogram_out.shape[0]\n","\tbin_width = (xmax - xmin) / nbins\n","\n","\tstart = cuda.grid(1)\n","\tstride=cuda.gridsize(1)\n","\n","\tfor i in range(start, x.shape[0], stride):\n","\t\tbin_number = np.int32((x[i] - xmin)/bin_width)\n","\t\tif bin_number >= 0 and bin_number < histogram_out.shape[0]:\n","\t\t\t# only increment if in range\n","\t\t\tcuda.atomic.add(histogram_out, bin_number, 1)"]},{"cell_type":"markdown","metadata":{"id":"A1Ibo9v6oA3_"},"source":["↩ TODO: Convolution 2D..."],"id":"A1Ibo9v6oA3_"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ddTpDuVZoEIK"},"outputs":[],"source":["@cuda.jit\n","def cuda_histogram(x, xmin, xmax, histogram_out):\n","\n","\t# TODO"],"id":"ddTpDuVZoEIK"},{"cell_type":"code","execution_count":null,"id":"07f1a348","metadata":{"id":"07f1a348"},"outputs":[],"source":["d_x = cuda.to_device(x)\n","d_histogram_out = cuda.to_device(np.zeros(shape=10, dtype=np.int32))\n","\n","blocks = 128\n","threads_per_block = 64\n","start = time.time()\n","cuda_histogram[blocks, threads_per_block](d_x, xmin, xmax, d_histogram_out)\n","\n","histogram_out = d_histogram_out.copy_to_host()\n","print(histogram_out)  # Print histogram\n","print(histogram_out.sum())  # Print sum of histogram\n","print(histogram_out.sum() == x.shape[0])  # Check if sum of histogram equals number of elements in x"]},{"cell_type":"code","execution_count":null,"id":"9701607b","metadata":{"id":"9701607b"},"outputs":[],"source":["# Define host array\n","threads_per_block = 256\n","blocks_per_grid = 32 * 40\n","a = np.ones(10_000_000, dtype=np.float32)\n","print(f\"Old sum: {a.sum():.2f}\")"]},{"cell_type":"markdown","id":"51f4a340","metadata":{"id":"51f4a340"},"source":["# 🐍 Streams"]},{"cell_type":"code","execution_count":null,"id":"1060a35c","metadata":{"id":"1060a35c"},"outputs":[],"source":["# Numba CUDA Stream Semantics\n","@cuda.jit\n","def partial_reduce(array, partial_reduction):\n","    i_start = cuda.grid(1)\n","    threads_per_grid = cuda.blockDim.x * cuda.gridDim.x\n","    s_thread = 0.0\n","    for i_arr in range(i_start, array.size, threads_per_grid):\n","        s_thread += array[i_arr]\n","\n","    s_block = cuda.shared.array((threads_per_block,), numba.float32)\n","    tid = cuda.threadIdx.x\n","    s_block[tid] = s_thread\n","    cuda.syncthreads()\n","\n","    i = cuda.blockDim.x // 2\n","    while (i > 0):\n","        if (tid < i):\n","            s_block[tid] += s_block[tid + i]\n","        cuda.syncthreads()\n","        i //= 2\n","\n","    if tid == 0:\n","        partial_reduction[cuda.blockIdx.x] = s_block[0]\n","\n","@cuda.jit\n","def single_thread_sum(partial_reduction, sum):\n","\tsum[0] = 0.0\n","\tfor element in partial_reduction:\n","\t\tsum[0] += element\n","\n","@cuda.jit\n","def divide_by(array, val_array):\n","\ti_start = cuda.grid(1)\n","\tthreads_per_grid = cuda.gridsize(1)\n","\tfor i in range(i_start, array.size, threads_per_grid):\n","\t\tarray[i] /= val_array[0]\n","\n","# Pin memory\n","with cuda.pinned(a):\n","\t# Create a CUDA stream\n","\tstream = cuda.stream()\n","\n","\t# Array copy to device and creation in the device\n","\tdev_a = cuda.to_device(a, stream=stream)\n","\tdev_a_reduce = cuda.device_array((blocks_per_grid,), dtype=dev_a.dtype, stream=stream)\n","\tdev_a_sum = cuda.device_array((1,), dtype=dev_a.dtype, stream=stream)\n","\n","\t# configuration, and it comes after the block dimension (`threads_per_block`)\n","\tpartial_reduce[blocks_per_grid, threads_per_block, stream](dev_a, dev_a_reduce)\n","\tsingle_thread_sum[1, 1, stream](dev_a_reduce, dev_a_sum)\n","\tdivide_by[blocks_per_grid, threads_per_block, stream](dev_a, dev_a_sum)\n","\n","\t# Array copy to host: like the copy to device, when a stream is passed, the copy\n","\t# is asynchronous. Note: the printed output will probably be nonsensical since\n","\t# the write has not been synchronized yet.\n","\tdev_a.copy_to_host(a, stream=stream)\n","\n","# Whenever we want to ensure that all operations in a stream are finished from\n","# the point of view of the host, we call:\n","stream.synchronize()\n","\n","# After that call, we can be sure that `a` has been overwritten with its normalized version\n","print(f\"New sum: {a.sum():.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"5UgcACiqoWXG"},"source":["↩ TODO: Convolution 2D..."],"id":"5UgcACiqoWXG"},{"cell_type":"code","execution_count":null,"id":"76aeee6e","metadata":{"id":"76aeee6e"},"outputs":[],"source":["# Multiple streams\n","from time import perf_counter\n","\n","N_streams = 10\n","\n","# Do not memory-collect (deallocate arrays) within this context\n","with cuda.defer_cleanup():\n","\t# Create 10 streams\n","\tstreams = [cuda.stream() for _ in range(1, N_streams + 1)]\n","\n","\t# Create base arrays\n","\tarrays = [i * np.ones(10_000_000, dtype=np.float32) for i in range(1, N_streams + 1)]\n","\n","\tfor i, arr in enumerate(arrays):\n","\t\tprint(f\"sum array {i}: {arr.sum():12.2f}\")\n","\n","\ttics = []  # Launch start times\n","\tfor i, (stream, arr) in enumerate(zip(streams, arrays)):\n","\t\ttic = perf_counter()\n","\t\twith cuda.pinned(arr):\n","\t\t\tdev_a = cuda.to_device(arr, stream=stream)\n","\t\t\tdev_a_reduce = cuda.device_array((blocks_per_grid,), dtype=dev_a.dtype, stream=stream)\n","\t\t\tdev_a_sum = cuda.device_array((1,), dtype=dev_a.dtype, stream=stream)\n","\n","\t\t\tpartial_reduce[blocks_per_grid, threads_per_block, stream](dev_a, dev_a_reduce)\n","\t\t\tsingle_thread_sum[1, 1, stream](dev_a_reduce, dev_a_sum)\n","\t\t\tdivide_by[blocks_per_grid, threads_per_block, stream](dev_a, dev_a_sum)\n","\n","\t\t\tdev_a.copy_to_host(arr, stream=stream)\n","\n","\t\ttoc = perf_counter()  # Stop time of launches\n","\t\tprint(f\"Launched processing {i} in {1e3 * (toc - tic):.2f} ms\")\n","\n","\t\t# Ensure that the reference to the GPU arrays are deleted, this will\n","\t\t# ensure garbage collection at the exit of the context.\n","\t\tdel dev_a, dev_a_reduce, dev_a_sum\n","\n","\t\ttics.append(tic)\n","\n","\ttocs = []\n","\tfor i, (stream, arr) in enumerate(zip(streams, arrays)):\n","\t\tstream.synchronize()\n","\t\ttoc = perf_counter()  # Stop time of sync\n","\t\ttocs.append(toc)\n","\t\tprint(f\"New sum (array {i}): {arr.sum():12.2f}\")\n","\tfor i in range(4):\n","\t\tprint(f\"Performed processing {i} in {1e3 * (tocs[i] - tics[i]):.2f} ms\")\n","\n","\tprint(f\"Total time {1e3 * (tocs[-1] - tics[0]):.2f} ms\")"]},{"cell_type":"markdown","metadata":{"id":"Ma6zmyAmofN7"},"source":["↩ SOLUTION: Convolution 2D...\n","\n"],"id":"Ma6zmyAmofN7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"yxe-HvR-ojmt"},"outputs":[],"source":["# Multiple streams\n","from time import perf_counter\n","\n","N_streams = 10\n","\n","# Do not memory-collect (deallocate arrays) within this context\n","with cuda.defer_cleanup():\n","\t# Create 10 streams\n","\tstreams = [cuda.stream() for _ in range(1, N_streams + 1)]\n","\n","\t# Create base arrays\n","\tarrays = [i * np.ones(10_000_000, dtype=np.float32) for i in range(1, N_streams + 1)]\n","\n","\tfor i, arr in enumerate(arrays):\n","\t\tprint(f\"sum array {i}: {arr.sum():12.2f}\")\n","\n","\ttics = []  # Launch start times\n","\tfor i, (stream, arr) in enumerate(zip(streams, arrays)):\n","\t\ttic = perf_counter()\n","\t\twith cuda.pinned(arr):\n","\t\t\tdev_a = cuda.to_device(arr, stream=stream)\n","\t\t\tdev_a_reduce = cuda.device_array((blocks_per_grid,), dtype=dev_a.dtype, stream=stream)\n","\t\t\tdev_a_sum = cuda.device_array((1,), dtype=dev_a.dtype, stream=stream)\n","\n","\t\t\tpartial_reduce[blocks_per_grid, threads_per_block, stream](dev_a, dev_a_reduce)\n","\t\t\tsingle_thread_sum[1, 1, stream](dev_a_reduce, dev_a_sum)\n","\t\t\tdivide_by[blocks_per_grid, threads_per_block, stream](dev_a, dev_a_sum)\n","\n","\t\t\tdev_a.copy_to_host(arr, stream=stream)\n","\n","\t\ttoc = perf_counter()  # Stop time of launches\n","\t\tprint(f\"Launched processing {i} in {1e3 * (toc - tic):.2f} ms\")\n","\n","\t\t# Ensure that the reference to the GPU arrays are deleted, this will\n","\t\t# ensure garbage collection at the exit of the context.\n","\t\tdel dev_a, dev_a_reduce, dev_a_sum\n","\n","\t\ttics.append(tic)\n","\n","\ttocs = []\n","\tfor i, (stream, arr) in enumerate(zip(streams, arrays)):\n","\t\tstream.synchronize()\n","\t\ttoc = perf_counter()  # Stop time of sync\n","\t\ttocs.append(toc)\n","\t\tprint(f\"New sum (array {i}): {arr.sum():12.2f}\")\n","\tfor i in range(4):\n","\t\tprint(f\"Performed processing {i} in {1e3 * (tocs[i] - tics[i]):.2f} ms\")\n","\n","\tprint(f\"Total time {1e3 * (tocs[-1] - tics[0]):.2f} ms\")"],"id":"yxe-HvR-ojmt"},{"cell_type":"markdown","id":"1ddb94a0","metadata":{"id":"1ddb94a0"},"source":["# 🐍 Pseudo-random generator"]},{"cell_type":"code","execution_count":null,"id":"05b68138","metadata":{"id":"05b68138"},"outputs":[],"source":["from numba.cuda.random import create_xoroshiro128p_states, xoroshiro128p_uniform_float32\n","\n","@cuda.jit\n","def compute_pi(rng_states, iterations, out):\n","\t\"\"\"Find the maximum value in values and store in result[0]\"\"\"\n","\ttid = cuda.grid(1)\n","\n","\t# Compute pi by drawing random (x, y) points and finding what\n","\t# fraction lie inside a unit circle\n","\tinside = 0\n","\tfor i in range(iterations):\n","\t\tx = xoroshiro128p_uniform_float32(rng_states, tid)\n","\t\ty = xoroshiro128p_uniform_float32(rng_states, tid)\n","\t\tif x**2 + y**2 <= 1.0:\n","\t\t\tinside += 1\n","\n","\tout[tid] = 4.0 * inside / iterations"]},{"cell_type":"code","execution_count":null,"id":"959d9bd8","metadata":{"id":"959d9bd8"},"outputs":[],"source":["# params\n","n_iter = 100000     # Number of iterations per thread\n","threads_per_block = 1024\n","blocks = 1024\n","\n","# Create random number generator states\n","rng_states = create_xoroshiro128p_states(threads_per_block * blocks, seed=1)\n","out = np.zeros(threads_per_block * blocks, dtype=np.float32)\n","d_output = cuda.to_device(out)\n","d_rng_states = cuda.to_device(rng_states)\n","\n","# Launch the kernel\n","compute_pi[blocks, threads_per_block](d_rng_states, n_iter, d_output)\n","\n","# Copy the result back to host\n","out = d_output.copy_to_host()\n","print('pi:', out.mean())\n","print('error:', np.abs(np.pi-out.mean()))"]},{"cell_type":"markdown","metadata":{"id":"k1Kr1_TQo73C"},"source":["↩ SOLUTION: Convolution 2D..."],"id":"k1Kr1_TQo73C"},{"cell_type":"code","execution_count":null,"id":"5ec2f345","metadata":{"id":"5ec2f345"},"outputs":[],"source":["import math\n","from numba import cuda\n","\n","@cuda.jit\n","def Gauss_GPU(rng_states, iterations, out, a, b):\n","\t\"\"\"Find the maximum value in values and store in result[0]\"\"\"\n","\ttid = cuda.grid(1)\n","\n","\t# Compute prob by drawing random (x, y) points and finding what fraction lie inside the area under the curve\n","\tinside = 0\n","\tfor i in range(iterations):\n","\t\tx = (b-a) * xoroshiro128p_uniform_float32(rng_states, tid) + a\n","\t\ty = xoroshiro128p_uniform_float32(rng_states, tid)\n","\t\tif y <= math.exp(-x*x/2):\n","\t\t\tinside += 1\n","\n","\tout[tid] = inside / iterations\n"]},{"cell_type":"markdown","metadata":{"id":"l1p7zLJ6pBJ7"},"source":["↩ TODO: Convolution 2D..."],"id":"l1p7zLJ6pBJ7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"GH6V7uC3pC00"},"outputs":[],"source":["import math\n","from numba import cuda\n","\n","@cuda.jit\n","def Gauss_GPU(rng_states, iterations, out, a, b):\n","\t\"\"\"Find the maximum value in values and store in result[0]\"\"\"\n","\ttid = cuda.grid(1)\n","\n","\t# Compute prob by drawing random (x, y) points and finding what fraction lie inside the area under the curve\n","\tinside = 0\n","\tfor i in range(iterations):\n","\t\tx = (b-a) * xoroshiro128p_uniform_float32(rng_states, tid) + a\n","\t\ty = xoroshiro128p_uniform_float32(rng_states, tid)\n","\t\tif y <= math.exp(-x*x/2):\n","\t\t\tinside += 1\n","\n","\tout[tid] = inside / iterations\n"],"id":"GH6V7uC3pC00"},{"cell_type":"code","execution_count":null,"id":"2ff062d5","metadata":{"id":"2ff062d5"},"outputs":[],"source":["\n","n_iter = 100000     # Number of iterations per thread\n","threads_per_block = 1024\n","blocks = 1024\n","a = -1\n","b = 2\n","\n","# Create random number generator states\n","rng_states = create_xoroshiro128p_states(threads_per_block * blocks, seed=1)\n","out = np.zeros(threads_per_block * blocks, dtype=np.float32)\n","d_output = cuda.to_device(out)\n","d_rng_states = cuda.to_device(rng_states)\n","\n","# Launch the kernel\n","Gauss_GPU[blocks, threads_per_block](d_rng_states, n_iter, d_output, a, b)\n","\n","# Copy the result back to host\n","out = d_output.copy_to_host()\n","print('prob:', out.mean())\n"]},{"cell_type":"markdown","id":"42df7879","metadata":{"id":"42df7879"},"source":["# 🐍 Mandelbrot"]},{"cell_type":"code","execution_count":null,"id":"9cbcb0cf","metadata":{"id":"9cbcb0cf"},"outputs":[],"source":["import math\n","\n","@cuda.jit\n","def mandelbrot_gpu(mat, maxiter=100, xmin=-2.6, xmax=1.85, ymin=-1.25, ymax=1.25):\n","\tx = cuda.blockIdx.x\n","\ty = cuda.threadIdx.x\n","\n","\t# Mapping pixel to C\n","\tcreal = xmin + x / mat.shape[0] * (xmax - xmin)\n","\tcim = ymin + y / mat.shape[1] * (ymax - ymin)\n","\n","\t# Initialisation of C and Z\n","\tc = complex(creal, cim)\n","\tz = complex(0, 0)\n","\n","\t# Mandelbrot iteration\n","\tfor n in range(maxiter):\n","\t\tz = z*z+c\n","\t\t# If unbounded: save iteration count and break\n","\t\tif z.real*z.real + z.imag*z.imag > 4.0:\n","\t\t\t# Smooth iteration count\n","\t\t\tmat[x,y] = n + 1 - math.log(math.log(abs(z*z+c)))/math.log(2)\n","\t\t\tbreak\n","\t\t# Otherwise: leave it to 0"]},{"cell_type":"code","execution_count":null,"id":"6b88bf03","metadata":{"id":"6b88bf03"},"outputs":[],"source":["# Parameters\n","xmin, xmax = -2.6, 1.85\n","ymin, ymax = -1.25, 1.25\n","xpixels = 512\n","ypixels = round(xpixels / (xmax-xmin) * (ymax-ymin))\n","\n","maxiter = 100\n","mat = np.zeros((xpixels, ypixels))\n","# Allocate device memory\n","d_mat = cuda.to_device(mat)\n","\n","# Running and plotting result\n","mandelbrot_gpu[xpixels, ypixels](d_mat, maxiter, xmin, xmax, ymin, ymax)\n","# Copy the result back to host\n","mat = d_mat.copy_to_host()\n","print(mat.shape)\n"]},{"cell_type":"code","execution_count":null,"id":"2ea66cba","metadata":{"id":"2ea66cba"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib as cm\n","\n","def draw_image(mat, cmap='inferno', powern=0.5, dpi=72):\n","  ## Value normalization\n","  # Apply power normalization, because number of iteration is\n","  # distributed according to a power law (fewer pixels have\n","  # higher iteration number)\n","  mat = np.power(mat, powern)\n","\n","  # Colormap: set the color the black for values under vmin (inner points of\n","  # the set), vmin will be set in the imshow function\n","  new_cmap = cm.colormaps[cmap]\n","  new_cmap.set_under('black')\n","\n","  ## Plotting image\n","\n","  # Figure size\n","  plt.figure(figsize=(mat.shape[0]/dpi, mat.shape[1]/dpi))\n","\n","  # Plotting mat with cmap\n","  # vmin=1 because smooth iteration count is always > 1\n","  # We need to transpose mat because images use row-major\n","  # ordering (C convention)\n","  # origin='lower' because mat[0,0] is the lower left pixel\n","  plt.imshow(mat.T, cmap=new_cmap, vmin=1, origin = 'lower')\n","\n","  # Remove axis and margins\n","  plt.subplots_adjust(left=0, right=1, bottom=0, top=1)\n","  plt.axis('off')\n","\n","\n","draw_image(mat)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.15"},"colab":{"provenance":[],"collapsed_sections":["-_i3qX0HhLrV","32aab92c","654daecb","cb659914","a82ccc30","67a5faae","acef8f2e","51f4a340","1ddb94a0","42df7879"],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}