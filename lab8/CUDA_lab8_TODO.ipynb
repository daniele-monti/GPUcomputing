{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1u--4d-M64C0XvN_8eucq1LQ6G_rYWdk9","timestamp":1746652865706}],"collapsed_sections":["NO_C5o9-xRF_","lAVvcKOX_DU0","vXUIQkZLCTcG","f7Us14vqivur","v2NIkXGQgsOb","SOFMQZAkjlLW"],"mount_file_id":"1mk0QXjREp-k_J-pdFfmgoC7-EVmgPyAo","authorship_tag":"ABX9TyOn09ccB4L7Jw6bstC2Sdms"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["---\n","# **LAB 8 - CUDA Streams**\n","---"],"metadata":{"id":"fZYqN0UwVLC_"}},{"cell_type":"markdown","metadata":{"id":"NO_C5o9-xRF_"},"source":["# ▶️ CUDA setup"]},{"cell_type":"code","metadata":{"id":"8fekR2O4xRGE"},"source":["!nvcc --version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Psl9iouxRGE"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GPU computing notebooks download (from github)"],"metadata":{"id":"gcg1GyK5srek"}},{"cell_type":"code","source":["!git clone https://github.com/giulianogrossi/GPUcomputing.git"],"metadata":{"id":"tyHOxci3s3H8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"227eLdP5csN1"},"source":["NVCC Plugin for Jupyter notebook"]},{"cell_type":"code","source":["%cd GPUcomputing/utils/nvcc4jupyter-master/\n","!!python3 -m build\n","%load_ext nvcc4jupyter\n","%cd /content/"],"metadata":{"id":"4TzxMBFds8aT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lAVvcKOX_DU0"},"source":["# ✅ Somma array con stream"]},{"cell_type":"code","metadata":{"id":"OH6TuWnB-_0M"},"source":["%%cuda_group_save --name \"sumArrayStream.cu\" --group \"lez8\"\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","\n","#define NSTREAM 8\n","#define BDIM 1024\n","\n","// function prototypes\n","void initialData(float *ip, int size);\n","void sumArraysOnHost(float *A, float *B, float *C, const int N);\n","void checkResult(float *hostRef, float *gpuRef, const int N);\n","\n","/**\n"," * Kernel to add the N elements of two arrays\n"," * C = A + B\n"," */\n","__global__ void sumArrays(float *A, float *B, float *C, const int N) {\n","  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","  if (idx < N)\n","    C[idx] = A[idx] + B[idx];\n","}\n","\n","/**\n"," * Main program\n"," */\n","int main(int argc, char **argv) {\n","  printf(\"Starting...\\n\");\n","\n","  // set up data size of vectors\n","  int nElem = 1 << 26;\n","  printf(\"   vector size = %d\\n\", nElem);\n","  size_t nBytes = nElem * sizeof(float);\n","  printf (\"   with streams = %d\\n\", NSTREAM);\n","\n","  // malloc pinned host memory for async memcpy\n","  float *h_A, *h_B, *hostRef, *gpuRef;\n","  CHECK(cudaHostAlloc((void**)&h_A, nBytes, cudaHostAllocDefault));\n","  CHECK(cudaHostAlloc((void**)&h_B, nBytes, cudaHostAllocDefault));\n","  CHECK(cudaHostAlloc((void**)&gpuRef, nBytes, cudaHostAllocDefault));\n","  CHECK(cudaHostAlloc((void**)&hostRef, nBytes, cudaHostAllocDefault));\n","\n","  // initialize data at host side\n","  initialData(h_A, nElem);\n","  initialData(h_B, nElem);\n","  memset(hostRef, 0, nBytes);\n","  memset(gpuRef,  0, nBytes);\n","\n","  /***************************************************\n","  *                  Host side                       *\n","  ****************************************************/\n","  printf(\"\\nHost compute...\\n\");\n","  double start = seconds();\n","  sumArraysOnHost(h_A, h_B, hostRef, nElem);\n","  double cpu_time = seconds() - start;\n","  printf(\"   CPU elapsed time: %.5f (sec)\\n\", cpu_time);\n","\n","  // malloc device global memory\n","  float *d_A, *d_B, *d_C;\n","  CHECK(cudaMalloc(&d_A, nBytes));\n","  CHECK(cudaMalloc(&d_B, nBytes));\n","  CHECK(cudaMalloc(&d_C, nBytes));\n","\n","  /***************************************************\n","  *                Default stream                    *\n","  ****************************************************/\n","  printf(\"\\nDefault stream...\\n\");\n","  dim3 block (BDIM);\n","  dim3 grid  ((nElem + block.x - 1) / block.x);\n","\n","  // mem copy data from host to device & run kernel\n","  start = seconds();\n","  CHECK(cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice));\n","  CHECK(cudaMemcpy(d_B, h_B, nBytes, cudaMemcpyHostToDevice));\n","  sumArrays<<<grid, block>>>(d_A, d_B, d_C, nElem);\n","  CHECK(cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost));\n","  CHECK(cudaDeviceSynchronize());\n","  double gpu_time = seconds() - start;\n","  printf(\"   GPU elapsed time (default stream): %.5f (sec)- speedup %.1f\\n\", gpu_time, cpu_time/gpu_time);\n","\n","  /***************************************************\n","  *                 multi-stream                     *\n","  ****************************************************/\n","  printf(\"\\nMulti stream...\\n\");\n","  int iElem = nElem / NSTREAM;\n","  size_t iBytes = iElem * sizeof(float);\n","  grid.x = (iElem + block.x - 1) / block.x;\n","\n","  // create streams\n","  start = seconds();\n","  cudaStream_t stream[NSTREAM];\n","  for (int i = 0; i < NSTREAM; ++i)\n","    CHECK(cudaStreamCreate(&stream[i]));\n","\n","  // initiate all asynchronous transfers to the device\n","  for (int i = 0; i < NSTREAM; ++i) {\n","  int ioffset = i * iElem;\n","  CHECK(cudaMemcpyAsync(&d_A[ioffset], &h_A[ioffset], iBytes, cudaMemcpyHostToDevice, stream[i]));\n","  CHECK(cudaMemcpyAsync(&d_B[ioffset], &h_B[ioffset], iBytes, cudaMemcpyHostToDevice, stream[i]));\n","\n","  // launch a kernel in each stream\n","  sumArrays<<<grid, block, 0, stream[i]>>>(&d_A[ioffset], &d_B[ioffset], &d_C[ioffset], iElem);\n","\n","  // enqueue asynchronous transfers from the device\n","  CHECK(cudaMemcpyAsync(&gpuRef[ioffset], &d_C[ioffset], iBytes, cudaMemcpyDeviceToHost, stream[i]));\n","  }\n","  CHECK(cudaDeviceSynchronize());\n","  gpu_time = seconds() - start;\n","  printf(\"   GPU elapsed time (multi-stream): %.5f (sec)- speedup %.1f\\n\", gpu_time, cpu_time/gpu_time);\n","\n","  // check device results\n","  checkResult(hostRef, gpuRef, nElem);\n","\n","  // free device global memory\n","  CHECK(cudaFree(d_A));\n","  CHECK(cudaFree(d_B));\n","  CHECK(cudaFree(d_C));\n","\n","  // free host memory\n","  CHECK(cudaFreeHost(h_A));\n","  CHECK(cudaFreeHost(h_B));\n","  CHECK(cudaFreeHost(hostRef));\n","  CHECK(cudaFreeHost(gpuRef));\n","\n","  // destroy streams\n","  for (int i = 0; i < NSTREAM; ++i)\n","    CHECK(cudaStreamDestroy(stream[i]));\n","\n","  CHECK(cudaDeviceReset());\n","  return(0);\n","}\n","\n","void initialData(float *ip, int size) {\n","   int i;\n","\n","   for(i = 0; i < size; i++)\n","     ip[i] = (float)(rand() & 0xFF) / 10.0f;\n"," }\n","\n"," void sumArraysOnHost(float *A, float *B, float *C, const int N) {\n","   for (int idx = 0; idx < N; idx++)\n","     C[idx] = A[idx] + B[idx];\n"," }\n","\n","void checkResult(float *hostRef, float *gpuRef, const int N) {\n","   double epsilon = 1.0E-8;\n","   bool match = 1;\n","\n","   for (int i = 0; i < N; i++) {\n","     if (abs(hostRef[i] - gpuRef[i]) > epsilon) {\n","       match = 0;\n","       printf(\"Arrays do not match!\\n\");\n","       printf(\"host %5.2f gpu %5.2f at %d\\n\", hostRef[i], gpuRef[i], i);\n","       break;\n","     }\n","   }\n","   if (match)\n","     printf(\"Arrays match.\\n\\n\");\n"," }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"9AhdYJUwH-wE"}},{"cell_type":"code","metadata":{"id":"7G91KROXBScK"},"source":["!nvcc -arch=sm_75  src/lez8/sumArrayStream.cu  -o sumArray\n","!./sumArray"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vXUIQkZLCTcG"},"source":["# ✅ Tabular\n"]},{"cell_type":"markdown","source":["↘️ **TODO...**"],"metadata":{"id":"1wm23k6-G38i"}},{"cell_type":"markdown","source":["1. Come modificare il kernel per usare gli stream\n","2. Gestione della memoria pinned e device\n","\n","Applicare\n","3. Schema: loop over {copy, kernel, copy}\n","4. Schema: loop over {copy H2D}, loop over {kernel}, loop over {copy D2H}\n"],"metadata":{"id":"NtEOIgt3utGz"}},{"cell_type":"code","metadata":{"id":"-Y52R0d3CA50"},"source":["%%cuda_group_save --name \"tabular.cu\" --group \"lez8\"\n","\n","#include <stdio.h>\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","#define PI 3.141592f\n","\n","/*\n"," * Kernel: tabular function\n"," */\n","__global__ void tabular(float *a, int n) {\n","\tint i = threadIdx.x + blockIdx.x * blockDim.x;\n","\tif (i < n) {\n","\t\tfloat x = PI * (float)i / (float)n;\n","\t\tfloat s = sinf(x);\n","\t\tfloat c = cosf(x);\n","\t\ta[i] = sqrtf(abs(s * s - c * c));\n","\t}\n","}\n","\n","/*\n"," * Kernel: tabular function using streams\n"," */\n","\n","\t// TODO\n","\n","/*\n"," * Error measure\n"," */\n","float maxError(float *a, int n) {\n","\tfloat maxE = 0;\n","\tfor (int i = 0; i < n; i++) {\n","\t\tfloat error = fabs(a[i] - 1.0f);\n","\t\tif (error > maxE)\n","\t\t\tmaxE = error;\n","\t}\n","\treturn maxE;\n","}\n","\n","/*\n"," * Main: tabular function\n"," */\n","int main(void) {\n","\n","  // main params\n","  uint MB = 1024*1024;\n","  uint n = 256*MB;\n","\tint blockSize = 256;\n","\tint nStreams = 8;\n","\n","\tint streamSize = n / nStreams;\n","\tint streamBytes = streamSize * sizeof(float);\n","\tint bytes = n * sizeof(float);\n","\n","\tint devId = 0;\n","\tcudaDeviceProp prop;\n","\tCHECK(cudaGetDeviceProperties(&prop, devId));\n","\tprintf(\"Device : %s\\n\\n\", prop.name);\n","\tCHECK(cudaSetDevice(devId));\n","  printf(\"Array size   : %d\\n\", n);\n","  printf(\"StreamSize   : %d\\n\", streamSize);\n","  printf(\"Memory bytes : %d (MB)\\n\", bytes/MB);\n","  printf(\"streamBytes  : %d (MB)\\n\", streamBytes/MB);\n","\n","\t// allocate pinned host memory and device memory\n","\tfloat *a, *d_a;\n","\tCHECK(cudaMallocHost((void**) &a, bytes));      // host pinned\n","\tCHECK(cudaMalloc((void**) &d_a, bytes));        // device\n","\n","\tfloat ms; // elapsed time in milliseconds\n","\n","\t// create events and streams\n","\tcudaEvent_t startEvent, stopEvent, dummyEvent;\n","\tcudaStream_t stream[nStreams];\n","\tCHECK(cudaEventCreate(&startEvent));\n","\tCHECK(cudaEventCreate(&stopEvent));\n","\tCHECK(cudaEventCreate(&dummyEvent));\n","\tfor (int i = 0; i < nStreams; ++i)\n","\t\tCHECK(cudaStreamCreate(&stream[i]));\n","\n","\t// baseline case - sequential transfer and execute\n","\tmemset(a, 0, bytes);\n","\tCHECK(cudaEventRecord(startEvent, 0));\n","\tCHECK(cudaMemcpy(d_a, a, bytes, cudaMemcpyHostToDevice));\n","\ttabular<<<n / blockSize, blockSize>>>(d_a, n);\n","\tCHECK(cudaMemcpy(a, d_a, bytes, cudaMemcpyDeviceToHost));\n","\tCHECK(cudaEventRecord(stopEvent, 0));\n","\tCHECK(cudaEventSynchronize(stopEvent));\n","\tCHECK(cudaEventElapsedTime(&ms, startEvent, stopEvent));\n","\tprintf(\"\\nTime for sequential transfer and execute (ms): %f\\n\", ms);\n","\tprintf(\"  max error: %e\\n\", maxError(a, n));\n","\n","\t// asynchronous version 1: loop over {copy, kernel, copy}\n","\n","\n","\t// asynchronous version 2: loop over copy, loop over kernel, loop over copy\n","\n","\t// cleanup\n","\tCHECK(cudaEventDestroy(startEvent));\n","\tCHECK(cudaEventDestroy(stopEvent));\n","\tCHECK(cudaEventDestroy(dummyEvent));\n","\tfor (int i = 0; i < nStreams; ++i)\n","\t\tCHECK(cudaStreamDestroy(stream[i]));\n","\tcudaFree(d_a);\n","\tcudaFreeHost(a);\n","\n","\treturn 0;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"OkioVBI0AwuE"}},{"cell_type":"code","metadata":{"id":"wNKJCtgzAwuF"},"source":["\n","!nvcc -arch=sm_75 src/lez8/tabular.cu  -o tabular\n","!./tabular"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f7Us14vqivur"},"source":["# ✅ Convoluzione con stream\n"]},{"cell_type":"markdown","source":["PPM filtering by 2D convolution..."],"metadata":{"id":"omFX96C-kHxi"}},{"cell_type":"code","source":["%%cuda_group_save --name \"ppm_conv2D.cu\" --group \"lez8\"\n","#include \"/content/GPUcomputing/utils/common.h\"\n","#include \"ppm.h\"\n","\n","#define BLOCK_SIZE   32\n","#define MASK_SIZE    21\n","#define TILE_SIZE    (BLOCK_SIZE + MASK_SIZE - 1)\n","\n","typedef struct {\n","   int width;\n","   int height;\n","   float* elements;\n"," } Matrix;\n","\n"," /*\n","  * 2D convolution using shared memory\n","  *   A: input matrix\n","  *   B: output matrix\n","  *   M: convolution mask matrix\n"," */\n","__global__ void conv2D(Matrix A, Matrix B, Matrix M) {\n","\n","   int x = blockIdx.x * blockDim.x + threadIdx.x; // Column index of matrix A\n","   int y = blockIdx.y * blockDim.y + threadIdx.y; // Row index of matrix A\n","\n","   int tile_size = BLOCK_SIZE + MASK_SIZE - 1;\n","   int radius = MASK_SIZE / 2;\n","\n","   // Allocate shared memory\n","   __shared__ float smem[TILE_SIZE][TILE_SIZE];\n","\n","   // Load data into shared memory\n","   for (int row = 0; row <= tile_size/blockDim.y; row++) {\n","      for (int col = 0; col <= tile_size/blockDim.x; col++) {\n","         int row_data = y + blockDim.y * row - radius;   // input data index row\n","         int col_data = x + blockDim.x * col - radius;   // input data index column\n","         int row_smem = threadIdx.y + blockDim.y * row;  // mask index row\n","         int col_smem = threadIdx.x + blockDim.x * col;  // mask index column\n","\n","         // Check valid range for smem and data\n","         if (row_smem < tile_size && col_smem < tile_size) {\n","            if (row_data >= 0 && row_data < A.height && col_data >= 0 && col_data < A.width) {\n","               smem[row_smem][col_smem] = A.elements[row_data * A.width + col_data];\n","            } else {\n","               smem[row_smem][col_smem] = 0.0f;\n","            }\n","         }\n","      }\n","   }\n","\n","   // Synchronize threads\n","   __syncthreads();\n","\n","   // Apply convolution\n","   float sum = 0.0f;\n","   for (int i = 0; i < MASK_SIZE; i++) {\n","      for (int j = 0; j < MASK_SIZE; j++) {\n","         int r = threadIdx.y + i;\n","         int c = threadIdx.x + j;\n","         if (r >= 0 && r < tile_size && c >= 0 && c < tile_size) {\n","            sum += smem[r][c] * M.elements[i * MASK_SIZE + j];\n","         }\n","      }\n","   }\n","\n","   // Write output\n","   if (y < A.height && x < A.width) {\n","      B.elements[y * B.width + x] = sum;\n","   }\n","}\n","\n","/*\n"," * Main function\n"," */\n","int main(void) {\n","   // Load image\n","   char path[] = \"GPUcomputing/images/dog.ppm\";\n","   PPM *img = ppm_load(path);\n","   int WIDTH = img->width;\n","   int HEIGHT = img->height;\n","   printf(\"PPM image size (w x h): %d x %d\\n\", WIDTH, HEIGHT);\n","\n","   // extract channels and set matrices\n","   Matrix R, G, B;\n","   R.width = WIDTH; R.height = HEIGHT;\n","   G.width = WIDTH; G.height = HEIGHT;\n","   B.width = WIDTH; B.height = HEIGHT;\n","   R.elements = (float *) malloc(WIDTH * HEIGHT * sizeof(float));\n","   G.elements = (float *) malloc(WIDTH * HEIGHT * sizeof(float));\n","   B.elements = (float *) malloc(WIDTH * HEIGHT * sizeof(float));\n","   color *r = ppm_extract_channel(img, 0); // get red channel\n","   color *g = ppm_extract_channel(img, 1); // get green channel\n","   color *b = ppm_extract_channel(img, 2); // get blue channel\n","   for (int i = 0; i < WIDTH * HEIGHT; i++) {\n","      R.elements[i] = (float) r[i];\n","      G.elements[i] = (float) g[i];\n","      B.elements[i] = (float) b[i];\n","   }\n","\n","   // get gaussian filter mask\n","   float SIGMA = 10.0;\n","   Matrix M;\n","   M.width = WIDTH; M.height = HEIGHT;\n","   M.elements = gaussMask(MASK_SIZE, SIGMA);\n","\n","   // Allocate device memory\n","   Matrix d_R, d_G, d_B, d_M;\n","   d_R.width = R.width; d_R.height = R.height;\n","   d_G.width = G.width; d_G.height = G.height;\n","   d_B.width = B.width; d_B.height = B.height;\n","   d_M.width = M.width; d_M.height = M.height;\n","   CHECK(cudaMalloc(&d_R.elements, R.width * R.height * sizeof(float)));\n","   CHECK(cudaMalloc(&d_G.elements, G.width * G.height * sizeof(float)));\n","   CHECK(cudaMalloc(&d_B.elements, B.width * B.height * sizeof(float)));\n","   CHECK(cudaMalloc(&d_M.elements, M.width * M.height * sizeof(float)));\n","\n","   /***********************************************************/\n","\t/*                    conv2D on host                       */\n","\t/***********************************************************/\n","   printf(\"\\nCPU procedure...\\n\");\n","\tdouble start = seconds();\n","   PPM *img_filtered = ppm_make(WIDTH, HEIGHT, (pel) {0,0,0}); // create a new image\n","   ppm_gaussFilter(img, img_filtered, MASK_SIZE, SIGMA);\n","   ppm_write(img_filtered, \"output_gaussian.ppm\");\n","\tdouble stopCPU = seconds() - start;\n","   printf(\"   Host elapsed time: %f\\n\", stopCPU);\n","\n","   /***********************************************************/\n","\t/*                  GPU conv2D wih smem                    */\n","\t/***********************************************************/\n","   printf(\"\\nGPU conv2D with smem...\\n\");\n","   dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);\n","   dim3 dimGrid((WIDTH + BLOCK_SIZE - 1) / BLOCK_SIZE, (HEIGHT + BLOCK_SIZE - 1) / BLOCK_SIZE);\n","\n","   start = seconds();\n","\n","   // Copy data to device\n","   CHECK(cudaMemcpy(d_R.elements, R.elements, R.width * R.height * sizeof(float), cudaMemcpyHostToDevice));\n","   CHECK(cudaMemcpy(d_G.elements, G.elements, G.width * G.height * sizeof(float), cudaMemcpyHostToDevice));\n","   CHECK(cudaMemcpy(d_B.elements, B.elements, B.width * B.height * sizeof(float), cudaMemcpyHostToDevice));\n","   CHECK(cudaMemcpy(d_M.elements, M.elements, M.width * M.height * sizeof(float), cudaMemcpyHostToDevice));\n","\n","   // invoke kernels\n","   conv2D<<<dimGrid, dimBlock>>>(d_R, d_R, d_M);\n","   conv2D<<<dimGrid, dimBlock>>>(d_G, d_G, d_M);\n","   conv2D<<<dimGrid, dimBlock>>>(d_B, d_B, d_M);\n","\n","   // Copy data back to host\n","   CHECK(cudaMemcpy(R.elements, d_R.elements, R.width * R.height * sizeof(float), cudaMemcpyDeviceToHost));\n","   CHECK(cudaMemcpy(G.elements, d_G.elements, G.width * G.height * sizeof(float), cudaMemcpyDeviceToHost));\n","   CHECK(cudaMemcpy(B.elements, d_B.elements, B.width * B.height * sizeof(float), cudaMemcpyDeviceToHost));\n","\n","   // print elapsed time: H2D + kernel + D2H\n","   double stopGPU = seconds() - start;\n","   printf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU, stopCPU / stopGPU);\n","\n","   // Copy channels to new image\n","   for (int i = 0; i < WIDTH * HEIGHT; i++) {\n","      r[i] = (color) R.elements[i];\n","      g[i] = (color) G.elements[i];\n","      b[i] = (color) B.elements[i];\n","   }\n","\n","   // build new image\n","   PPM *ppm_filtered = ppm_combine_channels(r, g, b, WIDTH, HEIGHT);\n","   ppm_write(img_filtered, \"output_gaussianGPU.ppm\");\n","\n","   return 0;\n","}\n","\n","\n"],"metadata":{"id":"d-G8iRsMi4PR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"Pv4zXce_krNG"}},{"cell_type":"code","metadata":{"id":"vx7uHpEDkrNI"},"source":["!nvcc -arch=sm_75 src/lez8/ppm_conv2D.cu  -o ppm_conv2D -I /content/GPUcomputing/utils/PPM /content/GPUcomputing/utils/PPM/ppm.cpp\n","!./ppm_conv2D"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["PPM filtering by 2D convolution with streams..."],"metadata":{"id":"6YT2lmK29HQQ"}},{"cell_type":"markdown","source":["↘️ **TODO...**"],"metadata":{"id":"RoDs9gZd9Wna"}},{"cell_type":"markdown","source":["Passi da fare:\n"," 1. CUDA stream creation\n"," 2. Async copy of data to device\n"," 3. invoke kernels on streams\n"," 4. Copy data back to host\n"," 5. Wait for all streams to finish\n"," 6. Free device memory destring streams\n"],"metadata":{"id":"ya4fZmdl1gHq"}},{"cell_type":"code","source":["%%cuda_group_save --name \"ppm_conv2D_stream.cu\" --group \"lez8\"\n","#include \"/content/GPUcomputing/utils/common.h\"\n","#include \"ppm.h\"\n","\n","#define BLOCK_SIZE   32\n","#define MASK_SIZE    21\n","#define TILE_SIZE    (BLOCK_SIZE + MASK_SIZE - 1)\n","\n","typedef struct {\n","   int width;\n","   int height;\n","   float* elements;\n"," } Matrix;\n","\n"," /*\n","  * 2D convolution using shared memory\n","  *   A: input matrix\n","  *   B: output matrix\n","  *   M: convolution mask matrix\n"," */\n","__global__ void conv2D(Matrix A, Matrix B, Matrix M) {\n","\n","   int x = blockIdx.x * blockDim.x + threadIdx.x; // Column index of matrix A\n","   int y = blockIdx.y * blockDim.y + threadIdx.y; // Row index of matrix A\n","\n","   int tile_size = BLOCK_SIZE + MASK_SIZE - 1;\n","   int radius = MASK_SIZE / 2;\n","\n","   // Allocate shared memory\n","   __shared__ float smem[TILE_SIZE][TILE_SIZE];\n","\n","   // Load data into shared memory\n","   for (int row = 0; row <= tile_size/blockDim.y; row++) {\n","      for (int col = 0; col <= tile_size/blockDim.x; col++) {\n","         int row_data = y + blockDim.y * row - radius;   // input data index row\n","         int col_data = x + blockDim.x * col - radius;   // input data index column\n","         int row_smem = threadIdx.y + blockDim.y * row;  // mask index row\n","         int col_smem = threadIdx.x + blockDim.x * col;  // mask index column\n","\n","         // Check valid range for smem and data\n","         if (row_smem < tile_size && col_smem < tile_size) {\n","            if (row_data >= 0 && row_data < A.height && col_data >= 0 && col_data < A.width) {\n","               smem[row_smem][col_smem] = A.elements[row_data * A.width + col_data];\n","            } else {\n","               smem[row_smem][col_smem] = 0.0f;\n","            }\n","         }\n","      }\n","   }\n","\n","   // Synchronize threads\n","   __syncthreads();\n","\n","   // Apply convolution\n","   float sum = 0.0f;\n","   for (int i = 0; i < MASK_SIZE; i++) {\n","      for (int j = 0; j < MASK_SIZE; j++) {\n","         int r = threadIdx.y + i;\n","         int c = threadIdx.x + j;\n","         if (r >= 0 && r < tile_size && c >= 0 && c < tile_size) {\n","            sum += smem[r][c] * M.elements[i * MASK_SIZE + j];\n","         }\n","      }\n","   }\n","\n","   // Write output\n","   if (y < A.height && x < A.width) {\n","      B.elements[y * B.width + x] = sum;\n","   }\n","}\n","\n","/*\n"," * Main function\n"," */\n","int main(void) {\n","   // Load image\n","   char path[] = \"GPUcomputing/images/dog.ppm\";\n","   PPM *img = ppm_load(path);\n","   int WIDTH = img->width;\n","   int HEIGHT = img->height;\n","   printf(\"PPM image size (w x h): %d x %d\\n\", WIDTH, HEIGHT);\n","\n","   // extract channels and set matrices\n","   Matrix R, G, B;\n","   R.width = WIDTH; R.height = HEIGHT;\n","   G.width = WIDTH; G.height = HEIGHT;\n","   B.width = WIDTH; B.height = HEIGHT;\n","   R.elements = (float *) malloc(WIDTH * HEIGHT * sizeof(float));\n","   G.elements = (float *) malloc(WIDTH * HEIGHT * sizeof(float));\n","   B.elements = (float *) malloc(WIDTH * HEIGHT * sizeof(float));\n","   color *r = ppm_extract_channel(img, 0); // get red channel\n","   color *g = ppm_extract_channel(img, 1); // get green channel\n","   color *b = ppm_extract_channel(img, 2); // get blue channel\n","   for (int i = 0; i < WIDTH * HEIGHT; i++) {\n","      R.elements[i] = (float) r[i];\n","      G.elements[i] = (float) g[i];\n","      B.elements[i] = (float) b[i];\n","   }\n","\n","   // get gaussian filter mask\n","   float SIGMA = 10.0;\n","   Matrix M;\n","   M.width = WIDTH; M.height = HEIGHT;\n","   M.elements = gaussMask(MASK_SIZE, SIGMA);\n","\n","   // Allocate device memory\n","   Matrix d_R, d_G, d_B, d_M;\n","   d_R.width = R.width; d_R.height = R.height;\n","   d_G.width = G.width; d_G.height = G.height;\n","   d_B.width = B.width; d_B.height = B.height;\n","   d_M.width = M.width; d_M.height = M.height;\n","   CHECK(cudaMalloc(&d_R.elements, R.width * R.height * sizeof(float)));\n","   CHECK(cudaMalloc(&d_G.elements, G.width * G.height * sizeof(float)));\n","   CHECK(cudaMalloc(&d_B.elements, B.width * B.height * sizeof(float)));\n","   CHECK(cudaMalloc(&d_M.elements, M.width * M.height * sizeof(float)));\n","\n","   // Copy mask to device\n","   CHECK(cudaMemcpy(d_M.elements, M.elements, M.width * M.height * sizeof(float), cudaMemcpyHostToDevice));\n","\n","   /***********************************************************/\n","\t/*                    conv2D on host                       */\n","\t/***********************************************************/\n","   printf(\"\\nCPU procedure...\\n\");\n","\tdouble start = seconds();\n","   PPM *img_filtered = ppm_make(WIDTH, HEIGHT, (pel) {0,0,0}); // create a new image\n","   ppm_gaussFilter(img, img_filtered, MASK_SIZE, SIGMA);\n","   ppm_write(img_filtered, \"output_gaussian.ppm\");\n","\tdouble stopCPU = seconds() - start;\n","   printf(\"   Host elapsed time: %f\\n\", stopCPU);\n","\n","   /***********************************************************/\n","\t/*                GPU conv2D with streams                  */\n","\t/***********************************************************/\n","   printf(\"\\nGPU conv2D with smem...\\n\");\n","   const int C = 3; // RGB channels\n","   dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);\n","   dim3 dimGrid((WIDTH + BLOCK_SIZE - 1) / BLOCK_SIZE, (HEIGHT + BLOCK_SIZE - 1) / BLOCK_SIZE);\n","\n","   start = seconds();\n","\n","   // CUDA stream creation\n","\n","   // Async copy of data to device\n","\n","   // invoke kernels on streams\n","\n","   // Copy data back to host\n","\n","   // Wait for all streams to finish\n","\n","   // Free device memory destring streams\n","\n","   // print elapsed time: H2D + kernel + D2H\n","   double stopGPU = seconds() - start;\n","   printf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU, stopCPU / stopGPU);\n","\n","   // Copy channels to new image\n","   for (int i = 0; i < WIDTH * HEIGHT; i++) {\n","      r[i] = (color) R.elements[i];\n","      g[i] = (color) G.elements[i];\n","      b[i] = (color) B.elements[i];\n","   }\n","\n","   // build new image\n","   PPM *ppm_filtered = ppm_combine_channels(r, g, b, WIDTH, HEIGHT);\n","   ppm_write(img_filtered, \"output_gaussianGPU.ppm\");\n","\n","   return 0;\n","}\n","\n","\n"],"metadata":{"id":"f1Drq3ha9Ymc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"cnS_HY6p9Ymd"}},{"cell_type":"code","metadata":{"id":"gwSBDscA9Ymd"},"source":["!nvcc -arch=sm_75 src/lez8/ppm_conv2D_stream.cu  -o ppm_conv2D_stream -I /content/GPUcomputing/utils/PPM /content/GPUcomputing/utils/PPM/ppm.cpp\n","!./ppm_conv2D_stream"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v2NIkXGQgsOb"},"source":["# ✅ Kernel per immagini PPM con stream ed eventi\n"]},{"cell_type":"markdown","source":["↘️ **TODO...**"],"metadata":{"id":"8ZpdNA9iyCHW"}},{"cell_type":"markdown","source":["Passi da fare:\n","\n"," 1. create an event on channel G for synchronization\n"," 2. CUDA stream creation (one for each channel)\n"," 3. Async copy of data to device  \n"," 4. invoke kernels on streams\n"," 5. histogram on streams\n"," 6. normalize histogram on stream 0\n"," 7. record event on stream 0\n"," 8. Wait for the second stream to finish before starting the other streams\n"," 9. equalize on streams\n"," 10. Copy data back to host to R, G, B matrices\n"," 11. Wait for all streams to finish\n"," 12. Free device memory destring streams"],"metadata":{"id":"PpfMig2pzUSx"}},{"cell_type":"code","source":["%%cuda_group_save --name \"ppm_conv2D_stream_event.cu\" --group \"lez8\"\n","#include \"/content/GPUcomputing/utils/common.h\"\n","#include \"kernels.h\"\n","\n","/*\n"," * Main function\n"," */\n","int main(void) {\n","   // Load image\n","   char path[] = \"GPUcomputing/images/dog.ppm\";\n","   PPM *img = ppm_load(path);\n","   int WIDTH = img->width;\n","   int HEIGHT = img->height;\n","   printf(\"PPM image size (w x h): %d x %d\\n\", WIDTH, HEIGHT);\n","\n","   // extract channels and set matrices\n","   Matrix R, G, B;\n","   R.width = WIDTH; R.height = HEIGHT;\n","   G.width = WIDTH; G.height = HEIGHT;\n","   B.width = WIDTH; B.height = HEIGHT;\n","   R.elements = (float *) malloc(WIDTH * HEIGHT * sizeof(float));\n","   G.elements = (float *) malloc(WIDTH * HEIGHT * sizeof(float));\n","   B.elements = (float *) malloc(WIDTH * HEIGHT * sizeof(float));\n","   color *r = ppm_extract_channel(img, 0); // get red channel\n","   color *g = ppm_extract_channel(img, 1); // get green channel\n","   color *b = ppm_extract_channel(img, 2); // get blue channel\n","   for (int i = 0; i < WIDTH * HEIGHT; i++) {\n","      R.elements[i] = (float) r[i];\n","      G.elements[i] = (float) g[i];\n","      B.elements[i] = (float) b[i];\n","   }\n","\n","   // Gaussian filter masks\n","   float SIGMA = 10.0;\n","   Matrix M;\n","   M.width = MASK_SIZE;\n","   M.height = MASK_SIZE;\n","   M.elements = gaussMask(MASK_SIZE, SIGMA);\n","\n","   // Allocate device memory for the RGB channels and mask\n","   Matrix d_R, d_G, d_B, d_R1, d_G1, d_B1, d_M;\n","   d_R.width = R.width; d_R.height = R.height;\n","   d_G.width = G.width; d_G.height = G.height;\n","   d_B.width = B.width; d_B.height = B.height;\n","   d_M.width = M.width; d_M.height = M.height;\n","   CHECK(cudaMalloc(&d_R.elements, R.width * R.height * sizeof(float)));\n","   CHECK(cudaMalloc(&d_G.elements, G.width * G.height * sizeof(float)));\n","   CHECK(cudaMalloc(&d_B.elements, B.width * B.height * sizeof(float)));\n","   CHECK(cudaMalloc(&d_M.elements, M.width * M.height * sizeof(float)));\n","   CHECK(cudaMalloc(&d_R1.elements, R.width * R.height * sizeof(float)));\n","   CHECK(cudaMalloc(&d_G1.elements, G.width * G.height * sizeof(float)));\n","   CHECK(cudaMalloc(&d_B1.elements, B.width * B.height * sizeof(float)));\n","\n","   // allocate memory for the histograms on the device\n","   float *histogram_R, *histogram_G, *histogram_B;\n","   int nBins = 256 * sizeof(float);\n","   CHECK(cudaMalloc(&histogram_R, nBins));\n","   CHECK(cudaMemset(histogram_R, 0, nBins));\n","   CHECK(cudaMalloc(&histogram_G, nBins));\n","   CHECK(cudaMemset(histogram_G, 0, nBins));\n","   CHECK(cudaMalloc(&histogram_B, nBins));\n","   CHECK(cudaMemset(histogram_B, 0, nBins));\n","\n","   // Copy mask to device\n","   CHECK(cudaMemcpy(d_M.elements, M.elements, M.width * M.height * sizeof(float), cudaMemcpyHostToDevice));\n","\n","   /***********************************************************/\n","\t/*   PPM filtering and equaliza with streams and events    */\n","\t/***********************************************************/\n","   printf(\"\\nFiltering PPM image with multiple kernels...\\n\");\n","   const int C = 3; // RGB channels\n","   dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);\n","   dim3 dimGrid((WIDTH + BLOCK_SIZE - 1) / BLOCK_SIZE, (HEIGHT + BLOCK_SIZE - 1) / BLOCK_SIZE);\n","   dim3 dimBlock1(512);\n","   dim3 dimGrid1((WIDTH * HEIGHT + dimBlock1.x - 1) / dimBlock1.x);\n","\n","   // create an event on channel G for synchronization\n","\n","\n","   // CUDA stream creation (one for each channel)\n","\n","\n","   // Async copy of data to device\n","\n","\n","   // invoke kernels on streams\n","\n","\n","   // histogram on streams\n","\n","\n","   // normalize histogram on stream 0\n","\n","\n","   // record event on stream 0\n","\n","\n","   // Wait for the second stream to finish before starting the other streams\n","\n","\n","   // equalize on streams\n","\n","\n","   // Copy data back to host to R, G, B matrices\n","\n","\n","   // Wait for all streams to finish\n","\n","\n","   // Free device memory destring streams\n","\n","\n","   // Copy channels to new image\n","   for (int i = 0; i < WIDTH * HEIGHT; i++) {\n","      r[i] = (color) R.elements[i];\n","      g[i] = (color) G.elements[i];\n","      b[i] = (color) B.elements[i];\n","   }\n","\n","   // build new image\n","   PPM *ppm_filtered = ppm_combine_channels(r, g, b, WIDTH, HEIGHT);\n","   ppm_write(ppm_filtered, \"outputGPU.ppm\");\n","\n","   printf(\"\\nImage saved: outputGPU.ppm\\n\");\n","\n","   return 0;\n","}\n","\n","\n"],"metadata":{"id":"99UA_PR-yEf5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"ST0jmySxyEf6"}},{"cell_type":"code","metadata":{"id":"iDJRsC64yEf6"},"source":["!nvcc -arch=sm_75 src/lez8/ppm_conv2D_stream_event.cu -o ppm_conv2D_stream_event -I /content/GPUcomputing/utils/PPM  /content/GPUcomputing/utils/PPM/kernels.cu /content/GPUcomputing/utils/PPM/ppm.cpp\n","!./ppm_conv2D_stream_event"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SOFMQZAkjlLW"},"source":["# ✅ MQDB con stream"]},{"cell_type":"markdown","source":["↘️ **TODO...**"],"metadata":{"id":"KWfzvvafR2ZX"}},{"cell_type":"markdown","source":["- Disegnare un kernel per il prodotto tra matrici MQDB con le seguenti specifiche:\n","- Allocare spazio per matrici MQDB su CPU e GPU\n","- Confrontare uso di memoria unificata vs memoria asincrona\n","- Introdurre gli stream su cui distribuire il carico (grid parall.)\n","- Analisi di prestazioni usando i tempi ricavati con CUDA event"],"metadata":{"id":"6yomfFJusrYT"}},{"cell_type":"code","metadata":{"id":"nJnr7bwN0dzA"},"source":["%%cuda_group_save --name \"MQDB_stream_Unified.cu\" --group \"lez8\"\n","\n","#include \"/content/GPUcomputing/utils/MQDB/mqdb.h\"\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","#define BLOCK_SIZE 16     // block size\n","#define TEST_CPU 0\n","\n","/*\n"," * Kernel for standard (naive) matrix product\n"," */\n","__global__ void matProdKernel(mqdb *A, mqdb *B, mqdb *C, int n) {\n","\t// row & col indexes\n","\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// each thread computes an entry of the product matrix\n","\tif ((row < n) && (col < n)) {\n","\t\tfloat val = 0;\n","\t\tfor (int k = 0; k < n; k++)\n","\t\t\tval += A->elem[row * n + k] * B->elem[k * n + col];\n","\t\tC->elem[row * n + col] = val;\n","\t}\n","}\n","\n","/*\n"," * Kernel for block sub-matrix product of mqdb\n"," */\n","__global__ void mqdbBlockProd(mqdb *A, mqdb *B, mqdb *C, uint sdim, uint d, uint n) {\n","\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// jump to the right block sub-matrix\n","\tuint  offset = (n+1)*sdim;\n","\n","\t// each thread computes an entry of the product matrix\n","\tif ((row < d) && (col < d)) {\n","\t\tfloat val = 0;\n","\t\tfor (int k = 0; k < d; k++)\n","\t\t\tval += A->elem[row * n + k + offset] * B->elem[k * n + col + offset];\n","\t\tC->elem[row * n + col + offset] = val;\n","\t}\n","}\n","\n","\n","/*\n"," * Test on MQDB kernels using Unified Memory\n"," */\n","void testKernelsMQDB_unified(uint n, uint k, cudaEvent_t start, cudaEvent_t stop) {\n","\n","\t// matrix instance generation - Unified Memory\n","\tmqdb *A, *B, *C;\n","\tCHECK(cudaMallocManaged(&A, sizeof(mqdb)));\n","  CHECK(cudaMallocManaged(&A->blkSize, k*sizeof(int)));\n","  CHECK(cudaMallocManaged(&A->elem, n*n*sizeof(float)));\n","\n","  CHECK(cudaMallocManaged(&B, sizeof(mqdb)));\n","  CHECK(cudaMallocManaged(&B->blkSize, k*sizeof(int)));\n","  CHECK(cudaMallocManaged(&B->elem, n*n*sizeof(float)));\n","\n","  CHECK(cudaMallocManaged(&C, sizeof(mqdb)));\n","  CHECK(cudaMallocManaged(&C->blkSize, k*sizeof(int)));\n","  CHECK(cudaMallocManaged(&C->elem, n*n*sizeof(float)));\n","\n","  // random fill mat entries\n","  int seed = 1;\n","\tgenRandDimsUnified(A, n, k, seed);\n","\tgenRandDimsUnified(B, n, k, seed);\n","\tgenRandDimsUnified(C, n, k, seed);\n","\tfillBlocksUnified(A, n, k, 'C', 1);\n","\tfillBlocksUnified(B, n, k, 'C', 2);\n","\tfillBlocksUnified(C, n, k, 'C', 0);\n","\n","\tulong nBytes = n * n * sizeof(float);\n","\tprintf(\"Memory size required = %3.4f (MB)\\n\",(float)nBytes/(1024.0*1024.0));\n","\n","\n","\t/***********************************************************/\n","\t/*                     GPU mat product                     */\n","\t/***********************************************************/\n","\n","  printf(\"Kernel (naive) mat product...\\n\");\n","\tdim3 block(BLOCK_SIZE, BLOCK_SIZE);\n","\tdim3 grid((n + block.x - 1) / block.x, (n + block.y - 1) / block.y);\n","  float milliseconds;\n","\tCHECK(cudaEventRecord(start));\n","\tmatProdKernel<<<grid, block>>>(A, B, C, n);\n","  CHECK(cudaDeviceSynchronize());\n","\tCHECK(cudaEventRecord(stop));\n","\tCHECK(cudaEventSynchronize(stop));\n","\tCHECK(cudaEventElapsedTime(&milliseconds, start, stop));\n","\tfloat GPUtime1 = milliseconds / 1000.0;\n","\tprintf(\"   elapsed time               : %.4f (sec)\\n\", GPUtime1);\n","\t//mqdbDisplay(C);\n","\n","\t/***********************************************************/\n","\t/*                     GPU MQDB product                    */\n","\t/***********************************************************/\n","\n","  printf(\"Kernel MQDB product...\\n\");\n","\tuint sdim = 0;\n","\tCHECK(cudaEventRecord(start));\n","\tfor (uint i = 0; i < k; i++ ) {\n","\t\tuint d = A->blkSize[i];\n","\t\tmqdbBlockProd<<<grid, block>>>(A, B, C, sdim, d, n);\n","\t\tsdim += d;\n","\t}\n","\tCHECK(cudaDeviceSynchronize());\n","\tCHECK(cudaEventRecord(stop));\n","\tCHECK(cudaEventSynchronize(stop));\n","\tCHECK(cudaEventElapsedTime(&milliseconds, start, stop));\n","\tfloat GPUtime2 = milliseconds / 1000.0;\n","\tprintf(\"   elapsed time                  :  %.4f (sec)\\n\", GPUtime2);\n","\tprintf(\"   speedup vs GPU std mat product:  %.4f\\n\\n\", GPUtime1/GPUtime2);\n","\n","  /***********************************************************/\n","\t/*             GPU MQDB product using streams              */\n","\t/***********************************************************/\n","\n","  printf(\"Kernel MQDB product using streams...\\n\");\n","\n","  // TODO\n","\n","\tfloat GPUtime3 = milliseconds / 1000.0;\n","\tprintf(\"   elapsed time                  : %.5f (sec)\\n\", GPUtime3);\n","\tprintf(\"   speedup vs GPU std mat product: %.2f\\n\",GPUtime1/GPUtime3);\n","  printf(\"   speedup vs GPU MQDB product   : %.2f\\n\",GPUtime2/GPUtime3);\n","  //mqdbDisplay(C);\n","\n","\t// clean up streams and events\n","\tfor (int i = 0; i < nstreams; i++)\n","\t\tcudaStreamDestroy(streams[i]);\n","\n","}\n","\n","/*\n"," * main function\n"," */\n","int main(int argc, char *argv[]) {\n","\n","  // set up device\n","\tint dev = 0;\n","\tcudaDeviceProp deviceProp;\n","\tCHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","\tprintf(\"%s starting mqdb product at \", argv[0]);\n","\tprintf(\"device %d: %s\\n\", dev, deviceProp.name);\n","\tCHECK(cudaSetDevice(dev));\n","\n","\t// events to measure time\n","\tcudaEvent_t start, stop;\n","\tcudaEventCreate(&start);\n","\tcudaEventCreate(&stop);\n","\n","\tuint n = 8*1024;         // matrix size\n","\tuint min_k = 20;         // min num of blocks\n","\tuint max_k = 30;         // max num of blocks\n","\n","\t// multiple tests for k = # diag blocks\n","\tfor (uint k = min_k; k <= max_k; k+=5) {\n","\t\tprintf(\"\\n*****   k = %d --- (avg block size = %f)\\n\",k,(float)n/k);\n","\t\ttestKernelsMQDB_unified(n, k, start, stop);\n","\t}\n","\n","  cudaEventDestroy(start);\n","\tcudaEventDestroy(stop);\n","\treturn 0;\n","}\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"HlO1nSPER2ZW"}},{"cell_type":"code","metadata":{"id":"wLxZjCx8bT3s"},"source":["!nvcc -arch=sm_75  src/lez8/MQDB_stream_Unified.cu /content/GPUcomputing/utils/MQDB/mqdb.cpp -o MQDBS\n","!./MQDBS"],"execution_count":null,"outputs":[]}]}