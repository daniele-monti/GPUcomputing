{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":["NO_C5o9-xRF_","iUYP4kCJhEIx","1snOeEsH_5z3","OHR7Zs3dNs1N","SOFMQZAkjlLW","khcgXsH8q-Ow","VR2PLOEZz4fD"],"mount_file_id":"1mk0QXjREp-k_J-pdFfmgoC7-EVmgPyAo","authorship_tag":"ABX9TyO40W1+zCyBUxDh7Dylbdej"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["---\n","# **LAB 6 - Ottimizzazione ed efficienza**\n","---"],"metadata":{"id":"fZYqN0UwVLC_"}},{"cell_type":"markdown","metadata":{"id":"NO_C5o9-xRF_"},"source":["# ▶️ CUDA setup"]},{"cell_type":"code","metadata":{"id":"8fekR2O4xRGE"},"source":["!nvcc --version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Psl9iouxRGE"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GPU computing notebooks download (from github)"],"metadata":{"id":"gcg1GyK5srek"}},{"cell_type":"code","source":["!git clone https://github.com/giulianogrossi/GPUcomputing.git"],"metadata":{"id":"tyHOxci3s3H8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"227eLdP5csN1"},"source":["NVCC Plugin for Jupyter notebook"]},{"cell_type":"code","source":["%cd GPUcomputing/utils/nvcc4jupyter-master/\n","!!python3 -m build\n","%load_ext nvcc4jupyter\n","%cd /content/"],"metadata":{"id":"4TzxMBFds8aT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iUYP4kCJhEIx"},"source":["# ✅ Occupancy Calculator - DeviceQuery"]},{"cell_type":"code","metadata":{"id":"Vvl8WXljg0WK"},"source":["%%cuda\n","#include <stdlib.h>\n","#include <stdio.h>\n","#include \"/content/GPUcomputing/utils/helper_string.h\"\n","#include \"/content/GPUcomputing/utils/helper_cuda.h\"\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","int main(void) {\n","\n","\tprintf(\"\\nCUDA Device Query (Runtime API) version (CUDART static linking)\\n\\n\");\n","\tint deviceCount = 0;\n","\tCHECK(cudaGetDeviceCount(&deviceCount));\n","\n","\t// This function call returns 0 if there are no CUDA capable devices.\n","\tif (deviceCount == 0)\n","\t\tprintf(\"There are no available device(s) that support CUDA\\n\");\n","\telse\n","\t\tprintf(\"Detected %d CUDA Capable device(s)\\n\", deviceCount);\n","\n","\tint dev, driverVersion = 0, runtimeVersion = 0;\n","\n","\tfor (dev = 0; dev < deviceCount; ++dev) {\n","\t\tcudaSetDevice(dev);\n","\t\tcudaDeviceProp deviceProp;\n","\t\tcudaGetDeviceProperties(&deviceProp, dev);\n","\n","\t\tprintf(\"\\nDevice %d: \\\"%s\\\"\\n\", dev, deviceProp.name);\n","\n","\t\tcudaDriverGetVersion(&driverVersion);\n","\t\tcudaRuntimeGetVersion(&runtimeVersion);\n","\n","\t\tprintf(\"  CUDA Driver Version / Runtime Version          %d.%d / %d.%d\\n\",\n","\t\t\t\tdriverVersion / 1000, (driverVersion % 100) / 10,\n","\t\t\t\truntimeVersion / 1000, (runtimeVersion % 100) / 10);\n","\n","\t\tprintf(\"  GPU arch name:                                 %s\\n\",\n","\t\t\t\t\t\t_ConvertSMVer2ArchName(deviceProp.major, deviceProp.minor));\n","\n","\t\tprintf(\"  CUDA Capability Major/Minor version number:    %d.%d\\n\",\n","\t\t\t\tdeviceProp.major, deviceProp.minor);\n","\n","\t\tprintf(\"  Total amount of global memory:                 %.0f MBytes (%llu bytes)\\n\",\n","\t\t\t\t(float) deviceProp.totalGlobalMem / 1048576.0f,\n","\t\t\t\t(unsigned long long) deviceProp.totalGlobalMem);\n","\n","\t\tprintf(\"  (%2d) Multiprocessors, (%3d) CUDA Cores/MP:     %d CUDA Cores\\n\",\n","\t\t\t\t\t\tdeviceProp.multiProcessorCount,\n","\t\t\t\t\t\t_ConvertSMVer2Cores(deviceProp.major, deviceProp.minor),\n","\t\t\t\t\t\t_ConvertSMVer2Cores(deviceProp.major, deviceProp.minor) *\n","\t\t\t\t\t\t\t\tdeviceProp.multiProcessorCount);\n","\n","\t\tprintf(\"  GPU Max Clock rate:                            %.0f MHz (%0.2f GHz)\\n\",\n","\t\t\t\tdeviceProp.clockRate * 1e-3f, deviceProp.clockRate * 1e-6f);\n","\n","\t\tprintf(\"  Memory Clock rate:                             %.0f Mhz\\n\", deviceProp.memoryClockRate * 1e-3f);\n","\t\tprintf(\"  Memory Bus Width:                              %d-bit\\n\", deviceProp.memoryBusWidth);\n","\t\tif (deviceProp.l2CacheSize)\n","\t\t\tprintf(\"  L2 Cache Size:                                 %d bytes\\n\", deviceProp.l2CacheSize);\n","\n","\t\tprintf(\"  Maximum Texture Dimension Size (x,y,z)         1D=(%d), 2D=(%d, %d), 3D=(%d, %d, %d)\\n\",\n","\t\t\t\tdeviceProp.maxTexture1D, deviceProp.maxTexture2D[0],\n","\t\t\t\tdeviceProp.maxTexture2D[1], deviceProp.maxTexture3D[0],\n","\t\t\t\tdeviceProp.maxTexture3D[1], deviceProp.maxTexture3D[2]);\n","\n","\t\tprintf(\"  Maximum Layered 1D Texture Size, (num) layers  1D=(%d), %d layers\\n\",\n","\t\t\t\tdeviceProp.maxTexture1DLayered[0],\n","\t\t\t\tdeviceProp.maxTexture1DLayered[1]);\n","\n","\t\tprintf(\"  Maximum Layered 2D Texture Size, (num) layers  2D=(%d, %d), %d layers\\n\",\n","\t\t\t\tdeviceProp.maxTexture2DLayered[0],\n","\t\t\t\tdeviceProp.maxTexture2DLayered[1],\n","\t\t\t\tdeviceProp.maxTexture2DLayered[2]);\n","\n","\t\tprintf(\"  Total amount of constant memory                %lu bytes\\n\",\n","\t\t\t\tdeviceProp.totalConstMem);\n","\t\tprintf(\"  Total amount of shared memory per block        %lu bytes\\n\",\n","\t\t\t\tdeviceProp.sharedMemPerBlock);\n","\t\tprintf(\"  Total number of registers available per block  %d\\n\",\n","\t\t\t\tdeviceProp.regsPerBlock);\n","\t\tprintf(\"  Warp size                                      %d\\n\",\n","\t\t\t\tdeviceProp.warpSize);\n","\t\tprintf(\"  Maximum number of threads per multiprocessor   %d\\n\",\n","\t\t\t\tdeviceProp.maxThreadsPerMultiProcessor);\n","\t\tprintf(\"  Maximum number of threads per block            %d\\n\",\n","\t\t\t\tdeviceProp.maxThreadsPerBlock);\n","\t\tprintf(\"  Max dimension size of a thread block (x,y,z)  (%d, %d, %d)\\n\",\n","\t\t\t\tdeviceProp.maxThreadsDim[0], deviceProp.maxThreadsDim[1],\n","\t\t\t\tdeviceProp.maxThreadsDim[2]);\n","\t\tprintf(\"  Max dimension size of a grid size    (x,y,z)  (%d, %d, %d)\\n\",\n","\t\t\t\tdeviceProp.maxGridSize[0], deviceProp.maxGridSize[1],\n","\t\t\t\tdeviceProp.maxGridSize[2]);\n","\t\tprintf(\"  Maximum memory pitch                           %lu bytes\\n\",\n","\t\t\t\tdeviceProp.memPitch);\n","\t\tprintf(\"  Texture alignment                              %lu bytes\\n\",\n","\t\t\t\tdeviceProp.textureAlignment);\n","\t\tprintf(\"  Concurrent copy and kernel execution           %s with %d copy engine(s)\\n\",\n","\t\t\t\t(deviceProp.deviceOverlap ? \"Yes\" : \"No\"),\n","\t\t\t\tdeviceProp.asyncEngineCount);\n","\t\tprintf(\"  Run time limit on kernels                      %s\\n\",\n","\t\t\t\tdeviceProp.kernelExecTimeoutEnabled ? \"Yes\" : \"No\");\n","\t\tprintf(\"  Integrated GPU sharing Host Memory             %s\\n\",\n","\t\t\t\tdeviceProp.integrated ? \"Yes\" : \"No\");\n","\t\tprintf(\"  Support host page-locked memory mapping        %s\\n\",\n","\t\t\t\tdeviceProp.canMapHostMemory ? \"Yes\" : \"No\");\n","\t\tprintf(\"  Alignment requirement for Surfaces             %s\\n\",\n","\t\t\t\tdeviceProp.surfaceAlignment ? \"Yes\" : \"No\");\n","\t\tprintf(\"  Device has ECC support                         %s\\n\",\n","\t\t\t\tdeviceProp.ECCEnabled ? \"Enabled\" : \"Disabled\");\n","\n","\t\tprintf(\"  Device supports Unified Addressing (UVA):      %s\\n\",\n","\t\t\t\tdeviceProp.unifiedAddressing ? \"Yes\" : \"No\");\n","\t\tprintf(\"  Device PCI Domain ID / Bus ID / location ID:   %d / %d / %d\\n\",\n","\t\t\t\tdeviceProp.pciDomainID, deviceProp.pciBusID,\n","\t\t\t\tdeviceProp.pciDeviceID);\n","\t}\n","\treturn 0;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Several API functions exist to assist programmers in choosing thread block size and cluster size based on register and shared memory requirements.\n","\n","- `cudaOccupancyMaxActiveBlocksPerMultiprocessor`, is the occupancy calculator API that provides an **occupancy prediction based on the block size and shared memory usage** of a kernel. This function reports occupancy in terms of the number of concurrent thread blocks per multiprocessor.\n","Note that this value can be converted to other metrics. Multiplying by the number of warps per block yields the number of concurrent warps per multiprocessor; further dividing concurrent warps by max warps per multiprocessor gives the occupancy as a percentage.\n","- `cudaOccupancyMaxPotentialBlockSize` and `cudaOccupancyMaxPotentialBlockSizeVariableSMem`, are the occupancy-based launch configurator APIs, **heuristically calculate an execution configuration that achieves the maximum multiprocessor-level occupancy.**\n","- `cudaOccupancyMaxActiveClusters`, can provided occupancy prediction based on the cluster size, block size and shared memory usage of a kernel. This function reports occupancy in terms of number of max active clusters of a given size on the GPU present in the system."],"metadata":{"id":"evll6aSlzVtd"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","The following code sample calculates the occupancy of MyKernel. It then reports the occupancy level with the **ratio between concurrent warps versus maximum warps per multiprocessor**."],"metadata":{"id":"JSbDdWC3zzHX"}},{"cell_type":"code","source":["%%cuda_group_save --name \"occupancy.cu\" --group \"lez6\"\n","#include <stdlib.h>\n","#include <stdio.h>\n","#include \"/content/GPUcomputing/utils/helper_string.h\"\n","#include \"/content/GPUcomputing/utils/helper_cuda.h\"\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","\n","// Device code\n","__global__ void MyKernel(int *d, int *a, int *b) {\n","  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n","  d[idx] = a[idx] * b[idx];\n","}\n","\n","// Host code\n","int main() {\n","  device_feat();    // device feats\n","  int numBlocks;    // number of of active blocks\n","\n","  // These variables are used to convert occupancy to warps\n","  int device;\n","  cudaDeviceProp prop;\n","  int activeWarps;\n","  int maxWarps;\n","\n","  cudaGetDevice(&device);\n","  cudaGetDeviceProperties(&prop, device);\n","  maxWarps = prop.maxThreadsPerMultiProcessor / prop.warpSize;\n","\n","  for (int blockSize = 16; blockSize <= 1024; blockSize*=2) {\n","    cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlocks, MyKernel, blockSize, 0);\n","    activeWarps = numBlocks * blockSize / prop.warpSize;\n","    double occup = (double)activeWarps / maxWarps * 100;\n","    printf(\"blockSize = %4d <-> Occupancy [numBlocks = %2d,  activeWarps = %2d]:\\t%2.2f%%\\n\", blockSize, numBlocks, activeWarps, occup);\n","  }\n","  return 0;\n","}"],"metadata":{"id":"13Kv4NcSOanG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"ctyo0LAUTyw8"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez6/occupancy.cu -o occupancy\n","!./occupancy"],"metadata":{"id":"sXoLaamxSk33"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","The following code sample configures an occupancy-based kernel launch of MyKernel according to the user input.\n"],"metadata":{"id":"JoXN6tHHz5AV"}},{"cell_type":"code","source":["%%cuda_group_save --name \"occupancy.cu\" --group \"lez6\"\n","#include <stdlib.h>\n","#include <stdio.h>\n","#include \"/content/GPUcomputing/utils/helper_string.h\"\n","#include \"/content/GPUcomputing/utils/helper_cuda.h\"\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","\n","// Device code\n","__global__ void MyKernel(float *array, int arrayCount) {\n","  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n","  if (idx < arrayCount)\n","    array[idx] *= array[idx];\n","}\n","\n","__global__ void MyKernel1(int *d, int *a, int *b) {\n","  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n","  d[idx] = a[idx] * b[idx];\n","}\n","\n","// occupancy calculator\n","int occupancy(float *array, int arrayCount) {\n","  int blockSize;      // The launch configurator returned block size\n","  int minGridSize;    // The minimum grid size needed to achieve the\n","                      // maximum occupancy for a full device launch\n","  int gridSize;       // The actual grid size needed, based on input size\n","\n","  cudaOccupancyMaxPotentialBlockSize(&minGridSize, &blockSize, (void*)MyKernel, 0, arrayCount);\n","\n","  // Round up according to array size\n","  gridSize = (arrayCount + blockSize - 1) / blockSize;\n","  printf(\"blockSize = %4d, gridSize = %d, minGridSize = %2d\\n\", blockSize, gridSize, minGridSize);\n","\n","  MyKernel<<<gridSize, blockSize>>>(array, arrayCount);\n","  cudaDeviceSynchronize();\n","\n","  // compute occupancy\n","  int device;\n","  int numBlocks;\n","  cudaDeviceProp prop;\n","  cudaGetDevice(&device);\n","  cudaGetDeviceProperties(&prop, device);\n","  int maxWarps = prop.maxThreadsPerMultiProcessor / prop.warpSize;\n","  cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlocks, MyKernel, blockSize, 0);\n","  int activeWarps = numBlocks * blockSize / 32;\n","  double occup = (double)activeWarps / maxWarps * 100;\n","  printf(\"Occupancy [blockSize = %4d, activeWarps = %2d]:\\t%2.2f%%\\n\", blockSize, activeWarps, occup);\n","\n","  return 0;\n","}\n","\n","// MAIN\n","int main(void) {\n","  device_feat();    // device feats\n","  int N = 2<<20;\n","  size_t size = N * sizeof(float);\n","\n","  // Allocate input vectors h_A and h_B in host memory\n","  float* h_A = (float*)malloc(size);\n","  float* h_B = (float*)malloc(size);\n","  float* h_C = (float*)malloc(size);\n","\n","  // Allocate vectors in device memory\n","  float* d_A;\n","  cudaMalloc(&d_A, size);\n","  float* d_B;\n","  cudaMalloc(&d_B, size);\n","  float* d_C;\n","  cudaMalloc(&d_C, size);\n","\n","  // Copy vectors from host memory to device memory\n","  cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n","  cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n","\n","  // Invoke kernel\n","  occupancy(d_A, N);\n","\n","  // Free device memory\n","  cudaFree(d_A);\n","  cudaFree(d_B);\n","  cudaFree(d_C);\n","\n","  return 0;\n","}"],"metadata":{"id":"o-tdfCu8LWfM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"qD6Mt57rQFwt"}},{"cell_type":"code","source":["!nvcc -arch=sm_75  src/lez6/occupancy.cu -o occupancy\n","!./occupancy"],"metadata":{"id":"vGWag71kX1Ad"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Apply the same calculator to the kernel:\n","\n","```\n","__global__ void blockParReduce1(int *in, int *out, ulong n)\n","```\n","\n"],"metadata":{"id":"lv1svOfqJFPr"}},{"cell_type":"code","source":["%%cuda_group_save --name \"occupancy.cu\" --group \"lez6\"\n","#include <stdlib.h>\n","#include <stdio.h>\n","#include \"/content/GPUcomputing/utils/helper_string.h\"\n","#include \"/content/GPUcomputing/utils/helper_cuda.h\"\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","__global__ void blockParReduce1(int *in, int *out, ulong n) {\n","\n","\tuint tid = threadIdx.x;\n","\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// boundary check\n","\tif (idx >= n)\n","\t\treturn;\n","\n","\t// convert global data pointer to the local pointer of this block\n","\tint *thisBlock = in + blockIdx.x * blockDim.x;\n","\n","\t// in-place reduction in global memory\n","\tfor (int stride = 1; stride < blockDim.x; stride *= 2) {\n","\t\tif ((tid % (2 * stride)) == 0)\n","\t\t\tthisBlock[tid] += thisBlock[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = thisBlock[0];\n","}\n","\n","// MAIN\n","int main(void) {\n","  int numBlocks;        // Occupancy in terms of active blocks\n","\n","  // These variables are used to convert occupancy to warps\n","  int device;\n","  cudaDeviceProp prop;\n","  int activeWarps;\n","  int maxWarps;\n","\n","  cudaGetDevice(&device);\n","  cudaGetDeviceProperties(&prop, device);\n","  maxWarps = prop.maxThreadsPerMultiProcessor / prop.warpSize;\n","\n","  for (int blockSize = 16; blockSize <= 1024; blockSize*=2) {\n","    cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlocks, blockParReduce1, blockSize, 0);\n","    activeWarps = numBlocks * blockSize / prop.warpSize;\n","    double occup = (double)activeWarps / maxWarps * 100;\n","    printf(\"Occupancy [blockSize = %4d, activeWarps = %2d]:\\t%2.2f%%\\n\", blockSize, activeWarps, occup);\n","  }\n","  return 0;\n","}"],"metadata":{"id":"8BSlGWgvHaCl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"NC8uszYhQq0I"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 --ptxas-options=-v src/lez6/occupancy.cu -o occupancy\n","!./occupancy"],"metadata":{"id":"I2H9W15WYKaE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Apply the same calculator to the kernel:\n","\n","```\n","__global__ void blockParReduce_SMEM(int *in, int *out, ulong n)\n","```\n"],"metadata":{"id":"PL72zrGpYB8U"}},{"cell_type":"code","source":["%%cuda_group_save --name \"occupancy.cu\" --group \"lez6\"\n","#include <stdlib.h>\n","#include <stdio.h>\n","#include \"/content/GPUcomputing/utils/helper_string.h\"\n","#include \"/content/GPUcomputing/utils/helper_cuda.h\"\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","#define SMEM_DIM 1024\n","\n","/*\n","    This version uses sequential addressing -- no divergence or bank conflicts.\n","*/\n","__global__ void blockParReduce_SMEM(int *in, int *out, ulong n) {\n","\n","\t// shared mem\n","\t__shared__ int smem[SMEM_DIM];\n","\n","\tunsigned int tid = threadIdx.x;\n","\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// load shared mem\n","\tsmem[tid] = (idx < n) ? in[idx] : 0;\n","\n","\t// synchronize within threadblock\n","\t__syncthreads();\n","\n","\t// do reduction in shared mem\n","\tfor (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n","\t\tif (tid < stride)\n","\t\t\tsmem[tid] += smem[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = smem[0];\n","}\n","\n","\n","// MAIN\n","int main(void) {\n","  int numBlocks;        // Occupancy in terms of active blocks\n","\n","  // These variables are used to convert occupancy to warps\n","  int device;\n","  cudaDeviceProp prop;\n","  int activeWarps;\n","  int maxWarps;\n","\n","  cudaGetDevice(&device);\n","  cudaGetDeviceProperties(&prop, device);\n","  maxWarps = prop.maxThreadsPerMultiProcessor / prop.warpSize;\n","\n","  for (int blockSize = 16; blockSize <= 1024; blockSize*=2) {\n","    cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlocks, blockParReduce_SMEM, blockSize, 0);\n","    activeWarps = numBlocks * blockSize / prop.warpSize;\n","    double occup = (double)activeWarps / maxWarps * 100;\n","    printf(\"Occupancy [blockSize = %4d, activeWarps = %2d]:\\t%2.2f%%\\n\", blockSize, activeWarps, occup);\n","  }\n","  return 0;\n","}"],"metadata":{"id":"2Yw-iUA9UcX0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"ZCtX8rnlQvrn"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 --ptxas-options=-v src/lez6/occupancy.cu -o occupancy\n","!./occupancy"],"metadata":{"id":"Rq5tZidEYNtQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvcc -arch=sm_75 --ptxas-options=-v src/lez6/occupancy.cu"],"metadata":{"id":"J6l-x5kymaN0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ✅ Profiling"],"metadata":{"id":"1snOeEsH_5z3"}},{"cell_type":"markdown","source":["### Profilazione dettagliata del filtro PPM blurring implemenato in  `ppm_blurGPU.cu`"],"metadata":{"id":"1mcH0btsFLos"}},{"cell_type":"code","source":["%%cuda_group_save --name \"ppm_blurGPU.cu\" --group \"lez6\"\n","\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include \"ppm.h\"\n","#include \"../../GPUcomputing/utils/common.h\"\n","\n","/*\n"," * Set pel (pixel element) in ppm image.\n"," */\n"," __device__ void ppm_setGPU(PPM ppm, int x, int y, pel c) {\n","  int i = x + y*ppm.width;\n","  ppm.image[3*i] = c.r;\n","  ppm.image[3*i + 1] = c.g;\n","  ppm.image[3*i + 2] = c.b;\n","}\n","\n","/*\n","* Get pel (pixel element) from ppm image.\n","*/\n","__device__ pel ppm_getGPU(PPM ppm, int x, int y) {\n","  pel p;\n","  int i = x + y*ppm.width;\n","  p.r = ppm.image[3*i];\n","  p.g = ppm.image[3*i + 1];\n","  p.b = ppm.image[3*i + 2];\n","  return p;\n","}\n","\n","/*\n"," * Kernel 2D for PPM image blurring\n"," */\n","__global__ void ppm_blurGPU(PPM ppm, PPM ppm1, int MASK_SIZE) {\n","\n","  int x = blockIdx.x * blockDim.x + threadIdx.x;\n","  int y = blockIdx.y * blockDim.y + threadIdx.y;\n","\n","  if(x < ppm.width && y < ppm.height) {\n","    float R=0, G=0, B=0;\n","    int numPixels = 0;\n","    int RADIUS = MASK_SIZE/2;\n","    for(int r = -RADIUS; r < RADIUS; ++r) {\n","        for(int c = -RADIUS; c < RADIUS; ++c) {\n","            int row = y + r;\n","            int col = x + c;\n","            if(row > -1 && row < ppm.height && col > -1 && col < ppm.width) {\n","                int i = col + row*ppm.width;\n","                R += ppm.image[3*i];\n","                G += ppm.image[3*i + 1];\n","                B += ppm.image[3*i + 2];\n","                numPixels++;\n","            }\n","        }\n","    }\n","    int i = x + y*ppm.width;\n","    ppm1.image[3*i]     = (color)(R/numPixels);\n","    ppm1.image[3*i + 1] = (color)(G/numPixels);\n","    ppm1.image[3*i + 2] = (color)(B/numPixels);\n","  }\n","}\n","\n","/*\n"," * MAIN\n"," */\n","int main(int argc, char **argv) {\n","\n","  // PPM images\n","  PPM *ppm, *ppm1, *ppm2;  // Where images are stored in CPU\n","  PPM ppm_d, ppm1_d;\t     // Where images are stored in GPU\n","\n","  // load a PPM image from file\n","  char path[] = \"GPUcomputing/images/dog.ppm\";\n","  ppm = ppm_load(path);\n","  ppm1 = ppm_copy(ppm);\n","  uint WIDTH = ppm->width;\n","  uint HEIGHT = ppm->height;\n","  printf(\"PPM image size (w x h): %d x %d\\n\", WIDTH, HEIGHT);\n","\n","  // set main params\n","  size_t nBytes= WIDTH * HEIGHT * sizeof(pel);\n","  ppm_d.width = WIDTH;\n","  ppm_d.height = HEIGHT;\n","  ppm_d.maxval = ppm->maxval;\n","  int MASK_SIZE = 21;\n","\n","  // Allocate GPU buffer for the input and output images\n","  CHECK(cudaMalloc(&ppm_d.image, nBytes));\n","  CHECK(cudaMalloc(&ppm1_d.image, nBytes));\n","\n","  // copy image from CPU to GPU\n","  CHECK(cudaMemcpy(ppm_d.image, ppm->image, nBytes, cudaMemcpyHostToDevice));\n","\n","  // invoke kernels (define grid and block sizes)\n","  uint dimBlock = 16;\n","  dim3 block(dimBlock, dimBlock);\n","  dim3 grid((WIDTH + block.x - 1) / block.x, (HEIGHT + block.y - 1) / block.y);\n","\n","  double start = seconds();\n","  ppm_blurGPU <<<grid, block>>> (ppm_d, ppm1_d, MASK_SIZE);\n","  CHECK(cudaDeviceSynchronize());\n","  double stopGPU = seconds() - start;\n","\n","  // copy image from GPU to CPU\n","  CHECK(cudaMemcpy(ppm1->image, ppm1_d.image, nBytes, cudaMemcpyDeviceToHost));\n","  ppm_write(ppm1, \"ppm_blurredGPU.ppm\");\n","\n","  // check results with CPU\n","  ppm2 = ppm_make(ppm->width, ppm->height, (pel){0,0,0});\n","  start = seconds();\n","  ppm_blur(ppm, ppm2, MASK_SIZE);\n","  double stopCPU = seconds() - start;\n","  ppm_write(ppm2, \"ppm_blurredCPU.ppm\");\n","  printf(\"PPM images are %s\\n\", ppm_equal(ppm1, ppm2) ? \"equal\" : \"not equal\");\n","\n","  // free device memory\n","  CHECK(cudaFree(ppm_d.image));\n","  CHECK(cudaFree(ppm1_d.image));\n","\n","  // times & speedup\n","  printf(\"CPU elapsed time: %.4f (msec) \\n\", stopCPU*1000);\n","  printf(\"CPU elapsed time: %.4f (msec) - Speedup %.1f\\n\", stopGPU*1000, stopCPU/stopGPU);\n","\n","  return (EXIT_SUCCESS);\n","}\n"],"metadata":{"id":"kJyDJu7wurWA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"T9S0V6giQ1QK"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez6/ppm_blurGPU.cu -o blur -I GPUcomputing/utils/PPM  GPUcomputing/utils/PPM/ppm.cpp\n","!./blur"],"metadata":{"id":"K8YvyTng_-iC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Profiling...**"],"metadata":{"id":"-93JHCdRy3aw"}},{"cell_type":"code","source":["!ncu -f -o ppm_blurGPU_T4_profiling.ncu-rep blur"],"metadata":{"id":"XUzxfgLAycxS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ncu -i /content/ppm_blurGPU_T4_profiling.ncu-rep"],"metadata":{"id":"8Oxzo8_W0HWh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Profilazione dettagliata del filtro di convoluzione implemenato in `conv2D.cu`"],"metadata":{"id":"a94Iq13XGBQT"}},{"cell_type":"code","source":["%%cuda_group_save --name \"conv2D.cu\" --group \"lez6\"\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","#define BLOCK_SIZE   16\n","#define MASK_SIZE    21\n","#define TILE_SIZE    (BLOCK_SIZE + MASK_SIZE - 1)\n","\n","typedef struct {\n","   int width;\n","   int height;\n","   float* elements;\n"," } Matrix;\n","\n","// Function declarations\n"," void conv2D_host(Matrix A, Matrix B, Matrix M);\n","__global__ void conv2D_basic(Matrix A, Matrix B, Matrix M);\n","\n"," /*\n","  * 2D convolution using shared memory\n","  *   A: input matrix\n","  *   B: output matrix\n","  *   M: convolution mask matrix\n"," */\n","__global__ void conv2D(Matrix A, Matrix B, Matrix M) {\n","\n","   int x = blockIdx.x * blockDim.x + threadIdx.x; // Column index of matrix A\n","   int y = blockIdx.y * blockDim.y + threadIdx.y; // Row index of matrix A\n","\n","   int tile_size = BLOCK_SIZE + MASK_SIZE - 1;\n","   int radius = MASK_SIZE / 2;\n","\n","   // Allocate shared memory\n","   __shared__ float smem[TILE_SIZE][TILE_SIZE];\n","\n","   // Load data into shared memory\n","   for (int row = 0; row <= tile_size/blockDim.y; row++) {\n","      for (int col = 0; col <= tile_size/blockDim.x; col++) {\n","         int row_data = y + blockDim.y * row - radius;   // input data index row\n","         int col_data = x + blockDim.x * col - radius;   // input data index column\n","         int row_smem = threadIdx.y + blockDim.y * row;  // mask index row\n","         int col_smem = threadIdx.x + blockDim.x * col;  // mask index column\n","\n","         // Check valid range for smem and data\n","         if (row_smem < tile_size && col_smem < tile_size) {\n","            if (row_data >= 0 && row_data < A.height && col_data >= 0 && col_data < A.width) {\n","               smem[row_smem][col_smem] = A.elements[row_data * A.width + col_data];\n","            } else {\n","               smem[row_smem][col_smem] = 0.0f;\n","            }\n","         }\n","      }\n","   }\n","\n","   // Synchronize threads\n","   __syncthreads();\n","\n","   // Apply convolution\n","   float sum = 0.0f;\n","   for (int i = 0; i < MASK_SIZE; i++) {\n","      for (int j = 0; j < MASK_SIZE; j++) {\n","         int r = threadIdx.y + i;\n","         int c = threadIdx.x + j;\n","         if (r >= 0 && r < tile_size && c >= 0 && c < tile_size) {\n","            sum += smem[r][c] * M.elements[i * MASK_SIZE + j];\n","         }\n","      }\n","   }\n","\n","   // Write output\n","   if (y < A.height && x < A.width) {\n","      B.elements[y * B.width + x] = sum;\n","   }\n","}\n","\n","/*\n"," * Main function\n"," */\n","int main(void) {\n","   // define matrices and params\n","   int block_size = BLOCK_SIZE;\n","   int mask_size = MASK_SIZE;\n","   int width = 256* block_size, height = 256* block_size;\n","   Matrix A, B, H, M;\n","   A.width = width; A.height = height;\n","   B.width = width; B.height = height;\n","   M.width = mask_size; M.height = mask_size;\n","   H.width = width; H.height = height;\n","   A.elements = (float *)malloc(width * height * sizeof(float));\n","   B.elements = (float *)malloc(width * height * sizeof(float));\n","   M.elements = (float *)malloc(mask_size * mask_size * sizeof(float));\n","   H.elements = (float *)malloc(width * height * sizeof(float));\n","\n","   // Initialize A, B, M\n","   // print data sizes\n","   printf(\"Data matrix A: %d x %d\\n\", width, height);\n","   printf(\"Mask matrix M: %d x %d\\n\", mask_size, mask_size);\n","   for (int i = 0; i < width * height; i++) {\n","      A.elements[i] = 1.0f;\n","      B.elements[i] = 0.0f;\n","   }\n","   for (int i = 0; i < mask_size * mask_size; i++) {\n","      M.elements[i] = 1.0f;\n","   }\n","\n","   // Allocate device memory\n","   Matrix d_A, d_B, d_M;\n","   d_A.width = A.width; d_A.height = A.height;\n","   d_B.width = B.width; d_B.height = B.height;\n","   d_M.width = M.width; d_M.height = M.height;\n","   CHECK(cudaMalloc(&d_A.elements, width * height * sizeof(float)));\n","   CHECK(cudaMalloc(&d_B.elements, width * height * sizeof(float)));\n","   CHECK(cudaMalloc(&d_M.elements, mask_size * mask_size * sizeof(float)));\n","\n","   // Copy data to device\n","   CHECK(cudaMemcpy(d_A.elements, A.elements, width * height * sizeof(float), cudaMemcpyHostToDevice));\n","   CHECK(cudaMemcpy(d_M.elements, M.elements, mask_size * mask_size * sizeof(float), cudaMemcpyHostToDevice));\n","\n","   /***********************************************************/\n","\t/*                    conv2D on host                       */\n","\t/***********************************************************/\n","   printf(\"\\nCPU procedure...\\n\");\n","\tdouble start = seconds();\n","\tconv2D_host(A, H, M);\n","\tdouble stopCPU = seconds() - start;\n","   printf(\"   Host elapsed time: %f\\n\", stopCPU);\n","\n","   /***********************************************************/\n","\t/*                    GPU naive conv2D                     */\n","\t/***********************************************************/\n","   printf(\"\\nGPU naive conv2D...\\n\");\n","   dim3 dimBlock(block_size, block_size);\n","   dim3 dimGrid((width + block_size - 1) / block_size, (height + block_size - 1) / block_size);\n","   start = seconds();\n","   conv2D_basic<<<dimGrid, dimBlock>>>(d_A, d_B, d_M);\n","   CHECK(cudaDeviceSynchronize());\n","   double stopGPU = seconds() - start;\n","   printf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU, stopCPU / stopGPU);\n","\n","   // Copy data back to host\n","   CHECK(cudaMemcpy(B.elements, d_B.elements, width * height * sizeof(float), cudaMemcpyDeviceToHost));\n","\n","   // check results\n","   for (int i = 0; i < width; i++) {\n","      for (int j = 0; j < height; j++) {\n","         if (B.elements[j * width + i] != H.elements[j * width + i]) {\n","            printf(\"Error at B[%d][%d] = %f\\n\", i, j, B.elements[j * width + i]);\n","         }\n","      }\n","   }\n","\n","   // zero out B in device\n","   CHECK(cudaMemset(d_B.elements, 0, width * height * sizeof(float)));\n","\n","   /***********************************************************/\n","\t/*                  GPU conv2D wih smem                    */\n","\t/***********************************************************/\n","   printf(\"\\nGPU conv2D with smem...\\n\");\n","   start = seconds();\n","   conv2D<<<dimGrid, dimBlock>>>(d_A, d_B, d_M);\n","   CHECK(cudaDeviceSynchronize());\n","   stopGPU = seconds() - start;\n","   printf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU, stopCPU / stopGPU);\n","\n","   // Copy data back to host\n","   CHECK(cudaMemcpy(B.elements, d_B.elements, width * height * sizeof(float), cudaMemcpyDeviceToHost));\n","\n","   // check results\n","   for (int i = 0; i < width; i++) {\n","      for (int j = 0; j < height; j++) {\n","         if (B.elements[j * width + i] != H.elements[j * width + i]) {\n","            printf(\"Error at B[%d][%d] = %f\\n\", i, j, B.elements[j * width + i]);\n","         }\n","      }\n","   }\n","\n","   return 0;\n","}\n","\n","/*\n"," * 2D convolution on host\n"," */\n","void conv2D_host(Matrix A, Matrix B, Matrix M) {\n","\n","   int radius = MASK_SIZE / 2;\n","\n","   // loop through all elements in the output array\n","   for (int y = 0; y < A.height; y++) {\n","\t   for (int x = 0; x < A.width; x++) {\n","\t\t\tfloat sum = 0.0f;\n","\n","\t\t\t// compute convolution\n","\t\t\tfor (int i = 0; i < MASK_SIZE; i++) {\n","            for (int j = 0; j < MASK_SIZE; j++) {\n","               int r = y - radius + i;\n","\t\t\t\t\tint c = x - radius + j;\n","\n","\t\t\t\t\t//boundary check\n","\t\t\t\t\tif ((c >= 0) && (c < A.width) && (r >= 0) && (r < A.height)) {\n","\t\t\t\t\t\tsum += A.elements[(r * A.width) + c] * M.elements[(j * MASK_SIZE) + i];\n","\t\t\t\t\t}\n","\t\t\t\t}\n","         }\n","\n","         //store final value\n","         B.elements[y * B.width + x] = sum;\n","\t   }\n","   }\n","}\n","\n","/*\n"," * Basic kernel for 2D convolution\n"," */\n"," __global__ void conv2D_basic(Matrix A, Matrix B, Matrix M) {\n","\n","\t//index computation\n","\tint x = blockIdx.x * blockDim.x + threadIdx.x;\n","\tint y = blockIdx.y * blockDim.y + threadIdx.y;\n","   int radius = MASK_SIZE / 2;\n","\n","   //boundary check\n","   if (x >= A.width && y >= A.height)  return;\n","\n","   // Apply convolution\n","   float sum = 0.0f;\n","   for (int i = 0; i < MASK_SIZE; i++) {\n","      for (int j = 0; j < MASK_SIZE; j++) {\n","         int r = y - radius + i;\n","         int c = x - radius + j;\n","         if (r >= 0 && r < A.height && c >= 0 && c < A.width) {\n","            sum += A.elements[r * A.width + c] * M.elements[i * MASK_SIZE + j];\n","         }\n","      }\n","   }\n","\n","   //store final value\n","   B.elements[y * B.width + x] = sum;\n","}"],"metadata":{"id":"0WGt14FTGFAj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"9AhdYJUwH-wE"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez6/conv2D.cu -o conv2D\n","!./conv2D"],"metadata":{"id":"sfUMBBBiH-wF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Profiling...**"],"metadata":{"id":"mKIS9PCAH-wF"}},{"cell_type":"code","source":["!ncu -o conv2D_T4_profiling.ncu-rep blur"],"metadata":{"id":"QnCJfdp9H-wF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ncu -i conv2D_T4_profiling.ncu-rep"],"metadata":{"id":"jSRjTUqxH-wG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ✅ Parallel reduction unrolling\n","\n","\n"],"metadata":{"id":"OHR7Zs3dNs1N"}},{"cell_type":"markdown","source":["↘️ **SOL...**"],"metadata":{"id":"dmtCni82wn9_"}},{"cell_type":"code","source":["%%cuda_group_save --name \"preduceUnroll.cu\" --group \"lez6\"\n","\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <assert.h>\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","#define SMEM_DIM 1024\n","\n","/*\n","*  Block by block parallel implementation with divergence (sequential schema)\n","*/\n","__global__ void blockParReduce1(int *in, int *out, ulong n) {\n","\n","\tint tid = threadIdx.x;\n","\tint idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// boundary check\n","\tif (idx >= n)\n","\t\treturn;\n","\n","\t// convert global data pointer to the local pointer of this block\n","\tint *thisBlock = in + blockIdx.x * blockDim.x;\n","\n","\t// in-place reduction in global memory\n","\tfor (int stride = 1; stride < blockDim.x; stride *= 2) {\n","\t\tif ((tid % (2 * stride)) == 0)\n","\t\t\tthisBlock[tid] += thisBlock[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = thisBlock[0];\n","}\n","\n","/*\n","*  Block by block parallel implementation without divergence (interleaved schema)\n","*/\n","__global__ void blockParReduce2(int *in, int *out, ulong n) {\n","\n","\tuint tid = threadIdx.x;\n","\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// boundary check\n","\tif (idx >= n)\n","\t\treturn;\n","\n","\t// convert global data pointer to the local pointer of this block\n","\tint *thisBlock = in + blockIdx.x * blockDim.x;\n","\n","\t// in-place reduction in global memory\n","\tfor (int stride = blockDim.x / 2; stride > 0; stride >>= 1)  {\n","\t\tif (tid < stride)\n","\t\t\tthisBlock[tid] += thisBlock[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = thisBlock[0];\n","}\n","\n","/*\n","* Using shared memory (no divergence nor bank conflicts)\n","*/\n","__global__ void blockParReduce3(int *in, int *out, ulong n) {\n","\n","\t// shared mem\n","\t__shared__ int smem[SMEM_DIM];\n","\n","\tunsigned int tid = threadIdx.x;\n","\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// load shared mem\n","\tsmem[tid] = (idx < n) ? in[idx] : 0;\n","\n","\t// synchronize within threadblock\n","\t__syncthreads();\n","\n","\t// do reduction in shared mem\n","\tfor (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n","\t\tif (tid < stride)\n","\t\t\tsmem[tid] += smem[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = smem[0];\n","}\n","\n","/*\n","*  Block by block parallel implementation using warp reduction\n","*/\n","__global__ void blockParReduce4(int *in, int *out, ulong n) {\n","\tint tid = blockIdx.x * blockDim.x + threadIdx.x;\n","\tint lane = threadIdx.x % warpSize;\n","\tint wid = threadIdx.x / warpSize;\n","\n","\tstatic __shared__ int shared[32];  \t// Shared mem for 32 partial sums\n","\tint val = in[tid];  \t\t\t\t\t\t// Each thread reads one element\n","\n","\t// 1° warp-shuffle reduction\n","\tfor (int offset = warpSize/2; offset > 0; offset /= 2)\n","\t\tval += __shfl_down_sync(0xffffffff, val, offset, 32);\n","\n","\tif (lane==0) shared[wid] = val; \t// Write reduced value to shared memory\n","\n","\t__syncthreads();          \t\t\t// Wait for all partial reductions\n","\n","\t// hereafter, just warp 0 (final warp-shuffle reduction)\n","\tif (wid == 0){\n","\t\tint val = shared[lane];\n","\n","\t\tfor (int offset = warpSize/2; offset > 0; offset >>= 1)\n","\t\t\tval += __shfl_down_sync(0xffffffff, val, offset);\n","\n","\t\t\t// write result for this block to global mem\n","\t\tif (threadIdx.x == 0)\n","\t\t\tout[blockIdx.x] = val;\n","\t}\n","}\n","\n","/*\n","*  Warp reduction using shared memory\n","*/\n","__device__ void warpReduce(volatile int *smem, unsigned int tid) {\n","   smem[tid] += smem[tid+32];\n","   smem[tid] += smem[tid+16];\n","   smem[tid] += smem[tid+8];\n","   smem[tid] += smem[tid+4];\n","   smem[tid] += smem[tid+2];\n","   smem[tid] += smem[tid+1];\n"," }\n","\n","/*\n","*  Block reduction using shared memory\n","*/\n","__device__ void blockReduce(volatile int *smem, unsigned int tid) {\n","   // block in-place reduction in shared memory\n","   if (blockDim.x >= 1024 && tid < 512) smem[tid] += smem[tid + 512];\n","   __syncthreads();\n","\n","   if (blockDim.x >= 512 && tid < 256) smem[tid] += smem[tid + 256];\n","   __syncthreads();\n","\n","   if (blockDim.x >= 256 && tid < 128) smem[tid] += smem[tid + 128];\n","   __syncthreads();\n","\n","   if (blockDim.x >= 128 && tid < 64)  smem[tid] += smem[tid + 64];\n","   __syncthreads();\n","\n","   // unrolling warp\n","   if (tid < 32) warpReduce(smem, tid);\n","}\n","\n","/*\n","*  Block by block parallel implementation using complete unroll for loop + smem\n","*/\n","__global__ void blockParReduce5(int *in, int *out, int n) {\n","   // SMEM for each block\n","   static __shared__ int smem[SMEM_DIM];\n","\n","   // set thread ID\n","   int tid = threadIdx.x;\n","\n","   // boundary check\n","   int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","   if (idx >= n) return;\n","\n","   // convert global data pointer to the local pointer of this block\n","   int *idata = in + blockIdx.x * blockDim.x;\n","\n","   // set to smem by each threads\n","   smem[tid] = idata[tid];\n","   __syncthreads();\n","\n","   // block in-place reduction in shared memory\n","   blockReduce(smem, tid);\n","\n","   // write result for this block to global mem\n","   if (tid == 0) out[blockIdx.x] = smem[0];\n","}\n","\n","/*\n"," *  Multi block parallel implementation with block and warp unrolling\n"," */\n"," __global__ void blockParReduce6(int *in, int *out, ulong n) {\n","\n","   // SMEM for each block\n","   static __shared__ int smem[SMEM_DIM];\n","\n","\tuint tid = threadIdx.x;\n","\tulong idx = blockIdx.x * blockDim.x * 8 + threadIdx.x;\n","\n","\t// boundary check\n","\tif (idx >= n) return;\n","\n","   // group blocks unrolling\n","    if (idx + 7 * blockDim.x < n) {\n","      smem[tid] = in[idx];\n","      smem[tid] += in[idx + blockDim.x];\n","      smem[tid] += in[idx + 2 * blockDim.x];\n","      smem[tid] += in[idx + 3 * blockDim.x];\n","      smem[tid] += in[idx + 4 * blockDim.x];\n","      smem[tid] += in[idx + 5 * blockDim.x];\n","      smem[tid] += in[idx + 6 * blockDim.x];\n","      smem[tid] += in[idx + 7 * blockDim.x];\n","    }\n","    __syncthreads();\n","\n","   // block in-place reduction in shared memory\n","   blockReduce(smem, tid);\n","\n","   // write result for this block to global mem\n","   if (tid == 0) out[blockIdx.x] = smem[0];\n","}\n","\n","/*\n","* MAIN: test on parallel reduction\n","*/\n","int main() {\n","\tint *a, *b, *d_a, *d_b;\n","\tint blockSize = 1024;             // block dim 1D\n","\tsize_t numBlock = 1024*1024;      // grid dim 1D\n","\tsize_t n = blockSize * numBlock;  // array dims\n","\tsize_t sum_CPU = 0, sum_GPU = 0;\n","\tsize_t nByte = n*sizeof(int);\n","\tsize_t mByte = numBlock * sizeof(int);\n","\tdouble start, stopGPU, stopCPU, speedup;\n","\n","\tprintf(\"\\n****  test on parallel reduction  ****\\n\");\n","\n","\t// init\n","\ta = (int *) malloc(nByte);\n","\tb = (int *) malloc(mByte);\n","\tfor (size_t i = 0; i < n; i++) a[i] = 1;  // initialize a[] = 1\n","\n","\tCHECK(cudaMalloc(&d_a, nByte));\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMalloc(&d_b, mByte));\n","\tCHECK(cudaMemset(d_b, 0, mByte));\n","\n","\t/***********************************************************/\n","\t/*                     CPU reduction                       */\n","\t/***********************************************************/\n","\tprintf(\"  Vector length: %.2f MB\\n\",n/(1024.0*1024.0));\n","\tprintf(\"\\n  CPU procedure...\\n\");\n","\tstart = seconds();\n","\tfor (ulong i = 0; i < n; i++)\n","\tsum_CPU += a[i];\n","\tstopCPU = seconds() - start;\n","\tprintf(\"    Elapsed time: %f (sec) \\n\", stopCPU);\n","\tprintf(\"    sum: %lu\\n\",sum_CPU);\n","\n","\tprintf(\"\\n  GPU kernels (mem required %lu bytes)\\n\", nByte);\n","\n","\t/***********************************************************/\n","\t/*         KERNEL blockParReduce1 (divergent)              */\n","\t/***********************************************************/\n","\t// block by block parallel implementation with divergence\n","\tprintf(\"\\n  Launch kernel: blockParReduce1...\\n\");\n","\tstart = seconds();\n","\tblockParReduce1<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaGetLastError());\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\n","\t// memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\n","\t// check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++) sum_GPU += b[i];\n","\tassert(sum_GPU == n);\n","\n","\t// copy and reset vectors on GPU\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMemset(d_b, 0, mByte));\n","\n","\t/***********************************************************/\n","\t/*        KERNEL blockParReduce2  (non divergent)          */\n","\t/***********************************************************/\n","\t// block by block parallel implementation without divergence\n","\tprintf(\"\\n  Launch kernel: blockParReduce2...\\n\");\n","\tstart = seconds();\n","\tblockParReduce2<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\tCHECK(cudaGetLastError());\n","\n","\t// memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\n","\t// check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++) sum_GPU += b[i];\n","\tassert(sum_GPU == n);\n","\n","\t// copy and reset vectors on GPU\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMemset(d_b, 0, mByte));\n","\n","\t/***********************************************************/\n","\t/*           KERNEL blockParReduce3 (with smem)            */\n","\t/***********************************************************/\n","\t// block by block parallel implementation using warp reduction\n","\tprintf(\"\\n  Launch kernel: blockParReduce3...\\n\");\n","\tstart = seconds();\n","\tblockParReduce3<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU, speedup);\n","\tCHECK(cudaGetLastError());\n","\n","\t// memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\n","\t// check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++) sum_GPU += b[i];\n","\tassert(sum_GPU == n);\n","\n","\t// copy and reset vectors on GPU\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMemset(d_b, 0, mByte));\n","\n","\t/***********************************************************/\n","\t/*        KERNEL blockParReduce4  (warp reducton)          */\n","\t/***********************************************************/\n","\t// block by block parallel implementation with smem\n","\tprintf(\"\\n  Launch kernel: blockParReduce4...\\n\");\n","\tstart = seconds();\n","\tblockParReduce4<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU, speedup);\n","\tCHECK(cudaGetLastError());\n","\n","\t// memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\n","\t// check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++) {\n","\t\tsum_GPU += b[i];\n","\t\t//printf(\"b[%d] = %d\\n\",i,b[i]);\n","\t}\n","\tassert(sum_GPU == n);\n","\n","\t// copy and reset vectors on GPU\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMemset(d_b, 0, mByte));\n","\n","   /***********************************************************/\n","\t/*           KERNEL blockParReduce5  (unroll)              */\n","\t/***********************************************************/\n","\t// block by block parallel implementation with unroll + smem\n","\tprintf(\"\\n  Launch kernel: blockParReduce5...\\n\");\n","\tstart = seconds();\n","\tblockParReduce5<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU, speedup);\n","\tCHECK(cudaGetLastError());\n","\n","\t// memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\n","\t// check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++) {\n","\t\tsum_GPU += b[i];\n","\t\t//printf(\"b[%d] = %d\\n\",i,b[i]);\n","\t}\n","\tassert(sum_GPU == n);\n","\n","\t// copy and reset vectors on GPU\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMemset(d_b, 0, mByte));\n","\n","   /***********************************************************/\n","\t/*         KERNEL blockParReduce6  (bock unroll)           */\n","\t/***********************************************************/\n","\t// block by block parallel implementation with unroll + smem\n","\tprintf(\"\\n  Launch kernel: blockParReduce6...\\n\");\n","\tstart = seconds();\n","\tblockParReduce6<<<numBlock/8, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU, speedup);\n","\tCHECK(cudaGetLastError());\n","\n","\t// memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\n","\t// check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++) {\n","\t\tsum_GPU += b[i];\n","\t\t//printf(\"b[%d] = %d\\n\",i,b[i]);\n","\t}\n","\tassert(sum_GPU == n);\n","\n","\tcudaFree(d_a);\n","\treturn 0;\n","}"],"metadata":{"id":"aF_XWzGyNmBL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvcc -arch=sm_75  src/lez6/preduceUnroll.cu -o preduce\n","!./preduce"],"metadata":{"id":"HCyY1Y8_sB0S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↘️ **TODO...**"],"metadata":{"id":"1wm23k6-G38i"}},{"cell_type":"markdown","source":["Kernel privo di divergenza + unrolling:\n","\n","* Introdurre warp unrolling\n","* Introdurre block unrolling\n","* Specializzare su diversi num blocchi\n","* Confrontare i vari kernel...\n","\n","\n"],"metadata":{"id":"WK1z7xCcSz5v"}},{"cell_type":"code","source":["%%cuda_group_save --name \"preduceUnroll.cu\" --group \"lez6\"\n","\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <assert.h>\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","#define SMEM_DIM 1024\n","\n","/*\n","*  Block by block parallel implementation with divergence (sequential schema)\n","*/\n","__global__ void blockParReduce1(int *in, int *out, ulong n) {\n","\n","\tint tid = threadIdx.x;\n","\tint idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// boundary check\n","\tif (idx >= n)\n","\t\treturn;\n","\n","\t// convert global data pointer to the local pointer of this block\n","\tint *thisBlock = in + blockIdx.x * blockDim.x;\n","\n","\t// in-place reduction in global memory\n","\tfor (int stride = 1; stride < blockDim.x; stride *= 2) {\n","\t\tif ((tid % (2 * stride)) == 0)\n","\t\t\tthisBlock[tid] += thisBlock[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = thisBlock[0];\n","}\n","\n","/*\n","*  Block by block parallel implementation without divergence (interleaved schema)\n","*/\n","__global__ void blockParReduce2(int *in, int *out, ulong n) {\n","\n","\tuint tid = threadIdx.x;\n","\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// boundary check\n","\tif (idx >= n)\n","\t\treturn;\n","\n","\t// convert global data pointer to the local pointer of this block\n","\tint *thisBlock = in + blockIdx.x * blockDim.x;\n","\n","\t// in-place reduction in global memory\n","\tfor (int stride = blockDim.x / 2; stride > 0; stride >>= 1)  {\n","\t\tif (tid < stride)\n","\t\t\tthisBlock[tid] += thisBlock[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = thisBlock[0];\n","}\n","\n","/*\n","* Using shared memory (no divergence nor bank conflicts)\n","*/\n","__global__ void blockParReduce3(int *in, int *out, ulong n) {\n","\n","\t// shared mem\n","\t__shared__ int smem[SMEM_DIM];\n","\n","\tunsigned int tid = threadIdx.x;\n","\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// load shared mem\n","\tsmem[tid] = (idx < n) ? in[idx] : 0;\n","\n","\t// synchronize within threadblock\n","\t__syncthreads();\n","\n","\t// do reduction in shared mem\n","\tfor (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n","\t\tif (tid < stride)\n","\t\t\tsmem[tid] += smem[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = smem[0];\n","}\n","\n","/*\n","*  Block by block parallel implementation using warp reduction\n","*/\n","__global__ void blockParReduce4(int *in, int *out, ulong n) {\n","\tint tid = blockIdx.x * blockDim.x + threadIdx.x;\n","\tint lane = threadIdx.x % warpSize;\n","\tint wid = threadIdx.x / warpSize;\n","\n","\tstatic __shared__ int shared[32];  \t// Shared mem for 32 partial sums\n","\tint val = in[tid];  \t\t\t\t\t\t// Each thread reads one element\n","\n","\t// 1° warp-shuffle reduction\n","\tfor (int offset = warpSize/2; offset > 0; offset /= 2)\n","\t\tval += __shfl_down_sync(0xffffffff, val, offset, 32);\n","\n","\tif (lane==0) shared[wid] = val; \t// Write reduced value to shared memory\n","\n","\t__syncthreads();          \t\t\t// Wait for all partial reductions\n","\n","\t// hereafter, just warp 0 (final warp-shuffle reduction)\n","\tif (wid == 0){\n","\t\tint val = shared[lane];\n","\n","\t\tfor (int offset = warpSize/2; offset > 0; offset >>= 1)\n","\t\t\tval += __shfl_down_sync(0xffffffff, val, offset);\n","\n","\t\t\t// write result for this block to global mem\n","\t\tif (threadIdx.x == 0)\n","\t\t\tout[blockIdx.x] = val;\n","\t}\n","}\n","\n","/*\n","*  Warp reduction using shared memory\n","*/\n","__device__ void warpReduce(volatile int *smem, unsigned int tid) {\n","   smem[tid] += smem[tid+32];\n","   smem[tid] += smem[tid+16];\n","   smem[tid] += smem[tid+8];\n","   smem[tid] += smem[tid+4];\n","   smem[tid] += smem[tid+2];\n","   smem[tid] += smem[tid+1];\n"," }\n","\n","/*\n","*  Block reduction using shared memory\n","*/\n","__device__ void blockReduce(volatile int *smem, unsigned int tid) {\n","   // block in-place reduction in shared memory\n","   if (blockDim.x >= 1024 && tid < 512) smem[tid] += smem[tid + 512];\n","   __syncthreads();\n","\n","   if (blockDim.x >= 512 && tid < 256) smem[tid] += smem[tid + 256];\n","   __syncthreads();\n","\n","   if (blockDim.x >= 256 && tid < 128) smem[tid] += smem[tid + 128];\n","   __syncthreads();\n","\n","   if (blockDim.x >= 128 && tid < 64)  smem[tid] += smem[tid + 64];\n","   __syncthreads();\n","\n","   // unrolling warp\n","   if (tid < 32) warpReduce(smem, tid);\n","}\n","\n","/*\n","*  Block by block parallel implementation using complete unroll for loop + smem\n","*/\n","__global__ void blockParReduce5(int *in, int *out, int n) {\n","   // SMEM for each block\n","   static __shared__ int smem[SMEM_DIM];\n","\n","   // set thread ID\n","   int tid = threadIdx.x;\n","\n","   // boundary check\n","   int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","   if (idx >= n) return;\n","\n","   // convert global data pointer to the local pointer of this block\n","   int *idata = in + blockIdx.x * blockDim.x;\n","\n","   // set to smem by each threads\n","   smem[tid] = idata[tid];\n","   __syncthreads();\n","\n","   // block in-place reduction in shared memory\n","   blockReduce(smem, tid);\n","\n","   // write result for this block to global mem\n","   if (tid == 0) out[blockIdx.x] = smem[0];\n","}\n","\n","/*\n"," *  Multi block parallel implementation with block and warp unrolling\n"," */\n"," __global__ void blockParReduce6(int *in, int *out, ulong n) {\n","\n","   // TODO\n","\n","}\n","\n","/*\n","* MAIN: test on parallel reduction\n","*/\n","int main() {\n","\tint *a, *b, *d_a, *d_b;\n","\tint blockSize = 1024;             // block dim 1D\n","\tsize_t numBlock = 1024*1024;      // grid dim 1D\n","\tsize_t n = blockSize * numBlock;  // array dims\n","\tsize_t sum_CPU = 0, sum_GPU = 0;\n","\tsize_t nByte = n*sizeof(int);\n","\tsize_t mByte = numBlock * sizeof(int);\n","\tdouble start, stopGPU, stopCPU, speedup;\n","\n","\tprintf(\"\\n****  test on parallel reduction  ****\\n\");\n","\n","\t// init\n","\ta = (int *) malloc(nByte);\n","\tb = (int *) malloc(mByte);\n","\tfor (size_t i = 0; i < n; i++) a[i] = 1;  // initialize a[] = 1\n","\n","\tCHECK(cudaMalloc(&d_a, nByte));\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMalloc(&d_b, mByte));\n","\tCHECK(cudaMemset(d_b, 0, mByte));\n","\n","\t/***********************************************************/\n","\t/*                     CPU reduction                       */\n","\t/***********************************************************/\n","\tprintf(\"  Vector length: %.2f MB\\n\",n/(1024.0*1024.0));\n","\tprintf(\"\\n  CPU procedure...\\n\");\n","\tstart = seconds();\n","\tfor (ulong i = 0; i < n; i++)\n","\tsum_CPU += a[i];\n","\tstopCPU = seconds() - start;\n","\tprintf(\"    Elapsed time: %f (sec) \\n\", stopCPU);\n","\tprintf(\"    sum: %lu\\n\",sum_CPU);\n","\n","\tprintf(\"\\n  GPU kernels (mem required %lu bytes)\\n\", nByte);\n","\n","\t/***********************************************************/\n","\t/*         KERNEL blockParReduce1 (divergent)              */\n","\t/***********************************************************/\n","\t// block by block parallel implementation with divergence\n","\tprintf(\"\\n  Launch kernel: blockParReduce1...\\n\");\n","\tstart = seconds();\n","\tblockParReduce1<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaGetLastError());\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\n","\t// memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\n","\t// check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++) sum_GPU += b[i];\n","\tassert(sum_GPU == n);\n","\n","\t// copy and reset vectors on GPU\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMemset(d_b, 0, mByte));\n","\n","\t/***********************************************************/\n","\t/*        KERNEL blockParReduce2  (non divergent)          */\n","\t/***********************************************************/\n","\t// block by block parallel implementation without divergence\n","\tprintf(\"\\n  Launch kernel: blockParReduce2...\\n\");\n","\tstart = seconds();\n","\tblockParReduce2<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\tCHECK(cudaGetLastError());\n","\n","\t// memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\n","\t// check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++) sum_GPU += b[i];\n","\tassert(sum_GPU == n);\n","\n","\t// copy and reset vectors on GPU\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMemset(d_b, 0, mByte));\n","\n","\t/***********************************************************/\n","\t/*           KERNEL blockParReduce3 (with smem)            */\n","\t/***********************************************************/\n","\t// block by block parallel implementation using warp reduction\n","\tprintf(\"\\n  Launch kernel: blockParReduce3...\\n\");\n","\tstart = seconds();\n","\tblockParReduce3<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU, speedup);\n","\tCHECK(cudaGetLastError());\n","\n","\t// memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\n","\t// check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++) sum_GPU += b[i];\n","\tassert(sum_GPU == n);\n","\n","\t// copy and reset vectors on GPU\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMemset(d_b, 0, mByte));\n","\n","\t/***********************************************************/\n","\t/*        KERNEL blockParReduce4  (warp reducton)          */\n","\t/***********************************************************/\n","\t// block by block parallel implementation with smem\n","\tprintf(\"\\n  Launch kernel: blockParReduce4...\\n\");\n","\tstart = seconds();\n","\tblockParReduce4<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU, speedup);\n","\tCHECK(cudaGetLastError());\n","\n","\t// memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\n","\t// check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++) {\n","\t\tsum_GPU += b[i];\n","\t\t//printf(\"b[%d] = %d\\n\",i,b[i]);\n","\t}\n","\tassert(sum_GPU == n);\n","\n","\t// copy and reset vectors on GPU\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMemset(d_b, 0, mByte));\n","\n","   /***********************************************************/\n","\t/*           KERNEL blockParReduce5  (unroll)              */\n","\t/***********************************************************/\n","\t// block by block parallel implementation with unroll + smem\n","\tprintf(\"\\n  Launch kernel: blockParReduce5...\\n\");\n","\tstart = seconds();\n","\tblockParReduce5<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU, speedup);\n","\tCHECK(cudaGetLastError());\n","\n","\t// memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\n","\t// check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++) {\n","\t\tsum_GPU += b[i];\n","\t\t//printf(\"b[%d] = %d\\n\",i,b[i]);\n","\t}\n","\tassert(sum_GPU == n);\n","\n","\t// copy and reset vectors on GPU\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMemset(d_b, 0, mByte));\n","\n","   /***********************************************************/\n","\t/*         KERNEL blockParReduce6  (bock unroll)           */\n","\t/***********************************************************/\n","\t// block by block parallel implementation with unroll + smem\n","\tprintf(\"\\n  Launch kernel: blockParReduce6...\\n\");\n","\tstart = seconds();\n","\tblockParReduce6<<<numBlock/8, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU, speedup);\n","\tCHECK(cudaGetLastError());\n","\n","\t// memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\n","\t// check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++) {\n","\t\tsum_GPU += b[i];\n","\t\t//printf(\"b[%d] = %d\\n\",i,b[i]);\n","\t}\n","\tassert(sum_GPU == n);\n","\n","\tcudaFree(d_a);\n","\treturn 0;\n","}"],"metadata":{"id":"at7HdfGfkzdw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvcc -arch=sm_70 src/preduceUnroll.cu -o preduceUnroll\n","!./preduceUnroll"],"metadata":{"id":"9u9U4XHAncOe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SOFMQZAkjlLW"},"source":["# ✅ Parallelismo dinamico"]},{"cell_type":"code","source":["%%cuda --name nestedHelloWorld.cu\n","#include <stdio.h>\n","\n","/*\n"," * A simple example of nested kernel launches from the GPU. Each thread displays\n"," * its information when execution begins, and also diagnostics when the next\n"," * lowest nesting layer completes.\n"," */\n","\n","__global__ void nestedHelloWorld(int const iSize, int iDepth) {\n","\tint tid = threadIdx.x;\n","\tprintf(\"Recursion=%d: Hello World from thread %d block %d\\n\", iDepth, tid,\tblockIdx.x);\n","\n","\t// condition to stop recursive execution\n","\tif (iSize == 1)\n","\t\treturn;\n","\n","\t// reduce block size to half\n","\tint nthreads = iSize >> 1;\n","\n","\n","\t// thread 0 launches child grid recursively\n","\tif (tid == 0 && nthreads > 0) {\n","\t\tnestedHelloWorld<<<gridDim.x, nthreads>>>(nthreads, ++iDepth);\n","\t\tprintf(\"-------> nested execution depth: %d\\n\", iDepth);\n","\t}\n","}\n","\n","int main(int argc, char **argv) {\n","\tint size = 8;\n","\tint blocksize = 8;   // initial block size\n","\tint igrid = 2;\n","\n","\tsize = igrid * blocksize;\n","\n","\tdim3 block(blocksize, 1);\n","\tdim3 grid((size + block.x - 1) / block.x, 1);\n","\tprintf(\"%s Execution Configuration: grid %d block %d\\n\", argv[0], grid.x,\n","\t\t\tblock.x);\n","\n","\tnestedHelloWorld<<<grid, block>>>(block.x, 0);\n","\n","\tcudaDeviceReset();\n","\treturn 0;\n","}"],"metadata":{"id":"WJQmiG5fyEqy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"lip8_s9FRFd8"}},{"cell_type":"code","source":["!nvcc -arch=sm_70 -dc src/nestedHelloWorld.cu\n","!nvcc -arch=sm_70 *.o -o nestedHelloWorld\n","!./nestedHelloWorld\n"],"metadata":{"id":"F4AkCagXzdAF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Mandelbrot"],"metadata":{"id":"khcgXsH8q-Ow"}},{"cell_type":"code","source":["%%cuda_group_save --name \"mandelbrot.cu\" --group \"lez6\"\n","\n","#include \"ppm.h\"\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","#define MAX_DWELL 1024\n","#define BS 256\n","#define CUT_DWELL (MAX_DWELL / 4)\n","\n","/** a useful function to compute the number of threads */\n","int divup(int x, int y) { return x / y + (x % y ? 1 : 0); }\n","\n","/** gets the color, given the dwell */\n","void dwell_color(int *r, int *g, int *b, int dwell);\n","\n","/** a simple complex type */\n","struct complex {\n","\t__host__ __device__ complex(float re, float im = 0) {\n","\t\tthis->re = re;\n","\t\tthis->im = im;\n","\t}\n","\tfloat re, im;   // real and imaginary part\n","};\n","\n","// operator overloads for complex numbers\n","inline __host__ __device__ complex operator+(const complex &a, const complex &b) {\n","\treturn complex(a.re + b.re, a.im + b.im);\n","}\n","\n","inline __host__ __device__ complex operator-(const complex &a) {\n","   return complex(-a.re, -a.im);\n","}\n","\n","inline __host__ __device__ complex operator-(const complex &a, const complex &b) {\n","\treturn complex(a.re - b.re, a.im - b.im);\n","}\n","\n","inline __host__ __device__ complex operator*(const complex &a, const complex &b) {\n","\treturn complex(a.re * b.re - a.im * b.im, a.im * b.re + a.re * b.im);\n","}\n","\n","inline __host__ __device__ float abs2(const complex &a) {\n","\treturn a.re * a.re + a.im * a.im;\n","}\n","\n","inline __host__ __device__ complex operator/(const complex &a, const complex &b) {\n","\tfloat invabs2 = 1 / abs2(b);\n","\treturn complex((a.re * b.re + a.im * b.im) * invabs2, (a.im * b.re - b.im * a.re) * invabs2);\n","}\n","\n","\n","/** computes the dwell for a single pixel */\n","__device__ int pixel_dwell (int w, int h, complex cmin, complex cmax, int x, int y) {\n","\tcomplex dc = cmax - cmin;\n","\tfloat fx = (float)x / w, fy = (float)y / h;\n","\tcomplex c = cmin + complex(fx * dc.re, fy * dc.im);\n","\tint dwell = 0;\n","\tcomplex z = c;\n","\twhile(dwell < MAX_DWELL && abs2(z) < 2 * 2) {\n","\t\tz = z * z + c;\n","\t\tdwell++;\n","\t}\n","\treturn dwell;\n","}\n","\n","/** computes the dwells for Mandelbrot image\n","\t\t@param dwells the output array\n","\t\t@param w the width of the output image\n","\t\t@param h the height of the output image\n","\t\t@param cmin the complex value associated with the left-bottom corner of the\n","\t\timage\n","\t\t@param cmax the complex value associated with the right-top corner of the\n","\t\timage\n"," */\n","__global__ void mandelbrot_k (int *dwells, int w, int h, complex cmin, complex cmax) {\n","\t// complex value to start iteration (c)\n","\tint x = threadIdx.x + blockIdx.x * blockDim.x;\n","\tint y = threadIdx.y + blockIdx.y * blockDim.y;\n","\tint dwell = pixel_dwell(w, h, cmin, cmax, x, y);\n","\tdwells[y * w + x] = dwell;\n","}\n","\n","\n","/** data size */\n","#define H (8*1024)\n","#define W (8*1024)\n","#define IMAGE_PATH \"./mandelbrot.ppm\"\n","\n","int main(int argc, char **argv) {\n","\t// allocate memory\n","\tint w = W,  h = H;\n","\tsize_t dwell_sz = w * h * sizeof(int);\n","\tint *h_dwells, *d_dwells;\n","\tCHECK(cudaMalloc(&d_dwells, dwell_sz));\n","\th_dwells = (int*)malloc(dwell_sz);\n","\n","\t// compute the dwells, copy them back\n","\tdim3 bs(64, 4), grid(divup(w, bs.x), divup(h, bs.y));\n","\tdouble start = seconds();\n","\tmandelbrot_k<<<grid, bs>>> (d_dwells, w, h, complex(-1.5, -1), complex(0.5, 1));\n","\tCHECK(cudaDeviceSynchronize());\n","   double gpu_time = seconds() - start;\n","\tCHECK(cudaMemcpy(h_dwells, d_dwells, dwell_sz, cudaMemcpyDeviceToHost));\n","\n","   // print performance\n","\tprintf(\"Mandelbrot set computed in %.5lf s, at %.3lf Mpix/s\\n\", gpu_time, h * w * 1e-6 / gpu_time);\n","\n","\n","   // save the image\n","   PPM *ppm = ppm_make(w, h, (pel) {0,0,0});\n","   for (int y = 0; y < h; y++) {\n","\t\tfor (int x = 0; x < w; x++) {\n","\t\t\tint r, g, b;\n","\t\t\tdwell_color(&r, &g, &b, h_dwells[y * w + x]);\n","         ppm_set(ppm, x, y, (pel) {(color)r,(color)g,(color)b});\n","\t\t}\n","   }\n","   ppm_write(ppm, IMAGE_PATH);\n","   printf(\"Image saved to %s\\n\", IMAGE_PATH);\n","\n","\n","\t// free data\n","\tcudaFree(d_dwells);\n","\tfree(h_dwells);\n","\treturn 0;\n","}\n","\n","/** gets the color, given the dwell (on host) */\n","void dwell_color(int *r, int *g, int *b, int dwell) {\n","\t// black for the Mandelbrot set\n","\tif(dwell >= MAX_DWELL) {\n","\t\t*r = *g = *b = 0;\n","\t} else {\n","\t\t// cut at zero\n","\t\tif(dwell < 0)\n","\t\t\tdwell = 0;\n","\t\tif(dwell <= CUT_DWELL) {\n","\t\t\t// from black to blue the first half\n","\t\t\t*r = *g = 0;\n","\t\t\t*b = 128 + dwell * 127 / (CUT_DWELL);\n","\t\t} else {\n","\t\t\t// from blue to white for the second half\n","\t\t\t*b = 255;\n","\t\t\t*r = *g = (dwell - CUT_DWELL) * 255 / (MAX_DWELL - CUT_DWELL);\n","\t\t}\n","\t}\n","}"],"metadata":{"id":"1ZrboXb4q9xg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"QkJFNykLrvh-"}},{"cell_type":"code","source":["# Compilazione ed esecuzione\n","!nvcc -arch=sm_75 src/lez6/mandelbrot.cu -o mandelbrot -I GPUcomputing/utils/PPM GPUcomputing/utils/PPM/ppm.cpp\n","!./mandelbrot"],"metadata":{"id":"I9ZhM_3_rvh_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%cuda_group_save --name \"mandelbrot_dyn.cu\" --group \"lez6\"\n","\n","#include \"ppm.h\"\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","\n","#define CUT_DWELL (MAX_DWELL / 4)\n","#define MAX_DWELL 1024\n","/** block size along */\n","#define BSX 64\n","#define BSY 4\n","/** maximum recursion depth */\n","#define MAX_DEPTH 4\n","/** region below which do per-pixel */\n","#define MIN_SIZE 32\n","/** subdivision factor along each axis */\n","#define SUBDIV 4\n","/** subdivision when launched from host */\n","#define INIT_SUBDIV 32\n","#define NEUT_DWELL (MAX_DWELL + 1)\n","#define DIFF_DWELL (-1)\n","\n","/** a useful function to compute the number of threads */\n","__device__ int divup(int x, int y) { return x / y + (x % y ? 1 : 0); }\n","\n","/** gets the color, given the dwell */\n","void dwell_color(int *r, int *g, int *b, int dwell);\n","\n","/** a simple complex type */\n","struct complex {\n","\t__host__ __device__ complex(float re, float im = 0) {\n","\t\tthis->re = re;\n","\t\tthis->im = im;\n","\t}\n","\tfloat re, im;   // real and imaginary part\n","};\n","\n","// operator overloads for complex numbers\n","inline __host__ __device__ complex operator+(const complex &a, const complex &b) {\n","\treturn complex(a.re + b.re, a.im + b.im);\n","}\n","\n","inline __host__ __device__ complex operator-(const complex &a) {\n","   return complex(-a.re, -a.im);\n","}\n","\n","inline __host__ __device__ complex operator-(const complex &a, const complex &b) {\n","\treturn complex(a.re - b.re, a.im - b.im);\n","}\n","\n","inline __host__ __device__ complex operator*(const complex &a, const complex &b) {\n","\treturn complex(a.re * b.re - a.im * b.im, a.im * b.re + a.re * b.im);\n","}\n","\n","inline __host__ __device__ float abs2(const complex &a) {\n","\treturn a.re * a.re + a.im * a.im;\n","}\n","\n","inline __host__ __device__ complex operator/(const complex &a, const complex &b) {\n","\tfloat invabs2 = 1 / abs2(b);\n","\treturn complex((a.re * b.re + a.im * b.im) * invabs2, (a.im * b.re - b.im * a.re) * invabs2);\n","}\n","\n","\n","\n","/********************************************************/\n","/*                   with dyn parall                    */\n","/********************************************************/\n","\n","/** find the dwell for the pixel */\n","__device__ int pixel_dwell(int w, int h, complex cmin, complex cmax, int x, int y) {\n","\tcomplex dc = cmax - cmin;\n","\tfloat fx = (float)x / w, fy = (float)y / h;\n","\tcomplex c = cmin + complex(fx * dc.re, fy * dc.im);\n","\tint dwell = 0;\n","\tcomplex z = c;\n","\twhile(dwell < MAX_DWELL && abs2(z) < 2 * 2) {\n","\t\tz = z * z + c;\n","\t\tdwell++;\n","\t}\n","\treturn dwell;\n","}\n","\n","\n","/** binary operation for common dwell \"reduction\": MAX_DWELL + 1 = neutral element, -1 = dwells are different */\n","__device__ int same_dwell(int d1, int d2) {\n","   if(d1 == d2)\n","      return d1;\n","   else if(d1 == NEUT_DWELL || d2 == NEUT_DWELL)\n","      return min(d1, d2);\n","   else\n","      return DIFF_DWELL;\n","}\n","\n","/** evaluates the common border dwell, if it exists */\n","__device__ int border_dwell(int w, int h, complex cmin, complex cmax, int x0, int y0, int d) {\n","   // check whether all boundary pixels have the same dwell\n","   int tid = threadIdx.y * blockDim.x + threadIdx.x;\n","   int bs = blockDim.x * blockDim.y;\n","   int comm_dwell = NEUT_DWELL;\n","   // for all boundary pixels, distributed across threads\n","   for(int r = tid; r < d; r += bs) {\n","      // for each boundary: b = 0 is east, then counter-clockwise\n","      for(int b = 0; b < 4; b++) {\n","         int x = b % 2 != 0 ? x0 + r : (b == 0 ? x0 + d - 1 : x0);\n","         int y = b % 2 == 0 ? y0 + r : (b == 1 ? y0 + d - 1 : y0);\n","         int dwell = pixel_dwell(w, h, cmin, cmax, x, y);\n","         comm_dwell = same_dwell(comm_dwell, dwell);\n","      }\n","   }\n","   // reduce across threads in the block\n","   __shared__ int ldwells[BSX * BSY];\n","   int nt = min(d, BSX * BSY);\n","   if(tid < nt)\n","      ldwells[tid] = comm_dwell;\n","   __syncthreads();\n","   for(; nt > 1; nt /= 2) {\n","      if(tid < nt / 2)\n","         ldwells[tid] = same_dwell(ldwells[tid], ldwells[tid + nt / 2]);\n","      __syncthreads();\n","   }\n","   return ldwells[0];\n","}\n","\n","/** the kernel to fill the image region with a specific dwell value */\n","__global__ void dwell_fill_k(int *dwells, int w, int x0, int y0, int d, int dwell) {\n","   int x = threadIdx.x + blockIdx.x * blockDim.x;\n","   int y = threadIdx.y + blockIdx.y * blockDim.y;\n","   if(x < d && y < d) {\n","      x += x0, y += y0;\n","      dwells[y * w + x] = dwell;\n","   }\n","}\n","\n","/** the kernel to fill in per-pixel values of the portion of the Mandelbrot set */\n","__global__ void mandelbrot_pixel_k(int *dwells, int w, int h, complex cmin, complex cmax, int x0, int y0, int d) {\n","   int x = threadIdx.x + blockDim.x * blockIdx.x;\n","   int y = threadIdx.y + blockDim.y * blockIdx.y;\n","   if(x < d && y < d) {\n","      x += x0, y += y0;\n","      dwells[y * w + x] = pixel_dwell(w, h, cmin, cmax, x, y);\n","   }\n","}\n","\n","/** computes the dwells for Mandelbrot image using dynamic parallelism; one\n","\t\tblock is launched per pixel\n","\t\t@param dwells the output array\n","\t\t@param w the width of the output image\n","\t\t@param h the height of the output image\n","\t\t@param cmin the complex value associated with the left-bottom corner of the\n","\t\timage\n","\t\t@param cmax the complex value associated with the right-top corner of the\n","\t\timage\n","\t\t@param x0 the starting x coordinate of the portion to compute\n","\t\t@param y0 the starting y coordinate of the portion to compute\n","\t\t@param d the size of the portion to compute (the portion is always a square)\n","\t\t@param depth kernel invocation depth\n","\t\t@remarks the algorithm reverts to per-pixel Mandelbrot evaluation once\n","\t\teither maximum depth or minimum size is reached\n","*/\n","__global__ void mandelbrot_block_k(int *dwells, int w, int h, complex cmin, complex cmax, int x0, int y0, int d, int depth) {\n","   x0 += d * blockIdx.x, y0 += d * blockIdx.y;\n","   int comm_dwell = border_dwell(w, h, cmin, cmax, x0, y0, d);\n","   if(threadIdx.x == 0 && threadIdx.y == 0) {\n","      if(comm_dwell != DIFF_DWELL) {\n","         // uniform dwell, just fill\n","         dim3 bs(BSX, BSY), grid(divup(d, BSX), divup(d, BSY));\n","         dwell_fill_k<<<grid, bs>>>(dwells, w, x0, y0, d, comm_dwell);\n","      } else if(depth + 1 < MAX_DEPTH && d / SUBDIV > MIN_SIZE) {\n","         // subdivide recursively\n","         dim3 bs(blockDim.x, blockDim.y), grid(SUBDIV, SUBDIV);\n","         mandelbrot_block_k<<<grid, bs>>>(dwells, w, h, cmin, cmax, x0, y0, d / SUBDIV, depth\t+ 1);\n","      } else {\n","         // leaf, per-pixel kernel\n","         dim3 bs(BSX, BSY), grid(divup(d, BSX), divup(d, BSY));\n","         mandelbrot_pixel_k<<<grid, bs>>>(dwells, w, h, cmin, cmax, x0, y0, d);\n","      }\n","   }\n"," }\n","\n","/********************************************************/\n","/*                        MAIN                          */\n","/********************************************************/\n","\n","/** data size */\n","#define H (1024)\n","#define W (1024)\n","#define IMAGE_PATH \"./mandelbrot.ppm\"\n","\n","int main(int argc, char **argv) {\n","\t// allocate memory\n","\tint w = W,  h = H;\n","\tsize_t dwell_sz = w * h * sizeof(int);\n","\tint *h_dwells, *d_dwells;\n","\tCHECK(cudaMalloc(&d_dwells, dwell_sz));\n","\th_dwells = (int*)malloc(dwell_sz);\n","\n","\t// compute the dwells, copy them back\n","\tdim3 bs(BSX, BSY), grid(INIT_SUBDIV, INIT_SUBDIV);\n","\tdouble start = seconds();\n","\tmandelbrot_block_k<<<grid, bs>>>(d_dwells, w, h, complex(-1.5, -1), complex(0.5, 1), 0, 0, W / INIT_SUBDIV, 1);\n","\tCHECK(cudaDeviceSynchronize());\n","   double gpu_time = seconds() - start;\n","\tCHECK(cudaMemcpy(h_dwells, d_dwells, dwell_sz, cudaMemcpyDeviceToHost));\n","\n","   // print performance\n","\tprintf(\"Mandelbrot set computed in %.5lf s, at %.3lf Mpix/s\\n\", gpu_time, h * w * 1e-6 / gpu_time);\n","\n","\n","   // save the image\n","   PPM *ppm = ppm_make(w, h, (pel) {0,0,0});\n","   for (int y = 0; y < h; y++) {\n","\t\tfor (int x = 0; x < w; x++) {\n","\t\t\tint r, g, b;\n","\t\t\tdwell_color(&r, &g, &b, h_dwells[y * w + x]);\n","         ppm_set(ppm, x, y, (pel) {(color)r,(color)g,(color)b});\n","\t\t}\n","   }\n","   ppm_write(ppm, IMAGE_PATH);\n","   printf(\"Image saved to %s\\n\", IMAGE_PATH);\n","\n","\t// free data\n","\tcudaFree(d_dwells);\n","\tfree(h_dwells);\n","\treturn 0;\n","}\n","\n","\n","/** gets the color, given the dwell (on host) */\n","void dwell_color(int *r, int *g, int *b, int dwell) {\n","\t// black for the Mandelbrot set\n","\tif(dwell >= MAX_DWELL) {\n","\t\t*r = *g = *b = 0;\n","\t} else {\n","\t\t// cut at zero\n","\t\tif(dwell < 0)\n","\t\t\tdwell = 0;\n","\t\tif(dwell <= CUT_DWELL) {\n","\t\t\t// from black to blue the first half\n","\t\t\t*r = *g = 0;\n","\t\t\t*b = 128 + dwell * 127 / (CUT_DWELL);\n","\t\t} else {\n","\t\t\t// from blue to white for the second half\n","\t\t\t*b = 255;\n","\t\t\t*r = *g = (dwell - CUT_DWELL) * 255 / (MAX_DWELL - CUT_DWELL);\n","\t\t}\n","\t}\n","}"],"metadata":{"id":"NeXybKyFq3n-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"8KzZQEeysuw6"}},{"cell_type":"code","source":["# Compilazione ed esecuzione\n","!nvcc -rdc=true  -arch=sm_75 src/lez6/mandelbrot_dyn.cu -o mandelbrot_dyn -I GPUcomputing/utils/PPM GPUcomputing/utils/PPM/ppm.cpp\n","!./mandelbrot_dyn"],"metadata":{"id":"brWEcxHKsuw7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Parallelismo dinamico nel calcolo di prodotti MQDB"],"metadata":{"id":"VR2PLOEZz4fD"}},{"cell_type":"code","source":["%%cuda --name mqdb.h\n","\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <string.h>\n","#include <math.h>\n","#include <sys/time.h>\n","#include <time.h>\n","\n","#ifndef MQDB_H\n","#define MQDB_H\n","\n","#define randu() ((float)rand() / (float) RAND_MAX)\n","#define abs(x) ((x)<0 ? (-x) : (x))\n","\n","typedef unsigned long ulong;\n","typedef unsigned int uint;\n","\n","typedef struct MQDB {\n","\tchar desc[100];   // description\n","\tint nBlocks;      // num. of blocks\n","\tint *blkSize;     // block dimensions\n","\tfloat *elem;       // elements in row-major order\n","\tulong nElems;     // actual number of elements\n","} mqdb;\n","\n","typedef unsigned long ulong;\n","typedef unsigned int uint;\n","\n","// # function prototypes #\n","int genRandDims(mqdb*, uint, uint);\n","void fillBlocks(mqdb*, uint, uint, char, float);\n","mqdb mqdbConst(uint, uint, uint, float);\n","void mqdbProd(mqdb, mqdb, mqdb);\n","void matProd(mqdb, mqdb, mqdb);\n","void checkResult(mqdb, mqdb);\n","void mqdbDisplay(mqdb);\n","\n","#endif"],"metadata":{"id":"lXx-e0jR3mL2"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QVQVpcvKjkIk"},"source":["%%cuda --name MQDB-CUDA-DP.cu\n","\n","#include \"mqdb.h\"\n","#include \"../GPUcomputing/utils/common.h\"\n","\n","#define BLOCK_SIZE 16     // block size\n","\n","struct tms {\n","\tdouble GPUtmsMQDB;\n","\tdouble GPUtmsMQDBDynPar1;\n","\tdouble GPUtmsMQDBDynPark;\n","\tfloat density;\n","};\n","\n","// the kernels prototype\n","__global__ void mqdbBlockProd(mqdb, mqdb, mqdb, uint, uint, uint);\n","__global__ void mqdbProdDP1(mqdb, mqdb, mqdb, uint, uint);\n","__global__ void mqdbProdDPk(mqdb, mqdb, mqdb, uint);\n","\n","/*\n"," * Test on MQDB kernels\n"," */\n","void testKernelsMQDB(uint n, uint k, struct tms* times) {\n","\n","\t// mqdb host matrices\n","\tmqdb A, B, C, C1;\n","\n","\t// mqdb device matrices\n","\tmqdb d_A, d_B, d_C;\n","\n","\t// fill in\n","\tA = mqdbConst(n, k, 10, 1);\n","\tB = mqdbConst(n, k, 10, 1);\n","\tC = mqdbConst(n, k, 10, 1);\n","\tC1 = mqdbConst(n, k, 10, 1);\n","\n","\tulong nBytes = n * n * sizeof(float);\n","\tulong kBytes = k * sizeof(uint);\n","\tprintf(\"Memory size required = %.1f (MB)\\n\",(float)nBytes/(1024.0*1024.0));\n","\n","\t// malloc and copy on device memory\n","\td_A.nBlocks = A.nBlocks;\n","\tCHECK(cudaMalloc((void**)&d_A.blkSize, kBytes));\n","\tCHECK(cudaMemcpy(d_A.blkSize, A.blkSize, kBytes, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMalloc((void**)&d_A.elem, nBytes));\n","\tCHECK(cudaMemcpy(d_A.elem, A.elem, nBytes, cudaMemcpyHostToDevice));\n","\td_B.nBlocks = B.nBlocks;\n","\tCHECK(cudaMalloc((void**)&d_B.blkSize, kBytes));\n","\tCHECK(cudaMemcpy(d_B.blkSize, B.blkSize, kBytes, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMalloc((void**)&d_B.elem, nBytes));\n","\tCHECK(cudaMemcpy(d_B.elem, B.elem, nBytes, cudaMemcpyHostToDevice));\n","\td_C.nBlocks = C.nBlocks;\n","\tCHECK(cudaMalloc((void**)&d_C.blkSize, kBytes));\n","\tCHECK(cudaMemcpy(d_C.blkSize, C.blkSize, kBytes, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMalloc((void**)&d_C.elem, nBytes));\n","\tCHECK(cudaMemset(d_C.elem, 0.0, nBytes));\n","\n","\t/***********************************************************/\n","\t/*                     GPU MQDB product                    */\n","\t/***********************************************************/\n","\tprintf(\"Kernel MQDB product...\\n\");\n","\tdim3 block(BLOCK_SIZE, BLOCK_SIZE);\n","\tdim3 grid((n + block.x - 1) / block.x, (n + block.y - 1) / block.y);\n","\tuint sdim = 0;\n","\tdouble start = seconds();\n","\tfor (uint i = 0; i < k; i++ ) {\n","\t\tuint d = A.blkSize[i];\n","\t\tmqdbBlockProd<<<grid, block>>>(d_A, d_B, d_C, sdim, d, n);\n","\t\tsdim += d;\n","\t}\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble GPUtime2 = seconds() - start;\n","\tprintf(\"   elapsed time:                    %.2f (sec)\\n\", GPUtime2);\n","\t// copy the array 'C' back from the GPU to the CPU\n","\tCHECK(cudaMemcpy(C1.elem, d_C.elem, nBytes, cudaMemcpyDeviceToHost));\n","\tcheckResult(C,C1);\n","\tCHECK(cudaMemset(d_C.elem, 0.0, nBytes));\n","\n","\t/***********************************************************/\n","\t/*              GPU MQDB dynamic par. GRID(1)              */\n","\t/***********************************************************/\n","\tstart = seconds();\n","\tprintf(\"Kernel MQDB product with dynamic parall. GRID(1)...\\n\");\n","\tmqdbProdDP1<<< 1, 1 >>>(d_A, d_B, d_C, k, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble GPUtime3 = seconds() - start;\n","\tprintf(\"   elapsed time:                        %.2f (sec)\\n\", GPUtime3);\n","\tprintf(\"   speedup vs GPU MQDB product:         %.2f\\n\", GPUtime2/GPUtime3);\n","\t// copy the array 'C' back from the GPU to the CPU\n","\tCHECK(cudaMemcpy(C1.elem, d_C.elem, nBytes, cudaMemcpyDeviceToHost));\n","\tcheckResult(C,C1);\n","\tCHECK(cudaMemset(d_C.elem, 0.0, nBytes));\n","\n","\t/***********************************************************/\n","\t/*              GPU MQDB dynamic par. GRID(k)              */\n","\t/***********************************************************/\n","\tstart = seconds();\n","\tprintf(\"Kernel MQDB product with dynamic parall. GRID(k)...\\n\");\n","\tmqdbProdDPk<<< 1, k >>>(d_A, d_B, d_C, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble GPUtime4 = seconds() - start;\n","\tprintf(\"   elapsed time:                        %.2f (sec)\\n\", GPUtime4);\n","\tprintf(\"   speedup vs GPU MQDB product:         %.2f\\n\", GPUtime2/GPUtime4);\n","\tprintf(\"   speedup vs GPU MQDB product GRID(1): %.2f\\n\", GPUtime3/GPUtime4);\n","\t// copy the array 'C' back from the GPU to the CPU\n","\tCHECK(cudaMemcpy(C1.elem, d_C.elem, nBytes, cudaMemcpyDeviceToHost));\n","\tcheckResult(C,C1);\n","\tCHECK(cudaMemset(d_C.elem, 0.0, nBytes));\n","\n","\tCHECK(cudaFree(d_A.elem));\n","\tCHECK(cudaFree(d_B.elem));\n","\tCHECK(cudaFree(d_C.elem));\n","\n","\t// collect times\n","\ttimes->GPUtmsMQDB = GPUtime2;\n","\ttimes->GPUtmsMQDBDynPar1 = GPUtime3;\n","\ttimes->GPUtmsMQDBDynPark = GPUtime4;\n","\tfloat den = 0;\n","\tfor (uint j = 0; j < k; j++)\n","\t\tden += A.blkSize[j]*A.blkSize[j];\n","\ttimes->density = den/(n*n);\n","}\n","\n","/*\n"," * main function\n"," */\n","int main(int argc, char *argv[]) {\n","\tuint n = 16*1024;      // matrix size\n","\tuint min_k = 10;       // max num of blocks\n","\tuint max_k = 20;       // max num of blocks\n","\n","\tstruct tms times[max_k-min_k+1];\n","\n","\t// multiple tests on kernels\n","\tfor (uint k = min_k; k <= max_k; k++) {\n","\t\tprintf(\"\\n*****   k = %d --- (avg block size = %f)\\n\",k,(float)n/k);\n","\t\ttestKernelsMQDB(n, k, &times[k-min_k]);\n","\t}\n","\n","\tFILE *fd;\n","\tfd = fopen(\"res.csv\", \"w\");\n","\tif (fd == NULL) {\n","\t\tperror(\"file error!\\n\");\n","\t\texit(1);\n","\t}\n","\n","\t// write results on file\n","\tfprintf(fd,\"num blocks,\");\n","\t\tfor (uint j = 0; j <= max_k-min_k; j++)\n","\t\t\tfprintf(fd,\"%d,\",j+min_k);\n","\n","\tfprintf(fd,\"\\nKernel MQDB product,\");\n","\tfor (uint j = 0; j <= max_k-min_k; j++)\n","\t\tfprintf(fd,\"%.4f,\",times[j].GPUtmsMQDB);\n","\n","\tfprintf(fd,\"\\nKernel MQDB product with dynamic parall. GRID(1),\");\n","\tfor (uint j = 0; j <= max_k-min_k; j++)\n","\t\tfprintf(fd,\"%.4f,\",times[j].GPUtmsMQDBDynPar1);\n","\n","\tfprintf(fd,\"\\nKernel MQDB product with dynamic parall. GRID(k),\");\n","\tfor (uint j = 0; j <= max_k-min_k; j++)\n","\t\tfprintf(fd,\"%.4f,\",times[j].GPUtmsMQDBDynPark);\n","\n","\tfprintf(fd,\"\\ndensity,\");\n","\tfor (uint j = 0; j <= max_k-min_k; j++)\n","\t\tfprintf(fd,\"%.4f,\",times[j].density);\n","\n","\tfclose(fd);\n","\n","\treturn 0;\n","}\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"5RRffIPhRKAw"}},{"cell_type":"code","source":["# Compilazione ed esecuzione\n","!nvcc -arch=sm_70 -dc src/MQDB-CUDA-DP.cu GPUcomputing/lab1/MQDB/mqdb.cpp\n","!nvcc -arch=sm_70 MQDB-CUDA-DP.o mqdb.o -o mqdb_prod\n","!rm -rf *.o\n","!./mqdb_prod"],"metadata":{"id":"_ci8xYBY2oSM"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v9nRkLgeB10A"},"source":["%%cuda --name mqdb_DP.cu\n","\n","#include \"../../utils/MQDB/mqdb.h\"\n","\n","#define BLOCK_SIZE 16     // block size\n","\n","\n","/*\n"," * Kernel for block sub-matrix product of mqdb\n"," */\n","__global__ void mqdbBlockProd(mqdb A, mqdb B, mqdb C, uint sdim, uint d, uint n) {\n","\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// jump to the right block sub-matrix\n","\tint  offset = (n+1)*sdim;\n","\n","\t// each thread computes an entry of the product matrix\n","\tif ((row < d) && (col < d)) {\n","\t\tfloat val = 0;\n","\n","\t\tfor (int k = 0; k < d; k++)\n","\t\t\tval += A.elem[row * n + k + offset] * B.elem[k * n + col + offset];\n","\t\tC.elem[row * n + col + offset] = val;\n","\t}\n","}\n","\n","/*\n"," * Kernel for block sub-matrix product of mqdb: parent grid(1)\n"," */\n","__global__ void mqdbProdDP1(mqdb A, mqdb B, mqdb C, uint k, uint n) {\n","\t// using grid(1,1)\n","\tuint sdim = 0;\n","\tdim3 block(BLOCK_SIZE, BLOCK_SIZE);\n","\tdim3 grid((n + block.x - 1) / block.x, (n + block.y - 1) / block.y);\n","\tfor (uint i = 0; i < k; i++ ) {\n","\t\tuint d = A.blkSize[i];\n","\t\tmqdbBlockProd<<<grid, block>>>(A, B, C, sdim, d, n);\n","\t\tsdim += d;\n","\t}\n","}\n","\n","/*\n"," * Kernel for block sub-matrix product of mqdb: parent grid(k)\n"," */\n","__global__ void mqdbProdDPk(mqdb A, mqdb B, mqdb C, uint n) {\n","\t// using grid(1,k)\n","\tuint i = threadIdx.x;\n","\tuint sdim = 0;\n","\n","\t// block displacement\n","\tuint d = A.blkSize[i];\n","\tif (i > 0) {\n","\t\tfor (uint j = 0; j < i; j++)\n","\t\t\tsdim += A.blkSize[j];\n","\t}\n","\n","\t// grid dims\n","\tdim3 grid((d + blockDim.x - 1) / blockDim.x, (d + blockDim.y - 1) / blockDim.y);\n","\tmqdbBlockProd<<<grid,blockDim>>>(A, B, C, sdim, d, n);\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"0VXmW46URK4n"}},{"cell_type":"code","metadata":{"id":"wLxZjCx8bT3s"},"source":["# Compilazione ed esecuzione\n","!nvcc -arch=sm_37 -dc MQDB-CUDA-DP/mqdb_DP.cu MQDB-CUDA-DP/main.cu ../utils/MQDB/mqdb.cpp\n","!nvcc -arch=sm_37 mqdb_DP.o main.o mqdb.o -o mqdb_prod\n","!rm -rf *.o\n","!./mqdb_prod"],"execution_count":null,"outputs":[]}]}