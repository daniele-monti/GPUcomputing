{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1tNSh4eJL3Sh0IpPWYg8qt4bZlm2RNINy","timestamp":1743057015256}],"collapsed_sections":["NO_C5o9-xRF_","sQDPOWMUQJN8","OHR7Zs3dNs1N","vXUIQkZLCTcG","AT_wFzgkw-5W","xYysz72z216s"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["---\n","# **LAB 4 - Shared memory (SMEM)**\n","---"],"metadata":{"id":"fZYqN0UwVLC_"}},{"cell_type":"markdown","metadata":{"id":"NO_C5o9-xRF_"},"source":["# ▶️ CUDA setup"]},{"cell_type":"code","metadata":{"id":"8fekR2O4xRGE"},"source":["!nvcc --version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Psl9iouxRGE"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GPU computing notebooks download (from github)"],"metadata":{"id":"gcg1GyK5srek"}},{"cell_type":"code","source":["!git clone https://github.com/giulianogrossi/GPUcomputing.git"],"metadata":{"id":"tyHOxci3s3H8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"227eLdP5csN1"},"source":["NVCC Plugin for Jupyter notebook"]},{"cell_type":"code","source":["%cd GPUcomputing/utils/nvcc4jupyter-master/\n","!!python3 -m build\n","%load_ext nvcc4jupyter\n","%cd /content/"],"metadata":{"id":"4TzxMBFds8aT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ✅ Warp shuffle"],"metadata":{"id":"sQDPOWMUQJN8"}},{"cell_type":"code","source":["#include <stdio.h>\n","\n","/*\n","* __shfl_sync(mask, var, srcLane) returns the value of var from the thread in srcLane within the warp.\n","* __shfl_down_sync(mask, var, delta) returns the value of var from the thread delta lanes below the current thread within the warp.\n","* __shfl_up_sync(mask, var, delta) returns the value of var from the thread delta lanes above the current thread within the warp.\n","* __shfl_xor_sync(mask, var, laneMask) returns the value of var from the thread whose lane ID is the XOR of the current thread’s lane ID and laneMask.\n","*/\n","\n","/*\n","broadcast value to all other lanes\n","*/\n","__global__ void bcast(int arg) {\n","   int laneId = threadIdx.x;\n","   int value;        // unused variable for all threads except lane 0\n","   if (laneId == 0)  // only lane 0 will write to value\n","      value = arg;\n","\n","   // Synchronize all threads in warp, and get \"value\" from lane 0\n","   value = __shfl_sync(0xffffffff, value, 0);  // broadcast value to all other lanes\n","   printf(\"value[%d] = %d\\n\", laneId, value);\n","}\n","\n","/*\n","* warpReduce performs a reduction across all threads in a warp\n","*/\n","__global__ void warpReduce(int arg) {\n","   int laneId = threadIdx.x;\n","   int value = 1;  // value of 1 for all threads\n","\n","   // Use DOWN mode to perform reduction\n","   for (int i=warpSize/2; i>0; i/=2)\n","      value += __shfl_down_sync(0xffffffff, value, i);\n","\n","   // \"value\" now contains the sum across all threads\n","   printf(\"Thread %d final value = %d\\n\", laneId, value);\n","}\n","\n","/*\n","* Main function\n","*/\n","int main() {\n","   bcast<<< 1, 32 >>>(1);     // 1 block, 32 threads, 1 warp\n","   warpReduce<<< 1, 32 >>>(1); // 1 block, 32 threads, 1 warp\n","   cudaDeviceSynchronize();\n","   return 0;\n","}"],"metadata":{"id":"TdUSfojEQot7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ✅ Parallel reduction con SMEM e warp shuffle\n"],"metadata":{"id":"OHR7Zs3dNs1N"}},{"cell_type":"markdown","source":["↘️ **TODO...**"],"metadata":{"id":"DyzjzGiv8F-W"}},{"cell_type":"code","source":["%%cuda_group_save --name \"pred.cu\" --group \"lez4\"\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <assert.h>\n","#include \"../../GPUcomputing/utils/common.h\"\n","\n","#define SMEM_DIM 1024\n","\n","/*\n","*  Block by block parallel implementation with divergence (sequential schema)\n","*/\n","__global__ void blockParReduce1(int *in, int *out, ulong n) {\n","\n","\tint tid = threadIdx.x;\n","\tint idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// boundary check\n","\tif (idx >= n)\n","\t\treturn;\n","\n","\t// convert global data pointer to the local pointer of this block\n","\tint *thisBlock = in + blockIdx.x * blockDim.x;\n","\n","\t// in-place reduction in global memory\n","\tfor (int stride = 1; stride < blockDim.x; stride *= 2) {\n","\t\tif ((tid % (2 * stride)) == 0)\n","\t\t\tthisBlock[tid] += thisBlock[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = thisBlock[0];\n","}\n","\n","/*\n","*  Block by block parallel implementation without divergence (interleaved schema)\n","*/\n","__global__ void blockParReduce2(int *in, int *out, ulong n) {\n","\n","\tuint tid = threadIdx.x;\n","\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// boundary check\n","\tif (idx >= n)\n","\t\treturn;\n","\n","\t// convert global data pointer to the local pointer of this block\n","\tint *thisBlock = in + blockIdx.x * blockDim.x;\n","\n","\t// in-place reduction in global memory\n","\tfor (int stride = blockDim.x / 2; stride > 0; stride >>= 1)  {\n","\t\tif (tid < stride)\n","\t\t\tthisBlock[tid] += thisBlock[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = thisBlock[0];\n","}\n","\n","/*\n","* Using shared memory (no divergence nor bank conflicts)\n","*/\n","__global__ void blockParReduce3(int *in, int *out, ulong n) {\n","\n","\t// shared mem\n","\n","\t// load shared mem\n","\n","\t// synchronize within threadblock\n","\n","\t// do reduction in shared mem\n","\n","\t// write result for this block to global mem\n","\n","}\n","\n","/*\n","*  Block by block parallel implementation using warp reduction\n","*/\n","__global__ void blockParReduce4(int *in, int *out, ulong n) {\n","\n","\t// TODO\n","\n","}\n","\n","\n","/*\n","* MAIN: test on parallel reduction\n","*/\n","int main() {\n","\tint *a, *b, *d_a, *d_b;\n","\tint blockSize = 1024;             // block dim 1D\n","\tsize_t numBlock = 1024*1024;      // grid dim 1D\n","\tsize_t n = blockSize * numBlock;  // array dims\n","\tsize_t sum_CPU = 0, sum_GPU = 0;\n","\tsize_t nByte = n*sizeof(int);\n","\tsize_t mByte = numBlock * sizeof(int);\n","\tdouble start, stopGPU, stopCPU, speedup;\n","\n","\tprintf(\"\\n****  test on parallel reduction  ****\\n\");\n","\n","\t// init\n","\ta = (int *) malloc(nByte);\n","\tb = (int *) malloc(mByte);\n","\tfor (size_t i = 0; i < n; i++) a[i] = 1;  // initialize a[] = 1\n","\n","\tCHECK(cudaMalloc(&d_a, nByte));\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMalloc(&d_b, mByte));\n","\tCHECK(cudaMemset(d_b, 0, mByte));\n","\n","\t/***********************************************************/\n","\t/*                     CPU reduction                       */\n","\t/***********************************************************/\n","\tprintf(\"  Vector length: %.2f MB\\n\",n/(1024.0*1024.0));\n","\tprintf(\"\\n  CPU procedure...\\n\");\n","\tstart = seconds();\n","\tfor (ulong i = 0; i < n; i++)\n","\tsum_CPU += a[i];\n","\tstopCPU = seconds() - start;\n","\tprintf(\"    Elapsed time: %f (sec) \\n\", stopCPU);\n","\tprintf(\"    sum: %lu\\n\",sum_CPU);\n","\n","\tprintf(\"\\n  GPU kernels (mem required %lu bytes)\\n\", nByte);\n","\n","\t/***********************************************************/\n","\t/*         KERNEL blockParReduce1 (divergent)              */\n","\t/***********************************************************/\n","\t// block by block parallel implementation with divergence\n","\tprintf(\"\\n  Launch kernel: blockParReduce1...\\n\");\n","\tstart = seconds();\n","\tblockParReduce1<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaGetLastError());\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\n","\t// memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\n","\t// check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++) sum_GPU += b[i];\n","\tassert(sum_GPU == n);\n","\n","\t// copy and reset vectors on GPU\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMemset(d_b, 0, mByte));\n","\n","\t/***********************************************************/\n","\t/*        KERNEL blockParReduce2  (non divergent)          */\n","\t/***********************************************************/\n","\t// block by block parallel implementation without divergence\n","\tprintf(\"\\n  Launch kernel: blockParReduce2...\\n\");\n","\tstart = seconds();\n","\tblockParReduce2<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\tCHECK(cudaGetLastError());\n","\n","\t// memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\n","\t// check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++) sum_GPU += b[i];\n","\tassert(sum_GPU == n);\n","\n","\t// copy and reset vectors on GPU\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMemset(d_b, 0, mByte));\n","\n","\t/***********************************************************/\n","\t/*           KERNEL blockParReduce3 (with smem)            */\n","\t/***********************************************************/\n","\t// block by block parallel implementation using warp reduction\n","\tprintf(\"\\n  Launch kernel: blockParReduce3...\\n\");\n","\n","\t// TODO\n","\n","\t// copy and reset vectors on GPU\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMemset(d_b, 0, mByte));\n","\n","\t/***********************************************************/\n","\t/*        KERNEL blockParReduce4  (warp reducton)          */\n","\t/***********************************************************/\n","\t// block by block parallel implementation with smem\n","\tprintf(\"\\n  Launch kernel: blockParReduce4...\\n\");\n","\n","\t// TODO\n","\n","\tcudaFree(d_a);\n","\treturn 0;\n","}\n"],"metadata":{"id":"iGfYbihmMlpe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"660NRr8hMr79"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez4/pred.cu -o pred\n","!./pred"],"metadata":{"id":"-bRRcFxmMr7-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vXUIQkZLCTcG"},"source":["# ✅ Moltiplicazione matriciale con SMEM\n"]},{"cell_type":"markdown","source":["↘️ **TODO...**"],"metadata":{"id":"LO8Q2CxFTyw9"}},{"cell_type":"markdown","source":["\n","Scrivere un programma CUDA per prodotto matrici $C = A*B$ che usi la SMEM e riduca così il 'traffico' in global mem\n","\n","**passi:**\n","1. Definire la SMEM per ogni blocco della matrice $C$\n","2. Svolgere un ciclo sui blocchi per caricare la SMEM da global mem\n","3. Sincronizzare -1-\n","4. Nel ciclo effettuare localmente all’interno di ogni blocco il calcolo del prodotto riga-colonna e caricare su registro\n","5. sincronizzare -2-\n","6. Scrivere il risultato finale su matrice prodotto in global mem\n"],"metadata":{"id":"Kdv736GrlSf1"}},{"cell_type":"markdown","source":["↘️ **TODO...**"],"metadata":{"id":"mqvAdVX8Nkr8"}},{"cell_type":"code","metadata":{"id":"-Y52R0d3CA50"},"source":["%%cuda_group_save --name \"matmul.cu\" --group \"lez4\"\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include \"../../GPUcomputing/utils/common.h\"\n","\n","#define IDX(i,j,n) (i*n+j)\n","#define ABS(x,y) (x-y>=0?x-y:y-x)\n","#define N 2048\n","#define P 2048\n","#define M 1024\n","#define BLOCK_SIZE 16\n","\n","\n","/*\n"," * Kernel for matrix product with static SMEM\n"," *      C  =  A  *  B\n"," *    (NxM) (MxP) (PxM)\n"," */\n","__global__ void matmulSMEMstatic(float* A, float* B, float* C) {\n","\n","\t// static shared memory\n","\n","\t//\tloop over blocks from block row of matrix A and block column of matrix B\n","\n","\t// copy block from matrix to shared memory\n","\n","\t//  BARRIER SYNC on SMEM loading\n","\n","\t// length of this part of row-column product is BLOCK_SIZE except for last block when it may be smaller\n","\n","\t// compute this part of row-column product\n","\n","\t//  BARRIER SYNC on prod over blocks\n","\n","\t// store computed element in matrix C\n","\n","}\n","\n","/*\n"," * Kernel for matrix product using dynamic SMEM\n"," */\n","__global__ void matmulSMEMdynamic(float* A, float* B, float* C, const uint SMEMsize) {\n","\n","\t// TODO\n","\n","}\n","\n","// functions definition\n","__global__ void matmul_naive(float*, float*, float*);\n","void matmulCPU(float*, float*, float*);\n","void checkResult(float*, float*);\n","\n","\n","/*\n"," * MAIN\n"," */\n","int main(void) {\n","\t // Kernels for matrix product\n","\t //      C  =  A  *  B\n","\t //    (NxM) (NxP) (PxM)\n","\tprintf(\"N = %d, M = %d, K = %d\\n\", N, M, P);\n","\tuint rowA = N, rowB = P;\n","\tuint colA = P, colB = M;\n","\tuint rowC = N, colC = M;\n","\tfloat *A, *B, *C, *C1;\n","\tfloat *dev_A, *dev_B, *dev_C;\n","\n","\t// dims\n","\tsize_t Asize = rowA * colA * sizeof(float);\n","\tsize_t Bsize = rowB * colB * sizeof(float);\n","\tsize_t Csize = rowC * colC * sizeof(float);\n","\n","\t// malloc host memory\n","\tA = (float*) malloc(Asize);\n","\tB = (float*) malloc(Bsize);\n","\tC = (float*) malloc(Csize);\n","\tC1 = (float*) malloc(Csize);\n","\n","\t// fill the matrices A and B\n","\tfor (size_t i = 0; i < N * P; i++) A[i] = 1.0;\n","\tfor (size_t i = 0; i < P * M; i++) B[i] = 1.0;\n","\n","\t// malloc device memory\n","\tCHECK(cudaMalloc(&dev_A, Asize));\n","\tCHECK(cudaMalloc(&dev_B, Bsize));\n","\tCHECK(cudaMalloc(&dev_C, Csize));\n","\tprintf(\"Total amount of allocated memory on GPU %.2f MB\\n\\n\", (float)(Asize + Bsize + Csize)/(1024.0*1024.0));\n","\n","   // copy matrices A and B to the GPU\n","\tCHECK(cudaMemcpy(dev_A, A, Asize, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMemcpy(dev_B, B, Bsize, cudaMemcpyHostToDevice));\n","\n","\t/***********************************************************/\n","\t/*                       CPU matmul                       */\n","\t/***********************************************************/\n","\tprintf(\"\\n  CPU procedure...\\n\");\n","\tdouble start = seconds();\n","\tmatmulCPU(A, B, C);\n","   double stopCPU = seconds() - start;\n","\tprintf(\"    Elapsed time: %f (sec) \\n\", stopCPU);\n","\n","\t/***********************************************************/\n","\t/*                    GPU naive matmul                     */\n","\t/***********************************************************/\n","\t// grid block dims = smem dims = BLOCK_SIZE\n","   printf(\"\\n  Launch kernel: naive matmul...\\n\");\n","\tdim3 block(BLOCK_SIZE, BLOCK_SIZE);\n","\tdim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n","\tstart = seconds();\n","\tmatmul_naive<<<grid, block>>>(dev_A, dev_B, dev_C);\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble stopGPU = seconds() - start;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU, stopCPU / stopGPU);\n","\n","\t// copy the array 'C' back from the GPU to the CPU\n","\tCHECK(cudaMemcpy(C1, dev_C, Csize, cudaMemcpyDeviceToHost));\n","\tcheckResult(C, C1);\n","\tCHECK(cudaMemset(dev_C, 0, Csize));\n","\n","\t/***********************************************************/\n","\t/*              GPU matmulSMEM static SMEM                 */\n","\t/***********************************************************/\n","\t// grid block dims = shared mem dims = BLOCK_SIZE\n","\tprintf(\"\\n  Launch kernel: matmul with static smem...\\n\");\n","\n","\t// TODO\n","\n","\t/***********************************************************/\n","\t/*            GPU matmulSMEMD dynamic SMEM                */\n","\t/***********************************************************/\n","\t// set cache size\n","\tcudaDeviceSetCacheConfig (cudaFuncCachePreferShared);\n","\tprintf(\"\\n  Launch kernel: matmul with dynamic smem...\\n\");\n","\n","\t// TODO\n","\n","\t// free the memory allocated on the GPU\n","\tcudaFree(dev_A);\n","\tcudaFree(dev_B);\n","\tcudaFree(dev_C);\n","\n","\tcudaDeviceReset();\n","\treturn EXIT_SUCCESS;\n","}\n","\n","// Kernel for naive matrix product\n","__global__ void matmul_naive(float* A, float* B, float* C) {\n","\t// indexes\n","\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// each thread computes an entry of the product matrix\n","\tif ((row < N) && (col < M)) {\n","\t\tfloat sum = 0;\n","\t\tfor (int k = 0; k < P; k++)\n","\t\t\tsum += A[IDX(row, k, P)] * B[IDX(k, col, M)];\n","\t\tC[IDX(row, col, M)] = sum;\n","\t}\n","}\n","\n","// matrix product on CPU\n","void matmulCPU(float* A, float* B, float* C) {\n","\tfor (int row = 0; row < N; row++)\n","\t\tfor (int col = 0; col < M; col++) {\n","\t\t\tfloat sum = 0;\n","\t\t\tfor (int k = 0; k < P; k++)\n","\t\t\t\tsum += A[IDX(row, k, P)] * B[IDX(k, col, M)];\n","\t\t\tC[IDX(row, col, M)] = sum;\n","\t\t}\n","}\n","\n","// Elementwise comparison between two mqdb\n","void checkResult(float *A, float *B) {\n","\tdouble epsilon = 1.0E-8;\n","\tbool match = 1;\n","\tfor (int i = 0; i < N*M; i++)\n","\t\tif (ABS(A[i], B[i]) > epsilon) {\n","\t\t\tmatch = 0;\n","\t\t\tprintf(\"   * Arrays do not match!\\n\");\n","\t\t\tbreak;\n","\t\t}\n","\tif (!match)\n","\t\tprintf(\"   Arrays do not match\\n\\n\");\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"XM8TOst9Rg3R"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez4/matmul.cu -o matmul\n","!./matmul"],"metadata":{"id":"4xELc640Rg3S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SOFMQZAkjlLW"},"source":["# ✅ Convoluzione con SMEM"]},{"cell_type":"markdown","source":["## 1D Convolution"],"metadata":{"id":"AT_wFzgkw-5W"}},{"cell_type":"code","metadata":{"id":"v9nRkLgeB10A"},"source":["%%cuda_group_save --name \"conv1D.cu\" --group \"lez4\"\n","\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","#define MASK_RADIUS  500\n","#define MASK_SIZE    2 * MASK_RADIUS + 1\n","#define BLOCK_SIZE   1024\n","#define TILE_SIZE    BLOCK_SIZE + MASK_SIZE - 1\n","\n","\n","__device__ __constant__ float d_mask[MASK_SIZE];\n","\n","// functions definition\n","void initialData(float*, int);\n","void movingAverage(float*, int n);\n","void printData(float*, const int);\n","void convolutionHost(float*, float*, float*, const int);\n","void checkResult(float*, float*, int);\n","\n","/*\n"," * kernel for 1D convolution: it holds only if MASK_RADIUS < BLOCK_SIZE\n"," */\n","__global__ void conv1D(float *result, float *data, int n) {\n","\tunsigned int i = blockDim.x * blockIdx.x + threadIdx.x;\n","\n","\t// shared memory size = BLOCK_SIZE + MASK\n","\t__shared__ float tile[TILE_SIZE];\n","\n","\t// boundary\n","\tint left = blockIdx.x * blockDim.x - MASK_RADIUS;\n","\tint right = (blockIdx.x + 1) * blockDim.x;\n","\n","  // left halo\n","\tif (threadIdx.x < MASK_RADIUS)\n","\t\ttile[threadIdx.x] = left < 0 ? 0 : data[left + threadIdx.x];\n","\n","  // center\n","\ttile[threadIdx.x + MASK_RADIUS] = data[i];\n","\n","  // right halo\n","\tif (threadIdx.x >= blockDim.x - MASK_RADIUS)\n","\t\ttile[threadIdx.x + MASK_SIZE - 1] = right >= n ? 0 : data[right + threadIdx.x - blockDim.x + MASK_RADIUS];\n","\n","\t__syncthreads();\n","\n","\t// convolution: tile * mask\n","\tfloat sum = 0;\n","\tfor (int i = -MASK_RADIUS; i <= MASK_RADIUS; i++)\n","\t\tsum += tile[threadIdx.x + MASK_RADIUS + i] * d_mask[i + MASK_RADIUS];\n","\n","\t// final result\n","\tresult[i] = sum;\n","}\n","\n","/*\n"," * Basic kernel for 1D convolution\n"," */\n","__global__ void conv1D_basic(float *result, float *data, int n) {\n","\n","\tunsigned int i = blockDim.x * blockIdx.x + threadIdx.x;\n","\tfloat sum = 0;\n","\n","\t// convolution of tile size elements\n","  int start_point = i - MASK_RADIUS;\n","\tfor (int j = 0; j < MASK_SIZE; j++) {\n","    if (start_point + j >= 0 && start_point + j < n)\n","      sum += data[start_point + j] * d_mask[j];\n","  }\n","\n","\t// final result\n","\tresult[i] = sum;\n","}\n","\n","\n","/*\n"," * MAIN: convolution 1D host & device\n"," */\n","int main(int argc, char **argv) {\n","\n","\t// set up array size\n","\tint n = 1 << 25;\n","\tint N = MASK_SIZE;\n","\n","\tprintf(\"Array of size = %.1f MB\\n\", n/(1024.0*1024.0));\n","\tprintf(\"Mask size     = %d elements\\n\\n\", N);\n","\n","\t// mem sizes\n","\tsize_t nBytes = n * sizeof(float);\n","\tsize_t nBytes_mask = N * sizeof(float);\n","\n","\t// grid configuration\n","\tdim3 block(BLOCK_SIZE);\n","\tdim3 grid((n + BLOCK_SIZE - 1) / BLOCK_SIZE);\n","\n","\t// allocate host memory\n","\tfloat *h_data = (float *) malloc(nBytes);\n","\tfloat *h_result = (float *) malloc(nBytes);\n","\tfloat *h_result_basic = (float *) malloc(nBytes);\n","\tfloat *result = (float *) malloc(nBytes);\n","\tfloat *h_mask = (float *) malloc(nBytes_mask);\n","\n","\t//  initialize host array\n","\tmovingAverage(h_mask, N);\n","\tinitialData(h_data, n);\n","\n","  /***********************************************************/\n","\t/*               convolution on host                       */\n","\t/***********************************************************/\n","\tdouble start = seconds();\n","\tconvolutionHost(h_data, result, h_mask, n);\n","\tdouble hostElaps = seconds() - start;\n","\n","\t/***********************************************************/\n","\t/*               convolution on device                     */\n","\t/***********************************************************/\n","\t// allocate device memory\n","\tfloat *d_data, *d_result;\n","\tCHECK(cudaMalloc((void**)&d_data, nBytes));\n","\tCHECK(cudaMalloc((void**)&d_result, nBytes));\n","\n","\t// copy data from host to device\n","\tCHECK(cudaMemcpy(d_data, h_data, nBytes, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMemcpyToSymbol(d_mask, h_mask, nBytes_mask));\n","\n","\tstart = seconds();\n","\tconv1D<<<grid, block>>>(d_result, d_data, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble devElaps = seconds() - start;\n","\n","\t// check result\n","\tCHECK(cudaMemcpy(h_result, d_result, nBytes, cudaMemcpyDeviceToHost));\n","\tcheckResult(h_result, result, n);\n","\n","\t/***********************************************************/\n","\t/*            convolution on device basic                  */\n","\t/***********************************************************/\n","\tstart = seconds();\n","\tconv1D_basic<<<grid, block>>>(d_result, d_data, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble devElaps1 = seconds() - start;\n","\n","\t// check result\n","\tCHECK(cudaMemcpy(h_result_basic, d_result, nBytes, cudaMemcpyDeviceToHost));\n","\tcheckResult(h_result_basic, result, n);\n","\n","\t// print exec times\n","\tprintf(\"Times:\\n\");\n","\tprintf(\"   - CPU elapsed time         = %f\\n\", hostElaps);\n","  printf(\"   - GPU elapsed time (SMEM)  = %f\\n\", devElaps);\n","\tprintf(\"   - GPU elapsed time (basic) = %f\\n\", devElaps1);\n","  printf(\"   - Speed-up (H/SMEM)        = %f\\n\", hostElaps / devElaps);\n","\tprintf(\"   - Speed-up (basic/SMEM)    = %f\\n\", devElaps1 / devElaps);\n","\n","\n","\t// free host and device memory\n","\tCHECK(cudaFree(d_result));\n","\tCHECK(cudaFree(d_data));\n","\n","\treturn EXIT_SUCCESS;\n","}\n","\n","void initialData(float *h_data, int n) {\n","\t// initialize the data\n","\tfor (int i = 0; i < n; i++)\n","\t\th_data[i] = 1.0;\n","}\n","\n","void movingAverage(float *h_mask, int n) {\n","\t// initialize mask moving average\n","\tfor (int i = 0; i < n; i++)\n","\t\th_mask[i] = 1.0 / ((float) n);\n","\treturn;\n","}\n","\n","void printData(float *a, const int size) {\n","\tprintf(\"\\n\");\n","\tfor (int i = 0; i < size; i++)\n","\t\tprintf(\"%.2f \", a[i]);\n","\tprintf(\"\\n\");\n","\treturn;\n","}\n","\n","void convolutionHost(float *data, float *result, float *mask, const int n) {\n","\tfor (int i = 0; i < n; i++) {\n","\t\tfloat sum = 0;\n","\t\tfor (int j = 0; j < MASK_SIZE; j++) {\n","\t\t\tint idx = i - MASK_RADIUS + j;\n","\t\t\tif (idx >= 0 && idx < n)\n","\t\t\t\tsum += data[idx] * mask[j];\n","\t\t}\n","\t\tresult[i] = sum;\n","\t}\n","}\n","\n","void checkResult(float *d_result, float *h_result, int n) {\n","\tdouble epsilon = 1.0E-8;\n","\n","\tfor (int i = 0; i < n; i++)\n","\t\tif (abs(h_result[i] - d_result[i]) > epsilon) {\n","\t\t\tprintf(\"different on entry (%d) |h_result - d_result| >  %f\\n\", i, epsilon);\n","\t\t\tbreak;\n","\t\t}\n","}\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"dt-JkDtuRzFj"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez4/conv1D.cu -o conv1D\n","!./conv1D"],"metadata":{"id":"za5qppjCRzFj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2D Convolution..."],"metadata":{"id":"xYysz72z216s"}},{"cell_type":"markdown","source":["↘️ **TODO...**"],"metadata":{"id":"xYC96elpSQpf"}},{"cell_type":"code","source":["%%cuda_group_save --name \"conv2D.cu\" --group \"lez4\"\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","#define BLOCK_SIZE   16\n","#define MASK_SIZE    21\n","#define TILE_SIZE    (BLOCK_SIZE + MASK_SIZE - 1)\n","\n","typedef struct {\n","   int width;\n","   int height;\n","   float* elements;\n"," } Matrix;\n","\n","// Function declarations\n"," void conv2D_host(Matrix A, Matrix B, Matrix M);\n","__global__ void conv2D_basic(Matrix A, Matrix B, Matrix M);\n","\n"," /*\n","  * 2D convolution using shared memory\n","  *   A: input matrix\n","  *   B: output matrix\n","  *   M: convolution mask matrix\n"," */\n","__global__ void conv2D(Matrix A, Matrix B, Matrix M) {\n","\n","   // Allocate shared memory\n","\n","   // Load data into shared memory\n","\n","   // Synchronize threads\n","\n","   // Apply convolution\n","\n","   // Write output\n","\n","}\n","\n","/*\n"," * Main function\n"," */\n","int main(void) {\n","   // define matrices and params\n","   int block_size = BLOCK_SIZE;\n","   int mask_size = MASK_SIZE;\n","   int width = 256* block_size, height = 256* block_size;\n","   Matrix A, B, H, M;\n","   A.width = width; A.height = height;\n","   B.width = width; B.height = height;\n","   M.width = mask_size; M.height = mask_size;\n","   H.width = width; H.height = height;\n","   A.elements = (float *)malloc(width * height * sizeof(float));\n","   B.elements = (float *)malloc(width * height * sizeof(float));\n","   M.elements = (float *)malloc(mask_size * mask_size * sizeof(float));\n","   H.elements = (float *)malloc(width * height * sizeof(float));\n","\n","   // Initialize A, B, M\n","   // print data sizes\n","   printf(\"Data matrix A: %d x %d\\n\", width, height);\n","   printf(\"Mask matrix M: %d x %d\\n\", mask_size, mask_size);\n","   for (int i = 0; i < width * height; i++) {\n","      A.elements[i] = 1.0f;\n","      B.elements[i] = 0.0f;\n","   }\n","   for (int i = 0; i < mask_size * mask_size; i++) {\n","      M.elements[i] = 1.0f;\n","   }\n","\n","   // Allocate device memory\n","   Matrix d_A, d_B, d_M;\n","   d_A.width = A.width; d_A.height = A.height;\n","   d_B.width = B.width; d_B.height = B.height;\n","   d_M.width = M.width; d_M.height = M.height;\n","   CHECK(cudaMalloc(&d_A.elements, width * height * sizeof(float)));\n","   CHECK(cudaMalloc(&d_B.elements, width * height * sizeof(float)));\n","   CHECK(cudaMalloc(&d_M.elements, mask_size * mask_size * sizeof(float)));\n","\n","   // Copy data to device\n","   CHECK(cudaMemcpy(d_A.elements, A.elements, width * height * sizeof(float), cudaMemcpyHostToDevice));\n","   CHECK(cudaMemcpy(d_M.elements, M.elements, mask_size * mask_size * sizeof(float), cudaMemcpyHostToDevice));\n","\n","   /***********************************************************/\n","\t/*                    conv2D on host                       */\n","\t/***********************************************************/\n","   printf(\"\\nCPU procedure...\\n\");\n","\tdouble start = seconds();\n","\tconv2D_host(A, H, M);\n","\tdouble stopCPU = seconds() - start;\n","   printf(\"   Host elapsed time: %f\\n\", stopCPU);\n","\n","   /***********************************************************/\n","\t/*                    GPU naive conv2D                     */\n","\t/***********************************************************/\n","   printf(\"\\nGPU naive conv2D...\\n\");\n","   dim3 dimBlock(block_size, block_size);\n","   dim3 dimGrid((width + block_size - 1) / block_size, (height + block_size - 1) / block_size);\n","   start = seconds();\n","   conv2D_basic<<<dimGrid, dimBlock>>>(d_A, d_B, d_M);\n","   CHECK(cudaDeviceSynchronize());\n","   double stopGPU = seconds() - start;\n","   printf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU, stopCPU / stopGPU);\n","\n","   // Copy data back to host\n","   CHECK(cudaMemcpy(B.elements, d_B.elements, width * height * sizeof(float), cudaMemcpyDeviceToHost));\n","\n","   // check results\n","   for (int i = 0; i < width; i++) {\n","      for (int j = 0; j < height; j++) {\n","         if (B.elements[j * width + i] != H.elements[j * width + i]) {\n","            printf(\"Error at B[%d][%d] = %f\\n\", i, j, B.elements[j * width + i]);\n","         }\n","      }\n","   }\n","\n","   // zero out B in device\n","   CHECK(cudaMemset(d_B.elements, 0, width * height * sizeof(float)));\n","\n","   /***********************************************************/\n","\t/*                  GPU conv2D wih smem                    */\n","\t/***********************************************************/\n","   printf(\"\\nGPU conv2D with smem...\\n\");\n","\n","   // TODO\n","\n","   return 0;\n","}\n","\n","/*\n"," * 2D convolution on host\n"," */\n","void conv2D_host(Matrix A, Matrix B, Matrix M) {\n","\n","   int radius = MASK_SIZE / 2;\n","\n","   // loop through all elements in the output array\n","   for (int y = 0; y < A.height; y++) {\n","\t   for (int x = 0; x < A.width; x++) {\n","\t\t\tfloat sum = 0.0f;\n","\n","\t\t\t// compute convolution\n","\t\t\tfor (int i = 0; i < MASK_SIZE; i++) {\n","            for (int j = 0; j < MASK_SIZE; j++) {\n","               int r = y - radius + i;\n","\t\t\t\t\tint c = x - radius + j;\n","\n","\t\t\t\t\t//boundary check\n","\t\t\t\t\tif ((c >= 0) && (c < A.width) && (r >= 0) && (r < A.height)) {\n","\t\t\t\t\t\tsum += A.elements[(r * A.width) + c] * M.elements[(j * MASK_SIZE) + i];\n","\t\t\t\t\t}\n","\t\t\t\t}\n","         }\n","\n","         //store final value\n","         B.elements[y * B.width + x] = sum;\n","\t   }\n","   }\n","}\n","\n","/*\n"," * Basic kernel for 2D convolution\n"," */\n"," __global__ void conv2D_basic(Matrix A, Matrix B, Matrix M) {\n","\n","\t//index computation\n","\tint x = blockIdx.x * blockDim.x + threadIdx.x;\n","\tint y = blockIdx.y * blockDim.y + threadIdx.y;\n","   int radius = MASK_SIZE / 2;\n","\n","   //boundary check\n","   if (x >= A.width && y >= A.height)  return;\n","\n","   // Apply convolution\n","   float sum = 0.0f;\n","   for (int i = 0; i < MASK_SIZE; i++) {\n","      for (int j = 0; j < MASK_SIZE; j++) {\n","         int r = y - radius + i;\n","         int c = x - radius + j;\n","         if (r >= 0 && r < A.height && c >= 0 && c < A.width) {\n","            sum += A.elements[r * A.width + c] * M.elements[i * MASK_SIZE + j];\n","         }\n","      }\n","   }\n","\n","   //store final value\n","   B.elements[y * B.width + x] = sum;\n","}"],"metadata":{"id":"W9PZa3zQSan2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"YU3qBT5ISan3"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez4/conv2D.cu -o conv2D\n","!./conv2D"],"metadata":{"id":"EGMvzrjHSan4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PPM Gaussian filter"],"metadata":{"id":"aHQHlZdo0wro"}},{"cell_type":"markdown","source":["Utilizzare la convoluzione `conv2D` per il filtraggio gaussiano su immagini PMM\n","- Caricare immagine PPM (`ppm_load(path)`)\n","- estrarre i canali RGB (`m_extract_channel(img, c)`)\n","- definire mask gaussiana (`gaussMask(MASK_SIZE, SIGMA)`)\n","- applicare il filtraggio all'immagine con mask gaussiana (separatamente su ogni canale)\n","- ricostruire l'immagine filtrata dai singoli canali filtrati (`ppm_combine_channels(r, g, b, WIDTH, HEIGHT)`)"],"metadata":{"id":"L2N3V-FnVa4G"}},{"cell_type":"markdown","source":["↘️ **TODO...**"],"metadata":{"id":"xM-Ai4OWxJLa"}},{"cell_type":"code","source":["%%cuda_group_save --name \"ppm_conv2D.cu\" --group \"lez4\"\n","#include \"/content/GPUcomputing/utils/common.h\"\n","#include \"ppm.h\"\n","\n","#define BLOCK_SIZE   32\n","#define MASK_SIZE    21\n","#define TILE_SIZE    (BLOCK_SIZE + MASK_SIZE - 1)\n","\n","typedef struct {\n","   int width;\n","   int height;\n","   float* elements;\n"," } Matrix;\n","\n"," /*\n","  * 2D convolution using shared memory\n","  *   A: input matrix\n","  *   B: output matrix\n","  *   M: convolution mask matrix\n"," */\n","__global__ void conv2D(Matrix A, Matrix B, Matrix M) {\n","\n","   int x = blockIdx.x * blockDim.x + threadIdx.x; // Column index of matrix A\n","   int y = blockIdx.y * blockDim.y + threadIdx.y; // Row index of matrix A\n","\n","   int tile_size = BLOCK_SIZE + MASK_SIZE - 1;\n","   int radius = MASK_SIZE / 2;\n","\n","   // Allocate shared memory\n","   __shared__ float smem[TILE_SIZE][TILE_SIZE];\n","\n","   // Load data into shared memory\n","   for (int row = 0; row <= tile_size/blockDim.y; row++) {\n","      for (int col = 0; col <= tile_size/blockDim.x; col++) {\n","         int row_data = y + blockDim.y * row - radius;   // input data index row\n","         int col_data = x + blockDim.x * col - radius;   // input data index column\n","         int row_smem = threadIdx.y + blockDim.y * row;  // mask index row\n","         int col_smem = threadIdx.x + blockDim.x * col;  // mask index column\n","\n","         // Check valid range for smem and data\n","         if (row_smem < tile_size && col_smem < tile_size) {\n","            if (row_data >= 0 && row_data < A.height && col_data >= 0 && col_data < A.width) {\n","               smem[row_smem][col_smem] = A.elements[row_data * A.width + col_data];\n","            } else {\n","               smem[row_smem][col_smem] = 0.0f;\n","            }\n","         }\n","      }\n","   }\n","\n","   // Synchronize threads\n","   __syncthreads();\n","\n","   // Apply convolution\n","   float sum = 0.0f;\n","   for (int i = 0; i < MASK_SIZE; i++) {\n","      for (int j = 0; j < MASK_SIZE; j++) {\n","         int r = threadIdx.y + i;\n","         int c = threadIdx.x + j;\n","         if (r >= 0 && r < tile_size && c >= 0 && c < tile_size) {\n","            sum += smem[r][c] * M.elements[i * MASK_SIZE + j];\n","         }\n","      }\n","   }\n","\n","   // Write output\n","   if (y < A.height && x < A.width) {\n","      B.elements[y * B.width + x] = sum;\n","   }\n","}\n","\n","/*\n"," * Main function\n"," */\n","int main(void) {\n","   // Load image\n","   char path[] = \"GPUcomputing/images/dog.ppm\";\n","   PPM *img = ppm_load(path);\n","   int WIDTH = img->width;\n","   int HEIGHT = img->height;\n","   printf(\"PPM image size (w x h): %d x %d\\n\", WIDTH, HEIGHT);\n","\n","   // extract channels and set matrices\n","\n","   // get gaussian filter mask\n","\n","   // Allocate device memory\n","\n","   // Copy data to device\n","\n","\n","   /***********************************************************/\n","\t/*                    conv2D on host                       */\n","\t/***********************************************************/\n","   printf(\"\\nCPU procedure...\\n\");\n","\t double start = seconds();\n","   PPM *img_filtered = ppm_make(WIDTH, HEIGHT, (pel) {0,0,0}); // create a new image\n","   ppm_gaussFilter(img, img_filtered, MASK_SIZE, SIGMA);\n","   ppm_write(img_filtered, \"output_gaussian.ppm\");\n","\t double stopCPU = seconds() - start;\n","   printf(\"   Host elapsed time: %f\\n\", stopCPU);\n","\n","   /***********************************************************/\n","\t/*                  GPU conv2D wih smem                    */\n","\t/***********************************************************/\n","   printf(\"\\nGPU conv2D with smem...\\n\");\n","\n","   // TODO\n","\n","   // check results\n","\n","   return 0;\n","}\n"],"metadata":{"id":"fz53AMgnVcS9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"nZiCbPcwYZmx"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez4/ppm_conv2D.cu -o conv2D -I GPUcomputing/utils/PPM GPUcomputing/utils/PPM/ppm.cpp\n","!./conv2D"],"metadata":{"id":"l9wh_zvfYZmy"},"execution_count":null,"outputs":[]}]}