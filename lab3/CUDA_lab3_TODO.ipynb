{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1B-_MmiljBUZFT3f0iOEK314RqqdWHc7P","timestamp":1710972597035}],"collapsed_sections":["F9PmBZql0ow4","5gUDpbz5TZml","_NlWW_V5T1GT","ygwWcMU9DJmG","SOFMQZAkjlLW"],"mount_file_id":"1PD8axMJPIW19SKS0LZUlg-Ps3b7OlikM","authorship_tag":"ABX9TyPpqcWk5ic3roQufs8rIezm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["---\n","# **LAB 3 - Modello di esecuzione CUDA**\n","---"],"metadata":{"id":"fZYqN0UwVLC_"}},{"cell_type":"markdown","metadata":{"id":"F9PmBZql0ow4"},"source":["# ▶️ CUDA setup"]},{"cell_type":"code","metadata":{"id":"p9RIwaPbVQHV"},"source":["!nvcc --version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n5YlC1IOTlNb"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GPU computing notebooks download (from github)"],"metadata":{"id":"gcg1GyK5srek"}},{"cell_type":"code","source":["!git clone https://github.com/giulianogrossi/GPUcomputing.git"],"metadata":{"id":"tyHOxci3s3H8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"227eLdP5csN1"},"source":["NVCC Plugin for Jupyter notebook"]},{"cell_type":"code","source":["%cd GPUcomputing/utils/nvcc4jupyter-master/\n","!python3 setup.py install\n","%load_ext nvcc4jupyter\n","%cd /content/"],"metadata":{"id":"4TzxMBFds8aT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5gUDpbz5TZml"},"source":["# ✅ Divergence analysis"]},{"cell_type":"code","metadata":{"id":"RlbVvBaXCHBs"},"source":["%%cuda_group_save --name \"div.cu\" --group \"DIV\"\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <assert.h>\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","/*\n"," * Kernel with warp divergence\n"," */\n","__global__ void evenOddDIV(int *c, const ulong N) {\n","\tulong tid = blockIdx.x * blockDim.x + threadIdx.x;\n","\tint a, b;\n","\n","\tif (!(tid % 2))   // branch divergence\n","\t\ta = 2;\n","\telse\n","\t\tb = 1;\n","\n","\t// check index\n","\tif (tid < N)\n","\t\tc[tid] = a + b;\n","}\n","\n","/*\n"," * MAIN\n"," */\n","int main(int argc, char **argv) {\n","\n","\t// set up data size\n","\tint blocksize = 1024;\n","\tulong size = 1024*1024;\n","\n","\tif (argc > 1)\n","\t\tblocksize = atoi(argv[1]);\n","\tif (argc > 2)\n","\t\tsize = atoi(argv[2]);\n","\tulong nBytes = size * sizeof(int);\n","\n","\tprintf(\"Data size: %lu  -- \", size);\n","  printf(\"Data size (bytes): %lu MB\\n\", nBytes/1000000);\n","\n","\t// set up execution configuration\n","\tdim3 block(blocksize, 1);\n","\tdim3 grid((size + block.x - 1) / block.x, 1);\n","\tprintf(\"Execution conf (block %d, grid %d)\\nKernels:\\n\", block.x, grid.x);\n","\n","\t// allocate memory\n","\tint *d_C, *C;\n","\tC = (int *) malloc(nBytes);\n","\tCHECK(cudaMalloc((void** )&d_C, nBytes));\n","\n","\t// run kernel 1\n","\tdouble iStart, iElaps;\n","\tiStart = seconds();\n","\tevenOddDIV<<<grid, block>>>(d_C, size);\n","\tCHECK(cudaDeviceSynchronize());\n","\tiElaps = seconds() - iStart;\n","\tprintf(\"\\tevenOddDIV<<<%d, %d>>> elapsed time %f sec \\n\\n\", grid.x, block.x, iElaps);\n","\tCHECK(cudaGetLastError());\n","\n","  CHECK(cudaMemcpy(C, d_C, nBytes, cudaMemcpyDeviceToHost));\n","\n","\tfree(C);\n","\t// free gpu memory and reset device\n","\tCHECK(cudaFree(d_C));\n","\tCHECK(cudaDeviceReset());\n","\treturn EXIT_SUCCESS;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kMEGfjJMcX_e"},"source":["# Compilazione ed esecuzione\n","!nvcc -arch=sm_75 src/DIV/div.cu -o div\n","!./div 1024 20000000"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ncu  ./div"],"metadata":{"id":"foUbQ9g49kxg"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PdQqnuH-54Ie"},"source":["# Compilazione ed esecuzione versione di debug\n","!nvcc -arch=sm_75 -g -G src/DIV/div.cu -o div_deb\n","!./div_deb 1024 2000000000"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ncu ./div_deb"],"metadata":{"id":"wk_7bbil9EZQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ↘️ *`TODO...`*"],"metadata":{"id":"_NlWW_V5T1GT"}},{"cell_type":"markdown","source":["Introdurre nuovo kernel che eviti la divergenza a livello di warp.\n","\n","- usare/creare nuova indicizzazione a livello di warp\n","- applicare nuova indicizzazione preservando il risultato finale\n"],"metadata":{"id":"Q4kYsvVbT1GU"}},{"cell_type":"code","source":["%%cuda_group_save --name \"div.cu\" --group \"DIV\"\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <assert.h>\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","/*\n"," * Kernel with warp divergence\n"," */\n","__global__ void evenOddDIV(int *c, const ulong N) {\n","\tulong tid = blockIdx.x * blockDim.x + threadIdx.x;\n","\tint a, b;\n","\n","\tif (!(tid % 2))   // branch divergence\n","\t\ta = 2;\n","\telse\n","\t\tb = 1;\n","\n","\t// check index\n","\tif (tid < N)\n","\t\tc[tid] = a + b;\n","}\n","\n","/*\n"," * Kernel without warp divergence\n"," */\n","__global__ void evenOddNODIV(int *c, const int N) {\n","\n","\t// TODO\n","\n","  // warp index wid = 0,1,2,3,...\n","\n","\t// using wid index for even and odd\n","\n","\t// check index\n","\n","}\n","\n","/*\n"," * MAIN\n"," */\n","int main(int argc, char **argv) {\n","\n","\t// set up data size\n","\tint blocksize = 1024;\n","\tulong size = 1024*1024;\n","\n","\tif (argc > 1)\n","\t\tblocksize = atoi(argv[1]);\n","\tif (argc > 2)\n","\t\tsize = atoi(argv[2]);\n","\tulong nBytes = size * sizeof(int);\n","\n","\tprintf(\"Data size: %lu  -- \", size);\n","  printf(\"Data size (bytes): %lu MB\\n\", nBytes/1000000);\n","\n","\t// set up execution configuration\n","\tdim3 block(blocksize, 1);\n","\tdim3 grid((size + block.x - 1) / block.x, 1);\n","\tprintf(\"Execution conf (block %d, grid %d)\\nKernels:\\n\", block.x, grid.x);\n","\n","\t// allocate memory\n","\tint *d_C, *C;\n","\tC = (int *) malloc(nBytes);\n","\tCHECK(cudaMalloc((void** )&d_C, nBytes));\n","\n","\t// run kernel 1\n","\tdouble iStart, iElaps;\n","\tiStart = seconds();\n","\tevenOddDIV<<<grid, block>>>(d_C, size);\n","\tCHECK(cudaDeviceSynchronize());\n","\tiElaps = seconds() - iStart;\n","\tprintf(\"\\tevenOddDIV<<<%d, %d>>> elapsed time %f sec \\n\\n\", grid.x, block.x, iElaps);\n","\tCHECK(cudaGetLastError());\n","\n","  CHECK(cudaMemcpy(C, d_C, nBytes, cudaMemcpyDeviceToHost));\n","\n","\t// run kernel 2\n","  CHECK(cudaMemset(d_C, 0.0, nBytes)); // reset memory\n","\tiStart = seconds();\n","\tevenOddNODIV<<<grid, block>>>(d_C, size);\n","\tiElaps = seconds() - iStart;\n","\tprintf(\"\\tevenOddNODIV<<<%d, %d>>> elapsed time %f sec \\n\\n\", grid.x, block.x, iElaps);\n","\tCHECK(cudaGetLastError());\n","\n","\tCHECK(cudaMemcpy(C, d_C, nBytes, cudaMemcpyDeviceToHost));\n","\n","\tfree(C);\n","\t// free gpu memory and reset device\n","\tCHECK(cudaFree(d_C));\n","\tCHECK(cudaDeviceReset());\n","\treturn EXIT_SUCCESS;\n","}\n"],"metadata":{"id":"NEQ9LoktUA3J"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZflE_OqGU75Q"},"source":["# Compilazione ed esecuzione\n","!nvcc -arch=sm_75 src/DIV/div.cu -o div\n","!./div 1024 20000000"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ygwWcMU9DJmG"},"source":["# ✅ Parallel Reduction"]},{"cell_type":"code","metadata":{"id":"WThhkz6GDMsm"},"source":["%%cuda_group_save --name \"preduce.cu\" --group \"PAR\"\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <assert.h>\n","\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","\n","/*\n"," *  Block by block parallel implementation with divergence (sequential schema)\n"," */\n","__global__ void blockParReduce1(int *in, int *out, ulong n) {\n","\n","\tuint tid = threadIdx.x;\n","\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// boundary check\n","\tif (idx >= n)\n","\t\treturn;\n","\n","\t// convert global data pointer to the local pointer of this block\n","\tint *thisBlock = in + blockIdx.x * blockDim.x;\n","\n","\t// in-place reduction in global memory\n","\tfor (int stride = 1; stride < blockDim.x; stride *= 2) {\n","\t\tif ((tid % (2 * stride)) == 0)\n","\t\t\tthisBlock[tid] += thisBlock[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = thisBlock[0];\n","}\n","\n","\n","\n","/*\n"," * MAIN: test on parallel reduction\n"," */\n","int main(void) {\n","\tint *a, *b, *d_a, *d_b;\n","\tint blockSize = 1024;            // block dim 1D\n","\tulong numBlock = 1024*1024;      // grid dim 1D\n","\tulong n = blockSize * numBlock;  // array dim\n","\tlong sum_CPU = 0, sum_GPU;\n","\tlong nByte = n*sizeof(int), mByte = numBlock * sizeof(int);\n","\tdouble start, stopGPU, stopCPU, speedup;\n","\n","\tprintf(\"\\n****  test on parallel reduction  ****\\n\");\n","\n","\t// init\n","\ta = (int *) malloc(nByte);\n","\tb = (int *) malloc(mByte);\n","\tfor (ulong i = 0; i < n; i++) a[i] = 1;\n","\n","\tCHECK(cudaMalloc((void **) &d_a, nByte));\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMalloc((void **) &d_b, mByte));\n","\tCHECK(cudaMemset((void *) d_b, 0, mByte));\n","\n","\t/***********************************************************/\n","\t/*                     CPU reduction                       */\n","\t/***********************************************************/\n","\tprintf(\"  Vector length: %.2f MB\\n\",n/(1024.0*1024.0));\n","\tprintf(\"\\n  CPU procedure...\\n\");\n","\tstart = seconds();\n","\tfor (ulong i = 0; i < n; i++)\n","    sum_CPU += a[i];\n","\tstopCPU = seconds() - start;\n","\tprintf(\"    Elapsed time: %f (sec) \\n\", stopCPU);\n","\tprintf(\"    sum: %lu\\n\",sum_CPU);\n","\n","\tprintf(\"\\n  GPU kernels (mem required %lu bytes)\\n\", nByte);\n","\n","\t/***********************************************************/\n","\t/*         KERNEL blockParReduce1 (divergent)              */\n","\t/***********************************************************/\n","\t// block by block parallel implementation with divergence\n","\tprintf(\"\\n  Launch kernel: blockParReduce1...\\n\");\n","\tstart = seconds();\n","\tblockParReduce1<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaGetLastError());\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\n","  // memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\n","  // check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++)\n","\t\tsum_GPU += b[i];\n","\tassert(sum_GPU == n);\n","\n","\t// reset input vector on GPU\n","\tfor (ulong i = 0; i < n; i++) a[i]=1;\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\n","\n","\tcudaFree(d_a);\n","\n","\tCHECK(cudaDeviceReset());\n","\treturn 0;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pIOextPZMiav"},"source":["#Compilazione ed esecuzione\n","\n","!nvcc -arch=sm_75 src/PAR/preduce.cu -o preduce\n","!./preduce"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ↘️ *`TODO...`*"],"metadata":{"id":"VX7H_0JiBx2j"}},{"cell_type":"markdown","source":["Kernel privo di divergenza:\n","\n","* Usare lo schema che suddivide in blocchi (richiesta sincronizzazione)\n","* Sommare su ogni blocco con parallel reduction (somma parziale)\n","* Utilizzare uno schema interlacciato\n","* Evitare la divergenza nella parallel reduction\n","* Unire le somme parziali dei blocchi\n","\n","\n"],"metadata":{"id":"rMXU-CfpCCep"}},{"cell_type":"code","metadata":{"id":"B5QUNJdYauND"},"source":["%%cuda_group_save --name \"preduce.cu\" --group \"PAR\"\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <assert.h>\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","\n","/*\n"," *  Block by block parallel implementation with divergence (sequential schema)\n"," */\n","__global__ void blockParReduce1(int *in, int *out, ulong n) {\n","\n","\tuint tid = threadIdx.x;\n","\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// boundary check\n","\tif (idx >= n)\n","\t\treturn;\n","\n","\t// convert global data pointer to the local pointer of this block\n","\tint *thisBlock = in + blockIdx.x * blockDim.x;\n","\n","\t// in-place reduction in global memory\n","\tfor (int stride = 1; stride < blockDim.x; stride *= 2) {\n","\t\tif ((tid % (2 * stride)) == 0)\n","\t\t\tthisBlock[tid] += thisBlock[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = thisBlock[0];\n","}\n","\n","/*\n"," *  Block by block parallel implementation without divergence (interleaved schema)\n"," */\n","__global__ void blockParReduce2(int *in, int *out, ulong n) {\n","\n","\t// TODO\n","\n","\t// boundary check\n","\n","\t// convert global data pointer to the local pointer of this block\n","\n","\t// in-place reduction in global memory\n","\n","\t// write result for this block to global mem\n","\n","}\n","\n","\n","/*\n"," * MAIN: test on parallel reduction\n"," */\n","int main(void) {\n","\tint *a, *b, *d_a, *d_b;\n","\tint blockSize = 1024;            // block dim 1D\n","\tulong numBlock = 1024*1024;      // grid dim 1D\n","\tulong n = blockSize * numBlock;  // array dim\n","\tlong sum_CPU = 0, sum_GPU;\n","\tlong nByte = n*sizeof(int), mByte = numBlock * sizeof(int);\n","\tdouble start, stopGPU, stopCPU, speedup;\n","\n","\tprintf(\"\\n****  test on parallel reduction  ****\\n\");\n","\n","\t// init\n","\ta = (int *) malloc(nByte);\n","\tb = (int *) malloc(mByte);\n","\tfor (ulong i = 0; i < n; i++) a[i] = 1;\n","\n","\tCHECK(cudaMalloc((void **) &d_a, nByte));\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMalloc((void **) &d_b, mByte));\n","\tCHECK(cudaMemset((void *) d_b, 0, mByte));\n","\n","\t/***********************************************************/\n","\t/*                     CPU reduction                       */\n","\t/***********************************************************/\n","\tprintf(\"  Vector length: %.2f MB\\n\",n/(1024.0*1024.0));\n","\tprintf(\"\\n  CPU procedure...\\n\");\n","\tstart = seconds();\n","\tfor (ulong i = 0; i < n; i++)\n","    sum_CPU += a[i];\n","\tstopCPU = seconds() - start;\n","\tprintf(\"    Elapsed time: %f (sec) \\n\", stopCPU);\n","\tprintf(\"    sum: %lu\\n\",sum_CPU);\n","\n","\tprintf(\"\\n  GPU kernels (mem required %lu bytes)\\n\", nByte);\n","\n","\t/***********************************************************/\n","\t/*         KERNEL blockParReduce1 (divergent)              */\n","\t/***********************************************************/\n","\t// block by block parallel implementation with divergence\n","\tprintf(\"\\n  Launch kernel: blockParReduce1...\\n\");\n","\tstart = seconds();\n","\tblockParReduce1<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaGetLastError());\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\n","  // memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\n","  // check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++)\n","\t\tsum_GPU += b[i];\n","\tassert(sum_GPU == n);\n","\n","\t// reset input vector on GPU\n","\tfor (ulong i = 0; i < n; i++) a[i]=1;\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\n","\t/***********************************************************/\n","\t/*        KERNEL blockParReduce2  (non divergent)          */\n","\t/***********************************************************/\n","\t// block by block parallel implementation without divergence\n","\tprintf(\"\\n  Launch kernel: blockParReduce2...\\n\");\n","\tstart = seconds();\n","\tblockParReduce2<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\tCHECK(cudaGetLastError());\n","\n","  // memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\n","  // check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++) {\n","\t\tsum_GPU += b[i];\n","  //\t\tprintf(\"b[%d] = %d\\n\",i,b[i]);\n","\t}\n","\tassert(sum_GPU == n);\n","\n","  // reset input vector on GPU\n","\tfor (ulong i = 0; i < n; i++) a[i] = 1;\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\n","\t// check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++)\n","\t\tsum_GPU += b[i];\n","\tassert(sum_GPU == n);\n","\n","\tcudaFree(d_a);\n","\n","\tCHECK(cudaDeviceReset());\n","\treturn 0;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Is7vbYHauNM"},"source":["#Compilazione ed esecuzione\n","\n","!nvcc -arch=sm_75 src/PAR/preduce.cu -o preduce\n","!./preduce"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IkYKd9J32ewH"},"source":["# ✅ Istogramma di un'immagine BMP"]},{"cell_type":"markdown","source":["Calcolare l'istogramma di un aimmagine BMP con uso di `atomicAdd`"],"metadata":{"id":"vfkxkSqmCmAD"}},{"cell_type":"markdown","source":["↘️ **TODO...**"],"metadata":{"id":"LO8Q2CxFTyw9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mTml7xNLgVUn"},"outputs":[],"source":["%%cuda_group_save --name \"ppm_hist.cu\" --group \"lez3\"\n","\n","%%cuda_group_save --name \"ppm_hist.cu\" --group \"lez3\"\n","\n","\n","#include <stdio.h>\n","#include <time.h>\n","#include <limits.h>\n","\n","#include \"/content/GPUcomputing/utils/common.h\"\n","#include \"ppm.h\"\n","\n","/*\n"," * Kernel 1D that computes histogram on GPU\n"," */\n","__global__ void ppm_histGPU(PPM ppm, int *histogram) {\n","\n","   // TODO\n","\n","}\n","\n","/*\n"," * MAIN\n"," */\n","int main(int argc, char **argv) {\n","\n","   // PPM images\n","   PPM *ppm;   // Where images are stored in CPU\n","   PPM ppm_d;  // Where images are stored in GPU\n","\n","   // load a PPM image from file\n","   char path[] = \"GPUcomputing/images/dog.ppm\";\n","   ppm = ppm_load(path);\n","   uint WIDTH = ppm->width;\n","   uint HEIGHT = ppm->height;\n","   ppm_d.width = WIDTH;\n","   ppm_d.height = HEIGHT;\n","   ppm_d.maxval = ppm->maxval;\n","   printf(\"PPM image size (w x h): %d x %d\\n\", WIDTH, HEIGHT);\n","\n","   // set main params\n","   uint nBins = 3 * 256 * sizeof(int);\n","   int nBytes = WIDTH * HEIGHT * sizeof(int);\n","\n","   // CPU version\n","   double start = seconds();\n","   int *histogram = ppm_histogram(ppm);\n","   double stopCPU = seconds() - start;\n","\n","   // Allocate GPU buffer & copy image from CPU to GPU\n","   CHECK(cudaMalloc(&ppm_d.image, nBytes));\n","   CHECK(cudaMemcpy(ppm_d.image, ppm->image, nBytes, cudaMemcpyHostToDevice));\n","\n","   // allocate memory for the histogram on the device\n","   int *histogram_d;\n","   CHECK(cudaMalloc(&histogram_d, nBins));\n","   CHECK(cudaMemset(histogram_d, 0, nBins));\n","\n","   // invoke kernel\n","   dim3 dimBlock(512);\n","   dim3 dimGrid((WIDTH * HEIGHT + dimBlock.x - 1) / dimBlock.x);\n","   start = seconds();\n","   ppm_histGPU<<<dimGrid, dimBlock>>>(ppm_d, histogram_d);\n","   CHECK(cudaDeviceSynchronize());\n","   double stopGPU = seconds() - start;\n","\n","   // read histogram from device memory\n","   int *histogram1 = (int*)malloc(nBins);  // for GPU copy to CPU\n","   CHECK(cudaMemcpy(histogram1, histogram_d, nBins, cudaMemcpyDeviceToHost));\n","   ppm_save_histogram(histogram1, \"histogramGPU.ppm\");\n","   ppm_save_histogram(histogram, \"histogramCPU.ppm\");\n","\n","   // check results\n","   for (int i = 0; i < 256; i++) {\n","      if (histogram[i] != histogram1[i]) {\n","         printf(\"Error r: hist[%d] = %d != d_hist[%d] = %d\\n\", i, histogram[i], i, histogram1[i]);\n","         break;\n","      }\n","      if (histogram[i+256] != histogram1[i+256]) {\n","         printf(\"Error g: hist[%d] = %d != d_hist[%d] = %d\\n\", i, histogram[i], i, histogram1[i]);\n","         break;\n","      }\n","      if (histogram[i+512] != histogram1[i+512]) {\n","         printf(\"Error b: hist[%d] = %d != d_hist[%d] = %d\\n\", i, histogram[i], i, histogram1[i]);\n","         break;\n","      }\n","   }\n","   printf(\"Test PASSED\\n\");\n","\n","   // times & speedup\n","   printf(\"CPU elapsed time: %.4f (msec) \\n\", stopCPU*1000);\n","   printf(\"CPU elapsed time: %.4f (msec) - Speedup %.1f\\n\", stopGPU*1000, stopCPU/stopGPU);\n","\n","   // free memory\n","   free(histogram);\n","   CHECK(cudaFree(histogram_d));\n","   cudaFree(ppm_d.image);\n","   return 0;\n","}\n","\n","}"]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"pdkXNws0gXr2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BqQOefuegXr2"},"outputs":[],"source":["!nvcc -arch=sm_75 src/lez3/ppm_hist.cu -o ppm_hist -I GPUcomputing/utils/PPM GPUcomputing/utils/PPM/ppm.cpp\n","!./ppm_hist"]},{"cell_type":"markdown","metadata":{"id":"SOFMQZAkjlLW"},"source":["# ✅ Prodotto MQDB CUDA"]},{"cell_type":"markdown","source":["### ↘️ *`TODO...`*"],"metadata":{"id":"9QunXrnuC8AA"}},{"cell_type":"markdown","source":["Calcolare il prodotto di matrici MQDB con kernel CUDA"],"metadata":{"id":"lY5KTfZaC82S"}},{"cell_type":"code","metadata":{"id":"QVQVpcvKjkIk"},"source":["%%cuda_group_save --name \"mqdb_prod.cu\" --group \"MQDB\"\n","\n","#include \"/content/GPUcomputing/utils/common.h\"\n","#include \"/content/GPUcomputing/utils/MQDB/mqdb.h\"\n","\n","#define BLOCK_SIZE 16     // block size\n","\n","struct tms {\n","\tdouble CPUtms;\n","\tdouble GPUtmsNaive;\n","\tdouble GPUtmsMQDB;\n","\tfloat density;\n","};\n","\n","/*\n"," * Kernel for standard (naive) matrix product\n"," */\n","__global__ void matProd(mqdb A, mqdb B, mqdb C, int n) {\n","\t// row & col indexes\n","\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// each thread computes an entry of the product matrix\n","\tif ((row < n) && (col < n)) {\n","\t\tfloat val = 0;\n","\t\tfor (int k = 0; k < n; k++)\n","\t\t\tval += A.elem[row * n + k] * B.elem[k * n + col];\n","\t\tC.elem[row * n + col] = val;\n","\t}\n","}\n","\n","/*\n"," * Kernel for block sub-matrix product of mqdb\n"," */\n","__global__ void mqdbBlockProd(mqdb A, mqdb B, mqdb C, int sdim, int d, int n) {\n","\n","\t// TODO\n","\n","\t// jump to the right block sub-matrix\n","\n","\t// each thread computes an entry of the product matrix\n","}\n","\n","/*\n"," * Test on MQDB kernels\n"," */\n","void testKernelsMQDB(uint n, uint k, struct tms* times) {\n","\n","\t// mqdb host matrices\n","\tmqdb A, B, C, C1;\n","\n","\t// mqdb device matrices\n","\tmqdb d_A, d_B, d_C;\n","\n","\t// fill in\n","\tA = mqdbConst(n, k, 10, 1);\n","\tB = mqdbConst(n, k, 10, 1);\n","\tC = mqdbConst(n, k, 10, 1);\n","\tC1 = mqdbConst(n, k, 10, 1);\n","\n","\tulong nBytes = n * n * sizeof(float);\n","\tulong kBytes = k * sizeof(uint);\n","\tprintf(\"        Memory size required = %.1f (MB)\\n\",(float)nBytes/(1024.0*1024.0));\n","\n","\t// malloc and copy on device memory\n","\td_A.nBlocks = A.nBlocks;\n","\tCHECK(cudaMalloc((void**)&d_A.blkSize, kBytes));\n","\tCHECK(cudaMemcpy(d_A.blkSize, A.blkSize, kBytes, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMalloc((void**)&d_A.elem, nBytes));\n","\tCHECK(cudaMemcpy(d_A.elem, A.elem, nBytes, cudaMemcpyHostToDevice));\n","\td_B.nBlocks = B.nBlocks;\n","\tCHECK(cudaMalloc((void**)&d_B.blkSize, kBytes));\n","\tCHECK(cudaMemcpy(d_B.blkSize, B.blkSize, kBytes, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMalloc((void**)&d_B.elem, nBytes));\n","\tCHECK(cudaMemcpy(d_B.elem, B.elem, nBytes, cudaMemcpyHostToDevice));\n","\td_C.nBlocks = C.nBlocks;\n","\tCHECK(cudaMalloc((void**)&d_C.blkSize, kBytes));\n","\tCHECK(cudaMemcpy(d_C.blkSize, C.blkSize, kBytes, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMalloc((void**)&d_C.elem, nBytes));\n","\tCHECK(cudaMemset(d_C.elem, 0.0, nBytes));\n","\n","\t/***********************************************************/\n","\t/*                    CPU MQDB product                     */\n","\t/***********************************************************/\n","\tprintf(\"\\nCPU MQDB product...\\n\");\n","\tdouble start = seconds();\n","\tmqdbProd(A,B,C);\n","\tdouble CPUTime = seconds() - start;\n","\tprintf(\"   CPU elapsed time: %.5f (sec)\\n\\n\", CPUTime);\n","\n","\t/***********************************************************/\n","\t/*                     GPU mat product                     */\n","\t/***********************************************************/\n","\tprintf(\"Kernel (naive) mat product...\\n\");\n","\tdim3 block(BLOCK_SIZE, BLOCK_SIZE);\n","\tdim3 grid((n + block.x - 1) / block.x, (n + block.y - 1) / block.y);\n","\tstart = seconds();\n","\tmatProd<<<grid, block>>>(d_A, d_B, d_C, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble GPUtime1 = seconds() - start;\n","\tprintf(\"   elapsed time:                %.4f (sec)\\n\", GPUtime1);\n","\tprintf(\"   speedup vs CPU MQDB product: %.4f\\n\", CPUTime/GPUtime1);\n","\tCHECK(cudaMemcpy(C1.elem, d_C.elem, nBytes, cudaMemcpyDeviceToHost));\n","\tCHECK(cudaMemset(d_C.elem, 0.0, nBytes));\n","\tcheckResult(C,C1);\n","\t//\tmqdbDisplay(C1);\n","\n","\t/***********************************************************/\n","\t/*                     GPU MQDB product                    */\n","\t/***********************************************************/\n","\tprintf(\"Kernel MQDB product...\\n\");\n","\tdouble start = seconds();\n","\n","\t// TODO\n","\n"," \tCHECK(cudaDeviceSynchronize());\n","\tdouble GPUtime2 = seconds() - start;\n","\tprintf(\"   elapsed time:                    %.4f (sec)\\n\", GPUtime2);\n","\tprintf(\"   speedup vs CPU MQDB product:     %.4f\\n\", CPUTime/GPUtime2);\n","\tprintf(\"   speedup vs GPU std mat product:  %.4f\\n\", GPUtime1/GPUtime2);\n","\t// copy the array 'C' back from the GPU to the CPU\n","\tCHECK(cudaMemcpy(C1.elem, d_C.elem, nBytes, cudaMemcpyDeviceToHost));\n","\tCHECK(cudaMemset(d_C.elem, 0.0, nBytes));\n","\tcheckResult(C,C1);\n","\n","\tCHECK(cudaFree(d_A.elem));\n","\tCHECK(cudaFree(d_B.elem));\n","\tCHECK(cudaFree(d_C.elem));\n","\n","\t// collect times\n","\ttimes->CPUtms = CPUTime;\n","\ttimes->GPUtmsNaive = GPUtime1;\n","\ttimes->GPUtmsMQDB = GPUtime2;\n","\n","\tfloat den = 0;\n","\tfor (uint j = 0; j < k; j++)\n","\t\tden += A.blkSize[j]*A.blkSize[j];\n","\ttimes->density = den/(n*n);\n","}\n","\n","/*\n"," * main function\n"," */\n","int main(int argc, char *argv[]) {\n","\tuint n = 8*1024;      // matrix size\n","\tuint min_k = 30;       // max num of blocks\n","\tuint max_k = 30;       // max num of blocks\n","\n","\tstruct tms times[max_k-min_k+1];\n","\n","\t// multiple tests on kernels\n","\tfor (uint k = min_k; k <= max_k; k++) {\n","\t\tprintf(\"\\n*****   k = %d --- (avg block size = %f)\\n\",k,(float)n/k);\n","\t\ttestKernelsMQDB(n, k, &times[k-min_k]);\n","\t}\n","\n","\tFILE *fd;\n","\tfd = fopen(\"res.csv\", \"w\");\n","\tif (fd == NULL) {\n","\t\tperror(\"file error!\\n\");\n","\t\texit(1);\n","\t}\n","\n","\t// write results on file\n","\tfprintf(fd,\"num blocks,\");\n","\t\tfor (uint j = 0; j <= max_k-min_k; j++)\n","\t\t\tfprintf(fd,\"%d,\",j+min_k);\n","\n","\tfprintf(fd,\"\\nCPU MQDB product,\");\n","\tfor (uint j = 0; j <= max_k-min_k; j++)\n","\t\tfprintf(fd,\"%.6f,\",times[j].CPUtms);\n","\n","\tfprintf(fd,\"\\nKernel mat product naive,\");\n","\tfor (uint j = 0; j <= max_k-min_k; j++)\n","\t\tfprintf(fd,\"%.6f,\",times[j].GPUtmsNaive);\n","\n","\tfprintf(fd,\"\\nKernel MQDB product,\");\n","\tfor (uint j = 0; j <= max_k-min_k; j++)\n","\t\tfprintf(fd,\"%.6f,\",times[j].GPUtmsMQDB);\n","\n","\tfprintf(fd,\"\\ndensity,\");\n","\tfor (uint j = 0; j <= max_k-min_k; j++)\n","\t\tfprintf(fd,\"%.6f,\",times[j].density);\n","\n","\tfclose(fd);\n","\n","\treturn 0;\n","}\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wLxZjCx8bT3s"},"source":["# Compilazione ed esecuzione\n","\n","!nvcc -arch=sm_75 src/MQDB/mqdb_prod.cu /content/GPUcomputing/utils/MQDB/mqdb.cpp  -o mqdb_prod\n","!./mqdb_prod"],"execution_count":null,"outputs":[]}]}