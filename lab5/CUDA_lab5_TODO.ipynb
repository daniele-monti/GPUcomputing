{"cells":[{"cell_type":"markdown","metadata":{"id":"fZYqN0UwVLC_"},"source":["---\n","# **LAB 5 - Global memory (GMEM)**\n","---"]},{"cell_type":"markdown","metadata":{"id":"NO_C5o9-xRF_"},"source":["# ▶️ CUDA setup"]},{"cell_type":"code","metadata":{"id":"8fekR2O4xRGE"},"source":["!nvcc --version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Psl9iouxRGE"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GPU computing notebooks download (from github)"],"metadata":{"id":"gcg1GyK5srek"}},{"cell_type":"code","source":["!git clone https://github.com/giulianogrossi/GPUcomputing.git"],"metadata":{"id":"tyHOxci3s3H8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"227eLdP5csN1"},"source":["NVCC Plugin for Jupyter notebook"]},{"cell_type":"code","source":["%cd GPUcomputing/utils/nvcc4jupyter-master/\n","!!python3 -m build\n","%load_ext nvcc4jupyter\n","%cd /content/"],"metadata":{"id":"4TzxMBFds8aT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DvmApCF76YD0"},"source":["# ✅ Static and pinned memory"]},{"cell_type":"markdown","source":["Static memory..."],"metadata":{"id":"TZ2U2fFn04yf"}},{"cell_type":"code","source":["%%cuda_group_save --name \"static_mem.cu\" --group \"lez5\"\n","#include \"/content/GPUcomputing/utils/common.h\"\n","#include <stdio.h>\n","#define SIZE 16\n","\n","// global memory\n","__device__ int  d_value[SIZE];\n","int             h_value[SIZE];\n","\n","// kernel\n","__global__ void write_value() {\n","  d_value[threadIdx.x] += threadIdx.x;\n","  printf(\"value GPU[%d] = %d\\n\", threadIdx.x, d_value[threadIdx.x]);\n","}\n","\n","int main() {\n","\n","  // load host data\n","  for (int i = 0; i < SIZE; i++)\n","    h_value[i] = i;\n","\n","  // copy H2D using symbols\n","  CHECK(cudaMemcpyToSymbol(d_value, h_value, sizeof(h_value)));\n","\n","  // kernel launch\n","  write_value<<<1, SIZE>>>();\n","\n","  // Synchronize required before cudaMemcpy was synchronizing\n","  CHECK(cudaDeviceSynchronize());\n","\n","  // copy D2H using symbols\n","  CHECK(cudaMemcpyFromSymbol(h_value, d_value, sizeof(h_value)));\n","  for (int i = 0; i < SIZE; i++)\n","    printf(\"value CPU [%d] = %d\\n\", i, h_value[i]);\n","  return 0;\n","}"],"metadata":{"id":"g9uOMdl-09Bw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"ctyo0LAUTyw8"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez5/static_mem.cu -o static_mem\n","!./static_mem"],"metadata":{"id":"HJSOz4_RTyw9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↘️ **TODO...**"],"metadata":{"id":"1wm23k6-G38i"}},{"cell_type":"markdown","source":["- Usare la pinned memory per il trasferimento dati usando cudaMallocHost"],"metadata":{"id":"6cVFAorqG-n4"}},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"},"id":"48fLxwjsGbnt"},"outputs":[],"source":["%%cuda_group_save --name \"pin_mem.cu\" --group \"lez5\"\n","\n","#include \"/content/GPUcomputing/utils/common.h\"\n","#include <cuda_runtime.h>\n","#include <stdio.h>\n","\n","int main(int argc, char **argv) {\n","\n","  // memory size\n","  size_t isize = 1 << 25;\n","  size_t nbytes = isize * sizeof(float);\n","  printf(\"memory size = %lu byte (%5.2f MB)\\n\", isize, nbytes / (1024.0f * 1024.0f));\n","\n","  float *h_a, *h_b;\n","  // allocate the host memory\n","  h_a = (float *)malloc(nbytes);\n","  h_b = (float *)malloc(nbytes);\n","  for (uint i = 0; i < isize; i++)\n","    h_a[i] = 100.10f;\n","\n","  /***********************************************************/\n","\t/*              cudaMalloc & cudaMemcpy                    */\n","\t/***********************************************************/\n","\n","  printf(\"\\ncudaMalloc & cudaMemcpy...\\n\");\n","\n","  double start = seconds();\n","\n","  // allocate device memory\n","  float *d_a;\n","  CHECK(cudaMalloc((float **)&d_a, nbytes));\n","\n","  // transfer data from the host to the device\n","  CHECK(cudaMemcpy(d_a, h_a, nbytes, cudaMemcpyHostToDevice));\n","\n","  // transfer data from the device to the host\n","  CHECK(cudaMemcpy(h_b, d_a, nbytes, cudaMemcpyDeviceToHost));\n","\n","  CHECK(cudaDeviceSynchronize());\n","  printf(\"    Elapsed time: %f\\n\", seconds() - start);\n","\n","\n","  // free memory\n","  CHECK(cudaFree(d_a));\n","\n","  // reset device\n","\n","  CHECK(cudaDeviceReset());\n","\n","  /***********************************************************/\n","\t/*                  cudaMallocHost                         */\n","\t/***********************************************************/\n","\n","  printf(\"\\ncudaMallocHost...\\n\");\n","  start = seconds();\n","\n","  // TODO\n","\n","\n","  return EXIT_SUCCESS;\n","}\n"]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"a59ze29pGspS"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez5/uni_mem.cu -o uni_mem\n","!./uni_mem"],"metadata":{"id":"zVUdc45KGspT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vXUIQkZLCTcG"},"source":["# ✅ Unified memory\n"]},{"cell_type":"code","source":["%%cuda_group_save --name \"uni_mem.cu\" --group \"lez5\"\n","#include <stdio.h>\n","\n","__global__ void printme(char *str) {\n","  printf(\"%s\", str);\n","}\n","\n","int main() {\n","  // Allocate 100 bytes of memory, accessible to both Host and Device code\n","  char *s;\n","  cudaMallocManaged(&s, 100);\n","\n","  // Note direct Host-code use of \"s\"\n","  strncpy(s, \"Hello Unified Memory GPU\\n\", 99);\n","\n","  // Here we pass \"s\" to a kernel without explicitly copying\n","  printme<<< 1, 1 >>>(s);\n","\n","  // Synchronize required (before, cudaMemcpy was synchronizing)\n","  cudaDeviceSynchronize();\n","\n","  // change s\n","  s[21] = 'C';\n","  printf(\"%s\", s);\n","\n","  // Free as for normal CUDA allocations\n","  cudaFree(s);\n","  return  0;\n","}"],"metadata":{"id":"2gWSJ89vi61v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"gdjlET3eVwIR"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez5/uni_mem.cu -o uni_mem\n","!./uni_mem"],"metadata":{"id":"fmFO4qA4VwIS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↘️ **TODO...**"],"metadata":{"id":"LO8Q2CxFTyw9"}},{"cell_type":"markdown","source":["1. Definire la UMEM per ogni matrice\n","2. Effettuare la somma invocando il kernel\n","3. Analizzare i tempi di esecuzione\n"],"metadata":{"id":"AWOfDng2kprv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Y52R0d3CA50","vscode":{"languageId":"python"}},"outputs":[],"source":["%%cuda_group_save --name \"sum_mat_uni.cu\" --group \"lez5\"\n","\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","void initialData(float *ip, const int size) {\n","  int i;\n","\n","  for (i = 0; i < size; i++)\n","    ip[i] = (float)( rand() & 0xFF ) / 10.0f;\n","  return;\n","}\n","\n","void sumMatrixOnHost(float *A, float *B, float *C, const int nx, const int ny) {\n","  float *ia = A;\n","  float *ib = B;\n","  float *ic = C;\n","\n","  for (int iy = 0; iy < ny; iy++) {\n","    for (int ix = 0; ix < nx; ix++)\n","      ic[ix] = ia[ix] + ib[ix];\n","\n","    ia += nx;\n","    ib += nx;\n","    ic += nx;\n","  }\n","  return;\n","}\n","\n","void checkResult(float *hostRef, float *gpuRef, const int N) {\n","  double epsilon = 1.0E-8;\n","  bool match = 1;\n","\n","  for (int i = 0; i < N; i++) {\n","    if (abs(hostRef[i] - gpuRef[i]) > epsilon) {\n","      match = 0;\n","      printf(\"host %f gpu %f\\n\", hostRef[i], gpuRef[i]);\n","      break;\n","    }\n","  }\n","\n","  if (!match)\n","    printf(\"Arrays do not match.\\n\\n\");\n","}\n","\n","// grid 2D block 2D\n","__global__ void sumMatrixGPU(float *MatA, float *MatB, float *MatC, int nx, int ny) {\n","  unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;\n","  unsigned int iy = threadIdx.y + blockIdx.y * blockDim.y;\n","  unsigned int idx = iy * nx + ix;\n","\n","  if (ix < nx && iy < ny)\n","    MatC[idx] = MatA[idx] + MatB[idx];\n","}\n","\n","// MAIN\n","int main(int argc, char **argv) {\n","  printf(\"%s Starting \", argv[0]);\n","\n","  // set up data size of matrix\n","\n","  // malloc unified host memory\n","\n","  // TODO\n","\n","  // invoke kernel at host side\n","\n","  // TODO\n","\n","  // free device global memory\n","\n","\n","  return (0);\n","}\n"]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"NIsdOKGOXB5m"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez5/sum_mat_uni.cu -o summat_uni\n","!./summat_uni 14"],"metadata":{"id":"5FHBRS0hXB5n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SbQlRthtJTQE"},"source":["# ✅ Transpose"]},{"cell_type":"markdown","source":["↘️ **TODO...**"],"metadata":{"id":"DzOSJoNGZIkZ"}},{"cell_type":"markdown","source":["passi per la trasposizione con SMEM:\n","\n","1. definire la dim della SMEM pari alla dim del blocco\n","2. Il warp scrive i dati nella shared memory in row-major ordering evitando bank conflict sulle scritture. Ogni warp fa una letture coalescente dei dati in global memory\n","3. sincronizzare i thread\n"],"metadata":{"id":"cFH-7ue8uYNM"}},{"cell_type":"code","source":["%%cuda_group_save --name \"transposeSMEM.cu\" --group \"lez5\"\n","\n","#include <stdio.h>\n","#include \"/content/GPUcomputing/utils/common.h\"\n","#define INDEX(rows, cols, stride) (rows * stride + cols)\n","#define BDIMX 32\n","#define BDIMY 32\n","\n","// prototipi funzioni\n","void initialData(float*, const int);\n","void printData(float*, int, int);\n","void checkResult(float*, float*, int, int);\n","void transposeHost(float*, float*, const int, const int);\n","\n","/*\n"," * Kernel per il calcolo della matrice trasposta usando la shared memory\n"," */\n","__global__ void transposeSmem(float *out, float *in, int nrows, int ncols) {\n","\t// static shared memory\n","\n","\t// TODO\n","\n","}\n","\n","// naive: access data in rows\n","__global__ void copyRow(float *out, float *in, const int nrows,\tconst int ncols) {\n","\t// matrix coordinate (ix,iy)\n","\tunsigned int row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tunsigned int col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// transpose with boundary test\n","\tif (row < nrows && col < ncols)\n","\t\tout[INDEX(col, row, nrows)] = in[INDEX(row, col, ncols)];\n","}\n","\n","// naive: access data in cols\n","__global__ void copyCol(float *out, float *in, const int nrows,\tconst int ncols) {\n","\t// matrix coordinate (ix,iy)\n","\tunsigned int row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tunsigned int col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// transpose with boundary test\n","\tif (row < nrows && col < ncols)\n","\t\tout[INDEX(row, col, ncols)] = in[INDEX(col, row, nrows)];\n","}\n","\n","// MAIN\n","int main(int argc, char **argv) {\n","\n","\tbool iprint = 0;\n","\n","\t// set up array size\n","\tint nrows = 1 << 14;\n","\tint ncols = 1 << 14;\n","\n","\tif (argc > 1)\n","\t\tiprint = atoi(argv[1]);\n","\tif (argc > 2)\n","\t\tnrows = atoi(argv[2]);\n","\tif (argc > 3)\n","\t\tncols = atoi(argv[3]);\n","\n","\tprintf(\"\\nMatrice con nrows = %d ncols = %d\\n\", nrows, ncols);\n","\tsize_t ncells = nrows * ncols;\n","\tsize_t nBytes = ncells * sizeof(float);\n","\n","\t// allocate host memory\n","\tfloat *A_h = (float *) malloc(nBytes);\n","  float *B_h = (float *) malloc(nBytes);\n","  float *AT_h = (float *) malloc(nBytes);\n","\n","\t// Allocate Unified Memory – accessible from CPU or GPU\n","\tfloat *d_A, *d_AT;\n","\tCHECK(cudaMalloc((void** )&d_A, nBytes));\n","  CHECK(cudaMalloc((void** )&d_AT, nBytes));\n","\n","\t//  initialize host array\n","\tinitialData(A_h, nrows * ncols);\n","\tif (iprint)\n","\t\tprintData(A_h, nrows, ncols);\n","\n","\t//  transpose at host side\n","\ttransposeHost(A_h, B_h, nrows, ncols);\n","\n","  /***********************************************************/\n","\t/*                KERNEL: col copy                         */\n","\t/***********************************************************/\n","\n","  printf(\"*** KERNEL: col copy  ***\\n\");\n","\t// tranpose gmem\n","  CHECK(cudaMemcpy(d_A, A_h, nBytes, cudaMemcpyHostToDevice));\n","  dim3 block(BDIMX, BDIMY, 1);\n","\tdim3 grid((ncols + block.x - 1) / block.x, (nrows + block.y - 1) / block.y, 1);\n","\n","  double iStart = seconds();\n","\tcopyCol<<<grid, block>>>(d_AT, d_A, nrows, ncols);\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble iElaps = seconds() - iStart;\n","\n","\t// check result\n","\tCHECK(cudaMemcpy(AT_h, d_AT, nBytes, cudaMemcpyDeviceToHost));\n","\tcheckResult(A_h, B_h, nrows, ncols);\n","\n","\tdouble ibnd = 2 * ncells * sizeof(float) / 1e9 / iElaps;\n","\tprintf(\"col copy elapsed %f sec\\n <<< grid (%d,%d) block (%d,%d)>>> \"\n","\t\t\t\"effective bandwidth %f GB\\n\\n\", iElaps, grid.x, grid.y, block.x,\tblock.y, ibnd);\n","\n","  /***********************************************************/\n","\t/*                KERNEL: row copy                         */\n","\t/***********************************************************/\n","\n","  printf(\"*** KERNEL: row copy  ***\\n\");\n","\t// tranpose gmem\n","\n","\tiStart = seconds();\n","\tcopyRow<<<grid, block>>>(d_AT, d_A, nrows, ncols);\n","\tCHECK(cudaDeviceSynchronize());\n","\tiElaps = seconds() - iStart;\n","\n","\t// check result\n","  CHECK(cudaMemcpy(AT_h, d_AT, nBytes, cudaMemcpyDeviceToHost));\n","\tcheckResult(A_h, B_h, nrows, ncols);\n","\n","\tibnd = 2 * ncells * sizeof(float) / 1e9 / iElaps;\n","\tprintf(\"row copy elapsed %f sec\\n <<< grid (%d,%d) block (%d,%d)>>> \"\n","\t\t\t\"effective bandwidth %f GB\\n\\n\", iElaps, grid.x, grid.y, block.x,\tblock.y, ibnd);\n","\n","  /***********************************************************/\n","\t/*                KERNEL: SMEM copy                        */\n","\t/***********************************************************/\n","\n","\tprintf(\"*** KERNEL: transposeSmem ***\\n\");\n","\t// tranpose smem\n","\n","\n","\t// TODO\n","\n","\t// free host and device memory\n","\n","\treturn EXIT_SUCCESS;\n","}\n","\n","void initialData(float *in, const int size) {\n","\tfor (int i = 0; i < size; i++)\n","\t\tin[i] = i; // (float)(rand()/INT_MAX) * 10.0f;\n","\treturn;\n","}\n","\n","void printData(float *in, int nrows, int ncols) {\n","\tfor (int i = 0; i < nrows; i++) {\n","\t\tfor (int j = 0; j < ncols; j++)\n","\t\t\tprintf(\"%3.0f \", in[INDEX(i, j, ncols)]);\n","\t\tprintf(\"\\n\");\n","\t}\n","}\n","\n","void transposeHost(float *out, float *in, const int nrows, const int ncols) {\n","\tfor (int iy = 0; iy < nrows; ++iy)\n","\t\tfor (int ix = 0; ix < ncols; ++ix)\n","\t\t\tout[INDEX(ix, iy, nrows)] = in[INDEX(iy, ix, ncols)];\n","}\n","\n","void checkResult(float *hostRef, float *gpuRef, int rows, int cols) {\n","\tdouble epsilon = 1.0E-8;\n","\tbool match = 1;\n","\n","\tfor (int i = 0; i < rows; i++) {\n","\t\tfor (int j = 0; j < cols; j++) {\n","\t\t\tint index = INDEX(i, j, cols);\n","\t\t\tif (abs(hostRef[index] - gpuRef[index]) > epsilon) {\n","\t\t\t\tmatch = 0;\n","\t\t\t\tprintf(\"different on (%d, %d) (offset=%d) element in \"\n","\t\t\t\t\t\t\"transposed matrix: host %f gpu %f\\n\", i, j, index,\n","\t\t\t\t\t\thostRef[index], gpuRef[index]);\n","\t\t\t\tbreak;\n","\t\t\t}\n","\t\t}\n","\t\tif (!match)\n","\t\t\tbreak;\n","\t}\n","\n","\tif (!match)\n","\t\tprintf(\"Arrays do not match.\\n\");\n","}\n"],"metadata":{"id":"xBFQxvgumfdu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"EZVOb_mOZEZ-"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez5/transposeSMEM.cu -o transposeSMEM\n","!./transposeSMEM"],"metadata":{"id":"_WzHYqto-TfG"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["NO_C5o9-xRF_","DvmApCF76YD0","vXUIQkZLCTcG","SbQlRthtJTQE"],"private_outputs":true,"provenance":[{"file_id":"1EAQ77JVY4mIL9gXqvuZwOSOQGrZPM_31","timestamp":1743657680968}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}