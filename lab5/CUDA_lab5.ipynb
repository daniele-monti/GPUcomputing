{"cells":[{"cell_type":"markdown","metadata":{"id":"fZYqN0UwVLC_"},"source":["---\n","# **LAB 5 - Global memory (GMEM)**\n","---"]},{"cell_type":"markdown","metadata":{"id":"NO_C5o9-xRF_"},"source":["# ▶️ CUDA setup"]},{"cell_type":"code","metadata":{"id":"8fekR2O4xRGE"},"source":["!nvcc --version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Psl9iouxRGE"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GPU computing notebooks download (from github)"],"metadata":{"id":"gcg1GyK5srek"}},{"cell_type":"code","source":["!git clone https://github.com/giulianogrossi/GPUcomputing.git"],"metadata":{"id":"tyHOxci3s3H8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"227eLdP5csN1"},"source":["NVCC Plugin for Jupyter notebook"]},{"cell_type":"code","source":["%cd GPUcomputing/utils/nvcc4jupyter-master/\n","!!python3 -m build\n","%load_ext nvcc4jupyter\n","%cd /content/"],"metadata":{"id":"4TzxMBFds8aT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DvmApCF76YD0"},"source":["# ✅ Static and pinned memory"]},{"cell_type":"markdown","source":["Static memory..."],"metadata":{"id":"TZ2U2fFn04yf"}},{"cell_type":"code","source":["%%cuda_group_save --name \"static_mem.cu\" --group \"lez5\"\n","#include \"/content/GPUcomputing/utils/common.h\"\n","#include <stdio.h>\n","#define SIZE 16\n","\n","// global memory\n","__device__ int  d_value[SIZE];\n","int             h_value[SIZE];\n","\n","// kernel\n","__global__ void write_value() {\n","  d_value[threadIdx.x] += threadIdx.x;\n","  printf(\"value GPU[%d] = %d\\n\", threadIdx.x, d_value[threadIdx.x]);\n","}\n","\n","int main() {\n","\n","  // load host data\n","  for (int i = 0; i < SIZE; i++)\n","    h_value[i] = i;\n","\n","  // copy H2D using symbols\n","  CHECK(cudaMemcpyToSymbol(d_value, h_value, sizeof(h_value)));\n","\n","  // kernel launch\n","  write_value<<<1, SIZE>>>();\n","\n","  // Synchronize required before cudaMemcpy was synchronizing\n","  CHECK(cudaDeviceSynchronize());\n","\n","  // copy D2H using symbols\n","  CHECK(cudaMemcpyFromSymbol(h_value, d_value, sizeof(h_value)));\n","  for (int i = 0; i < SIZE; i++)\n","    printf(\"value CPU [%d] = %d\\n\", i, h_value[i]);\n","  return 0;\n","}"],"metadata":{"id":"g9uOMdl-09Bw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"ctyo0LAUTyw8"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez5/static_mem.cu -o static_mem\n","!./static_mem"],"metadata":{"id":"HJSOz4_RTyw9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4ApFq6kF6Olh"},"source":["**Pinned memory**\n","\n","An example of using CUDA's memory copy API to transfer data to and from the device. In this case, `cudaMalloc` is used to allocate memory on the GPU and `cudaMemcpy` is used to transfer the contents of host memory to an array allocated using `cudaMalloc`. Host memory is allocated using `cudaMallocHost` to create a page-locked host array."]},{"cell_type":"markdown","source":["↘️ **SOL...**"],"metadata":{"id":"dmtCni82wn9_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"XObjkBa16Wx2","vscode":{"languageId":"python"}},"outputs":[],"source":["%%cuda_group_save --name \"pin_mem.cu\" --group \"lez5\"\n","\n","#include \"/content/GPUcomputing/utils/common.h\"\n","#include <cuda_runtime.h>\n","#include <stdio.h>\n","\n","int main(int argc, char **argv) {\n","\n","  // memory size\n","  size_t isize = 1 << 25;\n","  size_t nbytes = isize * sizeof(float);\n","  printf(\"memory size = %lu byte (%5.2f MB)\\n\", isize, nbytes / (1024.0f * 1024.0f));\n","\n","  float *h_a, *h_b;\n","  // allocate the host memory\n","  h_a = (float *)malloc(nbytes);\n","  h_b = (float *)malloc(nbytes);\n","  for (uint i = 0; i < isize; i++)\n","    h_a[i] = 100.10f;\n","\n","  /***********************************************************/\n","\t/*              cudaMalloc & cudaMemcpy                    */\n","\t/***********************************************************/\n","\n","  printf(\"\\ncudaMalloc & cudaMemcpy...\\n\");\n","\n","  double start = seconds();\n","\n","  // allocate device memory\n","  float *d_a;\n","  CHECK(cudaMalloc((float **)&d_a, nbytes));\n","\n","  // transfer data from the host to the device\n","  CHECK(cudaMemcpy(d_a, h_a, nbytes, cudaMemcpyHostToDevice));\n","\n","  // transfer data from the device to the host\n","  CHECK(cudaMemcpy(h_b, d_a, nbytes, cudaMemcpyDeviceToHost));\n","\n","  CHECK(cudaDeviceSynchronize());\n","  printf(\"    Elapsed time: %f\\n\", seconds() - start);\n","\n","\n","  // free memory\n","  CHECK(cudaFree(d_a));\n","\n","  // reset device\n","\n","  CHECK(cudaDeviceReset());\n","\n","  /***********************************************************/\n","\t/*                  cudaMallocHost                         */\n","\t/***********************************************************/\n","\n","  printf(\"\\ncudaMallocHost...\\n\");\n","  start = seconds();\n","\n","  // allocate pinned host memory\n","  float *h_c;\n","  CHECK(cudaMallocHost ((float **)&h_c, nbytes));\n","\n","  //for (uint i = 0; i < isize; i++)\n","  //  h_c[i] = 100.10f;\n","\n","\n","  // allocate device memory\n","  CHECK(cudaMalloc((float **)&d_a, nbytes));\n","\n","  // transfer data from the host to the device\n","  CHECK(cudaMemcpy(d_a, h_c, nbytes, cudaMemcpyHostToDevice));\n","\n","  // transfer data from the device to the host\n","  CHECK(cudaMemcpy(h_b, d_a, nbytes, cudaMemcpyDeviceToHost));\n","\n","  CHECK(cudaDeviceSynchronize());\n","  printf(\"    Elapsed time: %f\\n\", seconds() - start);\n","\n","  // free memory\n","  CHECK(cudaFree(d_a));\n","  CHECK(cudaFreeHost(h_c));\n","\n","\n","\n","\n","  return EXIT_SUCCESS;\n","}\n"]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"iecfvSEJKTDr"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez5/pin_mem.cu -o pin_mem\n","!./pin_mem"],"metadata":{"id":"7ynJAKGSKTDs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vXUIQkZLCTcG"},"source":["# ✅ Unified memory\n"]},{"cell_type":"code","source":["%%cuda_group_save --name \"uni_mem.cu\" --group \"lez5\"\n","#include <stdio.h>\n","\n","__global__ void printme(char *str) {\n","  printf(\"%s\", str);\n","}\n","\n","int main() {\n","  // Allocate 100 bytes of memory, accessible to both Host and Device code\n","  char *s;\n","  cudaMallocManaged(&s, 100);\n","\n","  // Note direct Host-code use of \"s\"\n","  strncpy(s, \"Hello Unified Memory GPU\\n\", 99);\n","\n","  // Here we pass \"s\" to a kernel without explicitly copying\n","  printme<<< 1, 1 >>>(s);\n","\n","  // Synchronize required (before, cudaMemcpy was synchronizing)\n","  cudaDeviceSynchronize();\n","\n","  // change s\n","  s[21] = 'C';\n","  printf(\"%s\", s);\n","\n","  // Free as for normal CUDA allocations\n","  cudaFree(s);\n","  return  0;\n","}"],"metadata":{"id":"2gWSJ89vi61v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"gdjlET3eVwIR"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez5/uni_mem.cu -o uni_mem\n","!./uni_mem"],"metadata":{"id":"fmFO4qA4VwIS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↘️ **SOL...**"],"metadata":{"id":"fIaesfiqIkZ5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Stv28L2PU-Kg","vscode":{"languageId":"python"}},"outputs":[],"source":["%%cuda_group_save --name \"sum_mat.cu\" --group \"lez5\"\n","\n","#include <stdio.h>\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","void initialData(float *ip, const int size) {\n","  int i;\n","\n","  for (i = 0; i < size; i++)\n","    ip[i] = (float)( rand() & 0xFF ) / 10.0f;\n","  return;\n","}\n","\n","void sumMatrixOnHost(float *A, float *B, float *C, const int nx, const int ny) {\n","  float *ia = A;\n","  float *ib = B;\n","  float *ic = C;\n","\n","  for (int iy = 0; iy < ny; iy++) {\n","    for (int ix = 0; ix < nx; ix++)\n","      ic[ix] = ia[ix] + ib[ix];\n","\n","    ia += nx;\n","    ib += nx;\n","    ic += nx;\n","  }\n","  return;\n","}\n","\n","void checkResult(float *hostRef, float *gpuRef, const int N) {\n","  double epsilon = 1.0E-8;\n","  bool match = 1;\n","\n","  for (int i = 0; i < N; i++) {\n","    if (abs(hostRef[i] - gpuRef[i]) > epsilon) {\n","      match = 0;\n","      printf(\"host %f gpu %f\\n\", hostRef[i], gpuRef[i]);\n","      break;\n","    }\n","  }\n","\n","  if (!match)\n","    printf(\"Arrays do not match.\\n\\n\");\n","}\n","\n","// matrix sum with grid 2D block 2D\n","__global__ void sumMatrixGPU(float *MatA, float *MatB, float *MatC, int nx, int ny) {\n","  unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;\n","  unsigned int iy = threadIdx.y + blockIdx.y * blockDim.y;\n","  unsigned int idx = iy * nx + ix;\n","\n","  if (ix < nx && iy < ny)\n","    MatC[idx] = MatA[idx] + MatB[idx];\n","}\n","\n","// MAIN\n","int main(int argc, char **argv) {\n","    printf(\"%s Starting \", argv[0]);\n","\n","    // set up data size of matrix\n","    int nx, ny;\n","    int ishift = 12;\n","    if  (argc > 1) ishift = atoi(argv[1]);\n","    nx = ny = 1 << ishift;\n","\n","    int nxy = nx * ny;\n","    int nBytes = nxy * sizeof(float);\n","    printf(\"Matrix size: nx %d ny %d\\n\", nx, ny);\n","\n","    // malloc host memory\n","    float *h_A, *h_B, *hostRef, *gpuRef;\n","    h_A = (float *)malloc(nBytes);\n","    h_B = (float *)malloc(nBytes);\n","    hostRef = (float *)malloc(nBytes);\n","    gpuRef = (float *)malloc(nBytes);\n","\n","    // initialize data at host side\n","    double iStart = seconds();\n","    initialData(h_A, nxy);\n","    initialData(h_B, nxy);\n","    double iElaps = seconds() - iStart;\n","\n","    printf(\"initialization: \\t %f sec\\n\", iElaps);\n","\n","    memset(hostRef, 0, nBytes);\n","    memset(gpuRef, 0, nBytes);\n","\n","    // add matrix at host side for result checks\n","    iStart = seconds();\n","    sumMatrixOnHost(h_A, h_B, hostRef, nx, ny);\n","    iElaps = seconds() - iStart;\n","    printf(\"sumMatrix on host:\\t %f sec\\n\", iElaps);\n","\n","    // malloc device global memory\n","    float *d_MatA, *d_MatB, *d_MatC;\n","    CHECK(cudaMalloc((void **)&d_MatA, nBytes));\n","    CHECK(cudaMalloc((void **)&d_MatB, nBytes));\n","    CHECK(cudaMalloc((void **)&d_MatC, nBytes));\n","\n","    // invoke kernel at host side\n","    int dimx = 32;\n","    int dimy = 32;\n","    dim3 block(dimx, dimy);\n","    dim3 grid((nx + block.x - 1) / block.x, (ny + block.y - 1) / block.y);\n","\n","    iStart =  seconds();\n","    // transfer data from host to device\n","    CHECK(cudaMemcpy(d_MatA, h_A, nBytes, cudaMemcpyHostToDevice));\n","    CHECK(cudaMemcpy(d_MatB, h_B, nBytes, cudaMemcpyHostToDevice));\n","\n","    sumMatrixGPU<<<grid, block>>>(d_MatA, d_MatB, d_MatC, nx, ny);\n","    CHECK(cudaDeviceSynchronize());\n","    iElaps = seconds() - iStart;\n","    printf(\"sumMatrix on gpu :\\t %f sec <<<(%d,%d), (%d,%d)>>> \\n\", iElaps, grid.x, grid.y, block.x, block.y);\n","    CHECK(cudaMemcpy(gpuRef, d_MatC, nBytes, cudaMemcpyDeviceToHost));\n","    // check kernel error\n","    CHECK(cudaGetLastError());\n","    // check device results\n","    checkResult(hostRef, gpuRef, nxy);\n","\n","    // free device global memory\n","    CHECK(cudaFree(d_MatA));\n","    CHECK(cudaFree(d_MatB));\n","    CHECK(cudaFree(d_MatC));\n","\n","    // free host memory\n","    free(h_A);\n","    free(h_B);\n","    free(hostRef);\n","    free(gpuRef);\n","\n","    // reset device\n","    CHECK(cudaDeviceReset());\n","\n","    return (0);\n","}\n"]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"771930GWKsdM"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez5/sum_mat.cu -o summat\n","!./summat 14"],"metadata":{"id":"rnarHfYHKsdN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↘️ **TODO...**"],"metadata":{"id":"LO8Q2CxFTyw9"}},{"cell_type":"markdown","source":["1. Definire la UMEM per ogni matrice\n","2. Effettuare la somma invocando il kernel\n","3. Analizzare i tempi di esecuzione\n"],"metadata":{"id":"AWOfDng2kprv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Y52R0d3CA50","vscode":{"languageId":"python"}},"outputs":[],"source":["%%cuda_group_save --name \"sum_mat_uni.cu\" --group \"lez5\"\n","\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","void initialData(float *ip, const int size) {\n","  int i;\n","\n","  for (i = 0; i < size; i++)\n","    ip[i] = (float)( rand() & 0xFF ) / 10.0f;\n","  return;\n","}\n","\n","void sumMatrixOnHost(float *A, float *B, float *C, const int nx, const int ny) {\n","  float *ia = A;\n","  float *ib = B;\n","  float *ic = C;\n","\n","  for (int iy = 0; iy < ny; iy++) {\n","    for (int ix = 0; ix < nx; ix++)\n","      ic[ix] = ia[ix] + ib[ix];\n","\n","    ia += nx;\n","    ib += nx;\n","    ic += nx;\n","  }\n","  return;\n","}\n","\n","void checkResult(float *hostRef, float *gpuRef, const int N) {\n","  double epsilon = 1.0E-8;\n","  bool match = 1;\n","\n","  for (int i = 0; i < N; i++) {\n","    if (abs(hostRef[i] - gpuRef[i]) > epsilon) {\n","      match = 0;\n","      printf(\"host %f gpu %f\\n\", hostRef[i], gpuRef[i]);\n","      break;\n","    }\n","  }\n","\n","  if (!match)\n","    printf(\"Arrays do not match.\\n\\n\");\n","}\n","\n","// grid 2D block 2D\n","__global__ void sumMatrixGPU(float *MatA, float *MatB, float *MatC, int nx, int ny) {\n","  unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;\n","  unsigned int iy = threadIdx.y + blockIdx.y * blockDim.y;\n","  unsigned int idx = iy * nx + ix;\n","\n","  if (ix < nx && iy < ny)\n","    MatC[idx] = MatA[idx] + MatB[idx];\n","}\n","\n","//# MAIN\n","int main(int argc, char **argv) {\n","  printf(\"%s Starting \", argv[0]);\n","\n","  // set up data size of matrix\n","  int nx, ny;\n","  int ishift = 14;\n","  if  (argc > 1) ishift = atoi(argv[1]);\n","  nx = ny = 1 << ishift;\n","\n","  int nxy = nx * ny;\n","  int nBytes = nxy * sizeof(float);\n","  printf(\"Matrix size: nx %d ny %d\\n\", nx, ny);\n","\n","  // malloc unified host memory\n","  float *A, *B, *gpuRef;\n","  CHECK(cudaMallocManaged((void **)&A, nBytes));\n","  CHECK(cudaMallocManaged((void **)&B, nBytes));\n","  CHECK(cudaMallocManaged((void **)&gpuRef,  nBytes);  );\n","\n","  // invoke kernel at host side\n","  int dimx = 32;\n","  int dimy = 32;\n","  dim3 block(dimx, dimy);\n","  dim3 grid((nx + block.x - 1) / block.x, (ny + block.y - 1) / block.y);\n","\n","  // after warm-up, time with unified memory\n","  sumMatrixGPU<<<grid, block>>>(A, B, gpuRef, nx, ny);\n","  CHECK(cudaDeviceSynchronize());\n","\n","  initialData(gpuRef, nxy);\n","\n","  double iStart = seconds();\n","  sumMatrixGPU<<<grid, block>>>(A, B, gpuRef, nx, ny);\n","  CHECK(cudaDeviceSynchronize());\n","  double iElaps = seconds() - iStart;\n","  printf(\"sumMatrix on gpu :\\t %f sec <<<(%d,%d), (%d,%d)>>> \\n\", iElaps, grid.x, grid.y, block.x, block.y);\n","\n","  // check kernel error\n","  CHECK(cudaGetLastError());\n","\n","  // free device global memory\n","  CHECK(cudaFree(A));\n","  CHECK(cudaFree(B));\n","  CHECK(cudaFree(gpuRef));\n","\n","  // reset device\n","  CHECK(cudaDeviceReset());\n","\n","  return (0);\n","}\n"]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"NIsdOKGOXB5m"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez5/sum_mat_uni.cu -o summat_uni\n","!./summat_uni 14"],"metadata":{"id":"5FHBRS0hXB5n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SbQlRthtJTQE"},"source":["# ✅ Transpose"]},{"cell_type":"markdown","source":["↘️ **SOL...**"],"metadata":{"id":"JNzcy79FJ-yA"}},{"cell_type":"markdown","source":["passi per la trasposizione con SMEM:\n","\n","1. definire la dim della SMEM pari alla dim del blocco\n","2. Il warp scrive i dati nella shared memory in row-major ordering evitando bank conflict sulle scritture. Ogni warp fa una letture coalescente dei dati in global memory\n","3. sincronizzare i thread\n"],"metadata":{"id":"cFH-7ue8uYNM"}},{"cell_type":"code","source":["%%cuda_group_save --name \"transposeSMEM.cu\" --group \"lez5\"\n","\n","#include <stdio.h>\n","#include \"/content/GPUcomputing/utils/common.h\"\n","#define INDEX(rows, cols, stride) (rows * stride + cols)\n","#define BDIMX 32\n","#define BDIMY 32\n","\n","// prototipi funzioni\n","void initialData(float*, const int);\n","void printData(float*, int, int);\n","void checkResult(float*, float*, int, int);\n","void transposeHost(float*, float*, const int, const int);\n","\n","/*\n"," * Kernel per il calcolo della matrice trasposta usando la shared memory\n"," */\n","__global__ void transposeSmem(float *out, float *in, int nrows, int ncols) {\n","\t// static shared memory\n","\t__shared__ float tile[BDIMY][BDIMX];\n","\n","\t// coordinate matrice originale\n","\tunsigned int row = blockDim.y * blockIdx.y + threadIdx.y;\n","\tunsigned int col = blockDim.x * blockIdx.x + threadIdx.x;\n","\n","\t// trasferimento dati dalla global memory alla shared memory\n","\tif (row < nrows && col < ncols)\n","\t\ttile[threadIdx.y][threadIdx.x] = in[INDEX(row, col, ncols)];\n","\n","\t// thread synchronization\n","\t__syncthreads();\n","\n","\t// offset blocco trasposto\n","\tint y = blockIdx.x * blockDim.x + threadIdx.y;\n","\tint x = blockIdx.y * blockDim.y + threadIdx.x;\n","\n","\t// controlli invertiti nelle dim riga colonna\n","\tif (y < ncols && x < nrows)\n","\t\tout[y*nrows + x] = tile[threadIdx.x][threadIdx.y];\n","}\n","\n","//# naive: access data in rows\n","__global__ void copyRow(float *out, float *in, const int nrows,\tconst int ncols) {\n","\t// matrix coordinate (ix,iy)\n","\tunsigned int row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tunsigned int col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// transpose with boundary test\n","\tif (row < nrows && col < ncols)\n","\t\tout[INDEX(col, row, nrows)] = in[INDEX(row, col, ncols)];\n","}\n","\n","//# naive: access data in cols\n","__global__ void copyCol(float *out, float *in, const int nrows,\tconst int ncols) {\n","\t// matrix coordinate (ix,iy)\n","\tunsigned int row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tunsigned int col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// transpose with boundary test\n","\tif (row < nrows && col < ncols)\n","\t\tout[INDEX(row, col, ncols)] = in[INDEX(col, row, nrows)];\n","}\n","\n","// MAIN\n","int main(int argc, char **argv) {\n","\t// set up device\n","\tint dev = 0;\n","\tcudaDeviceProp deviceProp;\n","\tCHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","\tprintf(\"%s starting transpose at \", argv[0]);\n","\tprintf(\"device %d: %s \", dev, deviceProp.name);\n","\tCHECK(cudaSetDevice(dev));\n","\n","\tbool iprint = 0;\n","\n","\t// set up array size\n","\tint nrows = 1 << 14;\n","\tint ncols = 1 << 14;\n","\n","\tif (argc > 1)\n","\t\tiprint = atoi(argv[1]);\n","\tif (argc > 2)\n","\t\tnrows = atoi(argv[2]);\n","\tif (argc > 3)\n","\t\tncols = atoi(argv[3]);\n","\n","\tprintf(\"\\nMatrice con nrows = %d ncols = %d\\n\", nrows, ncols);\n","\tsize_t ncells = nrows * ncols;\n","\tsize_t nBytes = ncells * sizeof(float);\n","\n","\t// allocate host memory\n","\tfloat *A_h = (float *) malloc(nBytes);\n","  float *B_h = (float *) malloc(nBytes);\n","  float *AT_h = (float *) malloc(nBytes);\n","\n","\t// Allocate Unified Memory – accessible from CPU or GPU\n","\tfloat *d_A, *d_AT;\n","\tCHECK(cudaMalloc((void** )&d_A, nBytes));\n","  CHECK(cudaMalloc((void** )&d_AT, nBytes));\n","\n","\t//  initialize host array\n","\tinitialData(A_h, nrows * ncols);\n","\tif (iprint)\n","\t\tprintData(A_h, nrows, ncols);\n","\n","\t//  transpose at host side\n","\ttransposeHost(A_h, B_h, nrows, ncols);\n","\n","\n","  printf(\"*** KERNEL: col copy  ***\\n\");\n","\t// tranpose gmem\n","  CHECK(cudaMemcpy(d_A, A_h, nBytes, cudaMemcpyHostToDevice));\n","  dim3 block(BDIMX, BDIMY, 1);\n","\tdim3 grid((ncols + block.x - 1) / block.x, (nrows + block.y - 1) / block.y, 1);\n","\n","  double iStart = seconds();\n","\tcopyCol<<<grid, block>>>(d_AT, d_A, nrows, ncols);\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble iElaps = seconds() - iStart;\n","\n","\t// check result\n","\tCHECK(cudaMemcpy(AT_h, d_AT, nBytes, cudaMemcpyDeviceToHost));\n","\tcheckResult(A_h, B_h, nrows, ncols);\n","\n","\tdouble ibnd = 2 * ncells * sizeof(float) / 1e9 / iElaps;\n","\tprintf(\"col copy elapsed %f sec\\n <<< grid (%d,%d) block (%d,%d)>>> \"\n","\t\t\t\"effective bandwidth %f GB\\n\\n\", iElaps, grid.x, grid.y, block.x,\tblock.y, ibnd);\n","\n","\n","  printf(\"*** KERNEL: row copy  ***\\n\");\n","\t// tranpose gmem\n","\n","\tiStart = seconds();\n","\tcopyRow<<<grid, block>>>(d_AT, d_A, nrows, ncols);\n","\tCHECK(cudaDeviceSynchronize());\n","\tiElaps = seconds() - iStart;\n","\n","\t// check result\n","  CHECK(cudaMemcpy(AT_h, d_AT, nBytes, cudaMemcpyDeviceToHost));\n","\tcheckResult(A_h, B_h, nrows, ncols);\n","\n","\tibnd = 2 * ncells * sizeof(float) / 1e9 / iElaps;\n","\tprintf(\"row copy elapsed %f sec\\n <<< grid (%d,%d) block (%d,%d)>>> \"\n","\t\t\t\"effective bandwidth %f GB\\n\\n\", iElaps, grid.x, grid.y, block.x,\tblock.y, ibnd);\n","\n","\n","\tprintf(\"*** KERNEL: transposeSmem ***\\n\");\n","\t// tranpose smem\n","\n","\tiStart = seconds();\n","\ttransposeSmem<<<grid, block>>>(d_AT, d_A, nrows, ncols);\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble iElapsSMEM = seconds() - iStart;\n","\n","\tCHECK(cudaMemcpy(AT_h, d_AT, nBytes, cudaMemcpyDeviceToHost));\n","\tcheckResult(A_h, B_h, nrows, ncols);\n","\n","\tibnd = 2 * ncells * sizeof(float) / 1e9 / iElapsSMEM;\n","\tprintf(\"transposeSmem elapsed %f sec\\n <<< grid (%d,%d) block (%d,%d)>>> \"\n","\t\t\t\"effective bandwidth %f GB\\n\", iElapsSMEM, grid.x, grid.y, block.x,\n","\t\t\tblock.y, ibnd);\n","\n","\tprintf(\"SPEEDUP = %f\\n\", iElaps/iElapsSMEM);\n","\n","\t// free host and device memory\n","\tCHECK(cudaFree(d_A));\n","\tCHECK(cudaFree(d_AT));\n","\tfree(A_h);\n","\n","\t// reset device\n","\tCHECK(cudaDeviceReset());\n","\treturn EXIT_SUCCESS;\n","}\n","\n","void initialData(float *in, const int size) {\n","\tfor (int i = 0; i < size; i++)\n","\t\tin[i] = i; // (float)(rand()/INT_MAX) * 10.0f;\n","\treturn;\n","}\n","\n","void printData(float *in, int nrows, int ncols) {\n","\tfor (int i = 0; i < nrows; i++) {\n","\t\tfor (int j = 0; j < ncols; j++)\n","\t\t\tprintf(\"%3.0f \", in[INDEX(i, j, ncols)]);\n","\t\tprintf(\"\\n\");\n","\t}\n","}\n","\n","void transposeHost(float *out, float *in, const int nrows, const int ncols) {\n","\tfor (int iy = 0; iy < nrows; ++iy)\n","\t\tfor (int ix = 0; ix < ncols; ++ix)\n","\t\t\tout[INDEX(ix, iy, nrows)] = in[INDEX(iy, ix, ncols)];\n","}\n","\n","void checkResult(float *hostRef, float *gpuRef, int rows, int cols) {\n","\tdouble epsilon = 1.0E-8;\n","\tbool match = 1;\n","\n","\tfor (int i = 0; i < rows; i++) {\n","\t\tfor (int j = 0; j < cols; j++) {\n","\t\t\tint index = INDEX(i, j, cols);\n","\t\t\tif (abs(hostRef[index] - gpuRef[index]) > epsilon) {\n","\t\t\t\tmatch = 0;\n","\t\t\t\tprintf(\"different on (%d, %d) (offset=%d) element in \"\n","\t\t\t\t\t\t\"transposed matrix: host %f gpu %f\\n\", i, j, index,\n","\t\t\t\t\t\thostRef[index], gpuRef[index]);\n","\t\t\t\tbreak;\n","\t\t\t}\n","\t\t}\n","\t\tif (!match)\n","\t\t\tbreak;\n","\t}\n","\n","\tif (!match)\n","\t\tprintf(\"Arrays do not match.\\n\");\n","}\n"],"metadata":{"id":"xBFQxvgumfdu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["↩ **Run...**"],"metadata":{"id":"EZVOb_mOZEZ-"}},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/lez5/transposeSMEM.cu -o transposeSMEM\n","!./transposeSMEM"],"metadata":{"id":"_WzHYqto-TfG"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["NO_C5o9-xRF_","DvmApCF76YD0","SbQlRthtJTQE"],"private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}